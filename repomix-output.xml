This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.github/
  workflows/
    ci.yml
config/
  prompts/
    __init__.py
    debate.yaml
  __init__.py
  settings.py
docker/
  docker-compose.yml
  Dockerfile
docs/
  Setup.md
  Tree.md
scripts/
  init_db.py
  verify_setup.py
src/
  agents/
    __init__.py
    base.py
    claude_agent.py
    gemini_agent.py
    openai_agent.py
  orchestration/
    __init__.py
    debate.py
    orchestrator.py
    voting.py
  orchestrator/
    __init__.py
    collaboration.py
    debate.py
    manager.py
    task_analyzer.py
    task_queue.py
    voting.py
  persistence/
    __init__.py
    cache.py
    database.py
    models.py
    repositories.py
  ui/
    __init__.py
    collaboration_ui.py
    display.py
    input_handler.py
    streaming.py
    terminal.py
  utils/
    __init__.py
    exceptions.py
    helpers.py
    logger.py
    metrics.py
  __init__.py
  cli.py
  main.py
  mcp_server.py
tests/
  integration/
    test_collaboration.py
  unit/
    test_collaboration_orchestrator.py
    test_debate_system.py
  __init__.py
  conftest.py
  README.md
  requirements-test.txt
  test_docker.py
.env.example
.gitattributes
.gitignore
.mcp.json
ARCHITECTURE.md
LICENSE
Makefile
pyproject.toml
pytest.ini
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(chmod:*)",
      "Bash(touch:*)",
      "Bash(./venv/bin/python -m pytest tests/unit/test_persistence/test_database.py::TestDatabaseManager::test_database_manager_initialization -v)",
      "Bash(APP_ENV=development DATABASE_URL=sqlite:///./test.db ./venv/bin/python -c \"from config.settings import settings; print('Settings loaded successfully')\")",
      "Bash(APP_ENV=development DATABASE_URL=sqlite:///./test.db ./venv/bin/python -m pytest tests/unit/test_persistence/test_database.py::TestDatabaseManager::test_database_manager_initialization -v)",
      "Bash(./venv/bin/python test_simple.py:*)",
      "Bash(grep:*)",
      "Bash(APP_ENV=testing DATABASE_URL=sqlite+aiosqlite:///./test.db ./venv/bin/python -m pytest tests/unit/test_persistence/test_database.py::TestDatabaseManager::test_database_manager_initialization -v -s)",
      "Bash(./venv/bin/python test_database_working.py:*)",
      "Bash(./venv/bin/python:*)",
      "Bash(rm:*)"
    ],
    "deny": []
  }
}
</file>

<file path="src/mcp_server.py">
#!/usr/bin/env python3
"""
MCP Server for AngelaMCP - Proper Model Context Protocol implementation.

This is the core MCP server that Claude Code connects to for multi-agent collaboration.
I'm implementing the official MCP protocol instead of subprocess calls.
"""

import asyncio
import json
import sys
import logging
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, asdict
from pydantic import BaseModel, Field

# MCP Protocol imports
from mcp.server import MCPServer, RequestContext
from mcp.server.models import (
    InitializeResult,
    ServerCapabilities,
    Tool,
    TextContent,
    CallToolResult,
)
from mcp.types import JSONRPCMessage, JSONRPCRequest, JSONRPCResponse

# AngelaMCP imports
from config.settings import settings
from src.agents.claude_agent import ClaudeCodeAgent
from src.agents.openai_agent import OpenAIAgent
from src.agents.gemini_agent import GeminiAgent
from src.orchestrator.manager import TaskOrchestrator
from src.persistence.database import DatabaseManager
from src.utils.logger import get_logger

logger = get_logger("mcp_server")


@dataclass
class CollaborationResult:
    """Result of a collaboration session."""
    success: bool
    final_solution: str
    agent_responses: List[Dict[str, Any]]
    consensus_score: float
    debate_summary: Optional[str] = None
    execution_time: float = 0.0
    cost_breakdown: Optional[Dict[str, float]] = None


class CollaborationRequest(BaseModel):
    """Request for agent collaboration."""
    task_description: str = Field(..., description="The task to collaborate on")
    agents: List[str] = Field(default=["claude", "openai", "gemini"], description="Agents to include")
    strategy: str = Field(default="debate", description="Collaboration strategy: debate, parallel, consensus")
    max_rounds: int = Field(default=3, description="Maximum debate rounds")
    require_consensus: bool = Field(default=True, description="Whether consensus is required")


class DebateRequest(BaseModel):
    """Request for structured debate."""
    topic: str = Field(..., description="Topic to debate")
    agents: List[str] = Field(default=["claude", "openai", "gemini"], description="Participating agents")
    max_rounds: int = Field(default=3, description="Maximum debate rounds")
    timeout_seconds: int = Field(default=300, description="Timeout per round")


class AngelaMCPServer:
    """MCP Server for AngelaMCP multi-agent collaboration."""
    
    def __init__(self):
        self.server = MCPServer("angelamcp")
        self.orchestrator: Optional[TaskOrchestrator] = None
        self.db_manager: Optional[DatabaseManager] = None
        self.agents: Dict[str, Any] = {}
        self.setup_tools()
        
    async def initialize(self) -> None:
        """Initialize all components."""
        try:
            logger.info("Initializing AngelaMCP MCP Server...")
            
            # Initialize database
            self.db_manager = DatabaseManager()
            await self.db_manager.initialize()
            
            # Initialize agents
            self.agents = {
                "claude": ClaudeCodeAgent(),
                "openai": OpenAIAgent(), 
                "gemini": GeminiAgent()
            }
            
            # Initialize orchestrator
            self.orchestrator = TaskOrchestrator(
                claude_agent=self.agents["claude"],
                openai_agent=self.agents["openai"],
                gemini_agent=self.agents["gemini"],
                db_manager=self.db_manager
            )
            
            logger.info("AngelaMCP MCP Server initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize MCP server: {e}", exc_info=True)
            raise
    
    def setup_tools(self) -> None:
        """Setup MCP tools that Claude Code can call."""
        
        @self.server.call_tool()
        async def collaborate(arguments: Dict[str, Any]) -> List[TextContent]:
            """Orchestrate collaboration between multiple AI agents."""
            try:
                request = CollaborationRequest(**arguments)
                
                if not self.orchestrator:
                    return [TextContent(
                        type="text",
                        text="‚ùå AngelaMCP orchestrator not initialized"
                    )]
                
                logger.info(f"Starting collaboration on: {request.task_description}")
                
                # Execute collaboration
                result = await self.orchestrator.collaborate_on_task(
                    task_description=request.task_description,
                    agents=request.agents,
                    strategy=request.strategy,
                    max_rounds=request.max_rounds,
                    require_consensus=request.require_consensus
                )
                
                # Format response
                response_text = f"""
ü§ù **Multi-Agent Collaboration Complete**

**Task:** {request.task_description}
**Strategy:** {request.strategy}
**Agents:** {', '.join(request.agents)}
**Success:** {'‚úÖ' if result.success else '‚ùå'}
**Consensus Score:** {result.consensus_score:.2f}

**Final Solution:**
{result.final_solution}

**Agent Responses:**
"""
                
                for i, response in enumerate(result.agent_responses, 1):
                    agent_name = response.get('agent', 'Unknown')
                    content = response.get('content', 'No response')
                    response_text += f"\n**{i}. {agent_name}:**\n{content}\n"
                
                if result.debate_summary:
                    response_text += f"\n**Debate Summary:**\n{result.debate_summary}"
                
                return [TextContent(type="text", text=response_text)]
                
            except Exception as e:
                error_msg = f"‚ùå Collaboration failed: {str(e)}"
                logger.error(error_msg, exc_info=True)
                return [TextContent(type="text", text=error_msg)]
        
        @self.server.call_tool()
        async def debate(arguments: Dict[str, Any]) -> List[TextContent]:
            """Start a structured debate between AI agents."""
            try:
                request = DebateRequest(**arguments)
                
                if not self.orchestrator:
                    return [TextContent(
                        type="text",
                        text="‚ùå AngelaMCP orchestrator not initialized"
                    )]
                
                logger.info(f"Starting debate on: {request.topic}")
                
                # Execute debate
                result = await self.orchestrator.start_debate(
                    topic=request.topic,
                    agents=request.agents,
                    max_rounds=request.max_rounds,
                    timeout_seconds=request.timeout_seconds
                )
                
                # Format debate results
                response_text = f"""
üó£Ô∏è **Structured Debate Complete**

**Topic:** {request.topic}
**Participants:** {', '.join(request.agents)}
**Rounds:** {result.get('rounds_completed', 0)}/{request.max_rounds}

**Debate Results:**
"""
                
                # Add debate rounds
                for round_num, round_data in enumerate(result.get('rounds', []), 1):
                    response_text += f"\n**Round {round_num}:**\n"
                    for agent_response in round_data.get('responses', []):
                        agent = agent_response.get('agent', 'Unknown')
                        position = agent_response.get('content', 'No position')
                        response_text += f"- **{agent}:** {position}\n"
                
                # Add final consensus
                if 'consensus' in result:
                    consensus = result['consensus']
                    response_text += f"\n**Final Consensus:**\n{consensus['summary']}\n"
                    response_text += f"**Confidence Score:** {consensus.get('score', 0):.2f}\n"
                
                return [TextContent(type="text", text=response_text)]
                
            except Exception as e:
                error_msg = f"‚ùå Debate failed: {str(e)}"
                logger.error(error_msg, exc_info=True)
                return [TextContent(type="text", text=error_msg)]
        
        @self.server.call_tool()
        async def analyze_task_complexity(arguments: Dict[str, Any]) -> List[TextContent]:
            """Analyze task complexity and recommend collaboration strategy."""
            try:
                task_description = arguments.get("task_description", "")
                
                if not task_description:
                    return [TextContent(
                        type="text",
                        text="‚ùå No task description provided"
                    )]
                
                if not self.orchestrator:
                    return [TextContent(
                        type="text",
                        text="‚ùå AngelaMCP orchestrator not initialized"
                    )]
                
                # Analyze complexity
                analysis = await self.orchestrator.analyze_task_complexity(task_description)
                
                response_text = f"""
üìä **Task Complexity Analysis**

**Task:** {task_description}

**Complexity Score:** {analysis.get('complexity_score', 0):.2f}/10
**Estimated Time:** {analysis.get('estimated_time', 'Unknown')}
**Recommended Strategy:** {analysis.get('recommended_strategy', 'single_agent')}

**Analysis Details:**
- **Technical Complexity:** {analysis.get('technical_complexity', 'Unknown')}
- **Collaboration Benefit:** {analysis.get('collaboration_benefit', 'Unknown')}
- **Recommended Agents:** {', '.join(analysis.get('recommended_agents', []))}

**Reasoning:**
{analysis.get('reasoning', 'No reasoning provided')}
"""
                
                return [TextContent(type="text", text=response_text)]
                
            except Exception as e:
                error_msg = f"‚ùå Task analysis failed: {str(e)}"
                logger.error(error_msg, exc_info=True)
                return [TextContent(type="text", text=error_msg)]
        
        @self.server.call_tool()
        async def get_agent_status(arguments: Dict[str, Any]) -> List[TextContent]:
            """Get status of all AI agents."""
            try:
                status_text = "ü§ñ **Agent Status Report**\n\n"
                
                for agent_name, agent in self.agents.items():
                    try:
                        # Check agent health
                        health = await agent.health_check() if hasattr(agent, 'health_check') else {"status": "unknown"}
                        status = "‚úÖ Online" if health.get("status") == "healthy" else "‚ùå Offline"
                        
                        status_text += f"**{agent_name.title()} Agent:** {status}\n"
                        if "model" in health:
                            status_text += f"  - Model: {health['model']}\n"
                        if "last_response_time" in health:
                            status_text += f"  - Response Time: {health['last_response_time']:.2f}s\n"
                        
                    except Exception as e:
                        status_text += f"**{agent_name.title()} Agent:** ‚ùå Error - {str(e)}\n"
                
                # Add orchestrator status
                if self.orchestrator:
                    status_text += f"\n**Orchestrator:** ‚úÖ Ready\n"
                    status_text += f"**Database:** {'‚úÖ Connected' if self.db_manager else '‚ùå Disconnected'}\n"
                else:
                    status_text += f"\n**Orchestrator:** ‚ùå Not initialized\n"
                
                return [TextContent(type="text", text=status_text)]
                
            except Exception as e:
                error_msg = f"‚ùå Status check failed: {str(e)}"
                logger.error(error_msg, exc_info=True)
                return [TextContent(type="text", text=error_msg)]

    async def run(self) -> None:
        """Run the MCP server."""
        try:
            await self.initialize()
            
            # Server capabilities
            capabilities = ServerCapabilities(
                tools={
                    "collaborate": Tool(
                        name="collaborate",
                        description="Orchestrate collaboration between multiple AI agents on a task",
                        inputSchema={
                            "type": "object",
                            "properties": {
                                "task_description": {
                                    "type": "string",
                                    "description": "Description of the task to collaborate on"
                                },
                                "agents": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "List of agents to include (claude, openai, gemini)",
                                    "default": ["claude", "openai", "gemini"]
                                },
                                "strategy": {
                                    "type": "string",
                                    "enum": ["debate", "parallel", "consensus"],
                                    "description": "Collaboration strategy to use",
                                    "default": "debate"
                                },
                                "max_rounds": {
                                    "type": "integer",
                                    "description": "Maximum number of debate rounds",
                                    "default": 3
                                },
                                "require_consensus": {
                                    "type": "boolean",
                                    "description": "Whether consensus is required",
                                    "default": True
                                }
                            },
                            "required": ["task_description"]
                        }
                    ),
                    "debate": Tool(
                        name="debate",
                        description="Start a structured debate between AI agents on a topic",
                        inputSchema={
                            "type": "object",
                            "properties": {
                                "topic": {
                                    "type": "string",
                                    "description": "Topic to debate"
                                },
                                "agents": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "Agents to include in debate",
                                    "default": ["claude", "openai", "gemini"]
                                },
                                "max_rounds": {
                                    "type": "integer",
                                    "description": "Maximum debate rounds",
                                    "default": 3
                                },
                                "timeout_seconds": {
                                    "type": "integer",
                                    "description": "Timeout per round in seconds",
                                    "default": 300
                                }
                            },
                            "required": ["topic"]
                        }
                    ),
                    "analyze_task_complexity": Tool(
                        name="analyze_task_complexity",
                        description="Analyze task complexity and recommend collaboration strategy",
                        inputSchema={
                            "type": "object",
                            "properties": {
                                "task_description": {
                                    "type": "string",
                                    "description": "Task to analyze"
                                }
                            },
                            "required": ["task_description"]
                        }
                    ),
                    "get_agent_status": Tool(
                        name="get_agent_status",
                        description="Get current status of all AI agents",
                        inputSchema={
                            "type": "object",
                            "properties": {},
                            "additionalProperties": False
                        }
                    )
                }
            )
            
            # Initialize server
            init_result = InitializeResult(
                protocolVersion="2024-11-05",
                capabilities=capabilities,
                serverInfo={
                    "name": "AngelaMCP",
                    "version": "1.0.0"
                }
            )
            
            logger.info("AngelaMCP MCP Server ready - waiting for connections...")
            
            # Run the MCP server
            await self.server.run(
                transport="stdio",
                init_result=init_result
            )
            
        except Exception as e:
            logger.error(f"MCP server run failed: {e}", exc_info=True)
            raise


async def main():
    """Main entry point for MCP server."""
    try:
        server = AngelaMCPServer()
        await server.run()
    except KeyboardInterrupt:
        logger.info("MCP server shutdown requested")
    except Exception as e:
        logger.error(f"MCP server error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="tests/test_docker.py">
#!/usr/bin/env python3
"""
Test script for AngelaMCP with Docker databases.

This tests the connection to Docker-hosted PostgreSQL and Redis from local Python code.
I'm making this comprehensive to catch any connection issues early.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import after adding to path
from config.settings import settings
from src.utils.logger import setup_logging, get_logger
from src.persistence.database import DatabaseManager
from src.agents.claude_agent import ClaudeCodeAgent
from src.agents.openai_agent import OpenAIAgent
from src.agents.gemini_agent import GeminiAgent
from src.orchestrator.manager import TaskOrchestrator

# Setup logging
setup_logging()
logger = get_logger("test_local_docker")


class LocalDockerTester:
    """Test AngelaMCP components with Docker databases."""
    
    def __init__(self):
        self.db_manager = None
        self.orchestrator = None
        self.test_results = {}
    
    async def run_all_tests(self) -> bool:
        """Run all tests and return success status."""
        logger.info("üß™ Starting AngelaMCP Local Docker Tests")
        
        tests = [
            ("Environment Variables", self.test_environment),
            ("Database Connection", self.test_database_connection),
            ("Redis Connection", self.test_redis_connection),
            ("Agent Initialization", self.test_agent_initialization),
            ("Basic Orchestration", self.test_basic_orchestration),
            ("Health Checks", self.test_health_checks),
        ]
        
        success_count = 0
        
        for test_name, test_func in tests:
            try:
                logger.info(f"üîç Running test: {test_name}")
                result = await test_func()
                
                if result:
                    logger.info(f"‚úÖ {test_name}: PASSED")
                    self.test_results[test_name] = "PASSED"
                    success_count += 1
                else:
                    logger.error(f"‚ùå {test_name}: FAILED")
                    self.test_results[test_name] = "FAILED"
                    
            except Exception as e:
                logger.error(f"‚ùå {test_name}: ERROR - {e}", exc_info=True)
                self.test_results[test_name] = f"ERROR: {e}"
        
        # Print summary
        self.print_test_summary(success_count, len(tests))
        
        # Cleanup
        await self.cleanup()
        
        return success_count == len(tests)
    
    async def test_environment(self) -> bool:
        """Test environment variables are set."""
        try:
            required_vars = [
                ("DATABASE_URL", settings.database_url),
                ("REDIS_URL", settings.redis_url),
                ("OPENAI_API_KEY", settings.openai_api_key),
                ("GOOGLE_API_KEY", settings.google_api_key),
            ]
            
            missing_vars = []
            for var_name, var_value in required_vars:
                if not var_value:
                    missing_vars.append(var_name)
                else:
                    logger.info(f"  ‚úÖ {var_name}: configured")
            
            if missing_vars:
                logger.error(f"  ‚ùå Missing variables: {missing_vars}")
                return False
            
            # Test Docker database URLs
            db_url = str(settings.database_url)
            redis_url = str(settings.redis_url)
            
            if "localhost" in db_url:
                logger.info("  ‚úÖ Database URL points to localhost (Docker)")
            else:
                logger.warning(f"  ‚ö†Ô∏è Database URL: {db_url}")
            
            if "localhost" in redis_url:
                logger.info("  ‚úÖ Redis URL points to localhost (Docker)")
            else:
                logger.warning(f"  ‚ö†Ô∏è Redis URL: {redis_url}")
            
            return True
            
        except Exception as e:
            logger.error(f"Environment test failed: {e}")
            return False
    
    async def test_database_connection(self) -> bool:
        """Test PostgreSQL connection (Docker)."""
        try:
            self.db_manager = DatabaseManager()
            await self.db_manager.initialize()
            
            # Test basic query
            async with self.db_manager.get_session() as session:
                from sqlalchemy import text
                result = await session.execute(text("SELECT current_timestamp, version()"))
                row = result.fetchone()
                
                logger.info(f"  ‚úÖ Connected to PostgreSQL")
                logger.info(f"  ‚úÖ Server time: {row[0]}")
                logger.info(f"  ‚úÖ Version: {row[1][:50]}...")
            
            return True
            
        except Exception as e:
            logger.error(f"Database connection test failed: {e}")
            return False
    
    async def test_redis_connection(self) -> bool:
        """Test Redis connection (Docker)."""
        try:
            if not self.db_manager:
                return False
            
            redis_client = await self.db_manager.get_redis()
            
            # Test basic operations
            await redis_client.ping()
            logger.info("  ‚úÖ Redis ping successful")
            
            # Test set/get
            test_key = "angelamcp:test"
            test_value = "docker_test_value"
            
            await redis_client.set(test_key, test_value, ex=60)  # Expire in 60s
            retrieved_value = await redis_client.get(test_key)
            
            if retrieved_value == test_value:
                logger.info("  ‚úÖ Redis set/get successful")
                await redis_client.delete(test_key)  # Cleanup
                return True
            else:
                logger.error(f"  ‚ùå Redis value mismatch: {retrieved_value} != {test_value}")
                return False
            
        except Exception as e:
            logger.error(f"Redis connection test failed: {e}")
            return False
    
    async def test_agent_initialization(self) -> bool:
        """Test agent initialization."""
        try:
            # Test Claude Code agent
            try:
                claude_agent = ClaudeCodeAgent()
                logger.info("  ‚úÖ Claude Code agent initialized")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Claude Code agent failed: {e}")
                # Create a mock for testing
                claude_agent = None
            
            # Test OpenAI agent
            try:
                openai_agent = OpenAIAgent()
                logger.info("  ‚úÖ OpenAI agent initialized")
            except Exception as e:
                logger.error(f"  ‚ùå OpenAI agent failed: {e}")
                return False
            
            # Test Gemini agent  
            try:
                gemini_agent = GeminiAgent()
                logger.info("  ‚úÖ Gemini agent initialized")
            except Exception as e:
                logger.error(f"  ‚ùå Gemini agent failed: {e}")
                return False
            
            # Store agents for orchestrator test
            self.agents = {
                'claude': claude_agent,
                'openai': openai_agent,
                'gemini': gemini_agent
            }
            
            return True
            
        except Exception as e:
            logger.error(f"Agent initialization test failed: {e}")
            return False
    
    async def test_basic_orchestration(self) -> bool:
        """Test basic orchestration functionality."""
        try:
            if not self.db_manager or not hasattr(self, 'agents'):
                logger.error("  ‚ùå Prerequisites not met")
                return False
            
            # Initialize orchestrator
            self.orchestrator = TaskOrchestrator(
                claude_agent=self.agents['claude'],
                openai_agent=self.agents['openai'],
                gemini_agent=self.agents['gemini'],
                db_manager=self.db_manager
            )
            logger.info("  ‚úÖ Task orchestrator initialized")
            
            # Test task complexity analysis
            analysis = await self.orchestrator.analyze_task_complexity(
                "Create a simple calculator function"
            )
            
            if analysis and 'complexity_score' in analysis:
                logger.info(f"  ‚úÖ Task analysis: {analysis['complexity_score']:.1f}/10")
                logger.info(f"  ‚úÖ Recommended strategy: {analysis['recommended_strategy']}")
                return True
            else:
                logger.error("  ‚ùå Task analysis failed")
                return False
            
        except Exception as e:
            logger.error(f"Orchestration test failed: {e}")
            return False
    
    async def test_health_checks(self) -> bool:
        """Test health check functionality."""
        try:
            if not self.db_manager:
                return False
            
            # Database health check
            db_health = await self.db_manager.health_check()
            
            if db_health['overall'] in ['healthy', 'degraded']:
                logger.info(f"  ‚úÖ Database health: {db_health['overall']}")
                logger.info(f"  ‚úÖ PostgreSQL: {db_health['postgres']['status']}")
                logger.info(f"  ‚úÖ Redis: {db_health['redis']['status']}")
            else:
                logger.error(f"  ‚ùå Database health: {db_health['overall']}")
                return False
            
            # Agent health checks
            if hasattr(self, 'agents'):
                for agent_name, agent in self.agents.items():
                    if agent:
                        try:
                            health = await agent.health_check()
                            status = health.get('status', 'unknown')
                            logger.info(f"  ‚úÖ {agent_name.title()} agent: {status}")
                        except Exception as e:
                            logger.warning(f"  ‚ö†Ô∏è {agent_name.title()} agent health check failed: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"Health check test failed: {e}")
            return False
    
    def print_test_summary(self, success_count: int, total_tests: int) -> None:
        """Print test summary."""
        print("\n" + "="*60)
        print("üß™ ANGELAMCP LOCAL DOCKER TEST SUMMARY")
        print("="*60)
        
        for test_name, result in self.test_results.items():
            status_emoji = "‚úÖ" if result == "PASSED" else "‚ùå"
            print(f"{status_emoji} {test_name}: {result}")
        
        print(f"\nPassed: {success_count}/{total_tests}")
        success_rate = (success_count / total_tests) * 100
        print(f"Success Rate: {success_rate:.1f}%")
        
        if success_count == total_tests:
            print("\nüéâ ALL TESTS PASSED - AngelaMCP is ready to use!")
            print("\nNext steps:")
            print("1. Run: python -m src.main")
            print("2. Or register MCP: make mcp-register")
        else:
            print(f"\n‚ö†Ô∏è {total_tests - success_count} test(s) failed")
            print("Check the logs above for specific issues")
        
        print("="*60)
    
    async def cleanup(self) -> None:
        """Cleanup test resources."""
        try:
            if self.db_manager:
                await self.db_manager.close()
                logger.info("Database connections closed")
        except Exception as e:
            logger.error(f"Cleanup error: {e}")


async def main():
    """Main test function."""
    try:
        tester = LocalDockerTester()
        success = await tester.run_all_tests()
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.info("Test interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Test runner failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path=".mcp.json">
{
  "mcpServers": {
    "angelamcp": {
      "command": "python",
      "args": ["-m", "src.mcp_server"],
      "env": {
        "OPENAI_API_KEY": "${OPENAI_API_KEY}",
        "GOOGLE_API_KEY": "${GOOGLE_API_KEY}",
        "DATABASE_URL": "${DATABASE_URL}",
        "REDIS_URL": "${REDIS_URL}",
        "APP_ENV": "production"
      }
    }
  }
}
</file>

<file path="config/prompts/__init__.py">
"""
Prompt templates and configuration for AngelaMCP agents.
Centralizes all prompt templates used across the multi-agent system.
"""

import yaml
from pathlib import Path
from typing import Dict, Any, Optional
from config.settings import settings

# Path to prompts directory
PROMPTS_DIR = Path(__file__).parent

def load_prompt_template(template_name: str, category: str = "debate") -> Optional[str]:
    """Load a prompt template from YAML files."""
    template_file = PROMPTS_DIR / f"{category}.yaml"
    
    if not template_file.exists():
        return None
        
    with open(template_file, 'r', encoding='utf-8') as f:
        templates = yaml.safe_load(f)
        
    return templates.get(category, {}).get(template_name)

def format_prompt(template: str, **kwargs) -> str:
    """Format a prompt template with provided variables."""
    try:
        return template.format(**kwargs)
    except KeyError as e:
        raise ValueError(f"Missing required template variable: {e}")

class PromptTemplates:
    """Central repository for all prompt templates."""
    
    @staticmethod
    def get_debate_initial_proposal(specialty: str, task_description: str, constraints: str = "") -> str:
        """Get the initial proposal prompt for debate mode."""
        template = load_prompt_template("initial_proposal", "debate")
        if not template:
            return f"As an AI agent specializing in {specialty}, analyze and provide your solution for: {task_description}"
        
        return format_prompt(template, 
                           specialty=specialty,
                           task_description=task_description,
                           constraints=constraints)
    
    @staticmethod
    def get_debate_critique(agent_name: str, task_description: str, solution: str) -> str:
        """Get the critique prompt for debate mode."""
        template = load_prompt_template("critique", "debate")
        if not template:
            return f"Review and critique this solution from {agent_name}: {solution}"
        
        return format_prompt(template,
                           agent_name=agent_name,
                           task_description=task_description,
                           solution=solution)
    
    @staticmethod
    def get_debate_rebuttal(original_solution: str, critiques: str) -> str:
        """Get the rebuttal prompt for debate mode."""
        template = load_prompt_template("rebuttal", "debate")
        if not template:
            return f"Address these critiques of your solution:\nOriginal: {original_solution}\nCritiques: {critiques}"
        
        return format_prompt(template,
                           original_solution=original_solution,
                           critiques=critiques)
    
    @staticmethod
    def get_debate_final_proposal(task_description: str, debate_summary: str) -> str:
        """Get the final proposal prompt for debate mode."""
        template = load_prompt_template("final_proposal", "debate")
        if not template:
            return f"Provide your final solution for: {task_description}\nConsidering: {debate_summary}"
        
        return format_prompt(template,
                           task_description=task_description,
                           debate_summary=debate_summary)

# Convenience instance
prompts = PromptTemplates()
</file>

<file path="config/prompts/debate.yaml">
# Debate protocol prompts
debate:
  initial_proposal: |
    As an AI agent specializing in {specialty}, analyze the following task and provide your proposed solution:
    
    Task: {task_description}
    Constraints: {constraints}
    
    Provide a detailed solution with rationale.

  critique: |
    Review the following solution from {agent_name} and provide constructive critique:
    
    Original Task: {task_description}
    Proposed Solution: {solution}
    
    Identify:
    1. Strengths of the approach
    2. Potential weaknesses or gaps
    3. Suggested improvements

  rebuttal: |
    Consider the following critiques of your solution and provide a rebuttal or refined proposal:
    
    Your Original Solution: {original_solution}
    Critiques Received: {critiques}
    
    Address each critique and explain your reasoning or propose modifications.

  final_proposal: |
    Based on the debate, provide your final solution for the task:
    
    Task: {task_description}
    Key Points from Debate: {debate_summary}
    
    Present your refined solution incorporating valid feedback.
</file>

<file path="config/__init__.py">
"""Configuration module for AngelaMCP."""
</file>

<file path="docker/docker-compose.yml">
services:
  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: angelamcp
    env_file:
      - ../.env
    depends_on:
      - postgres
      - redis
    volumes:
      - ..:/app
      - ~/.claude:/home/angelamcp/.claude:ro
    stdin_open: true
    tty: true

  postgres:
    image: postgres:14-alpine
    container_name: angelamcp_postgres
    environment:
      POSTGRES_USER: ${DATABASE_USER}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
      POSTGRES_DB: ${DATABASE_NAME}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    container_name: angelamcp_redis
    env_file:
      - ../.env
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"

volumes:
  postgres_data:
  redis_data:
</file>

<file path="docker/Dockerfile">
FROM python:3.12-slim-bookworm

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV APP_HOME=/app

RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    postgresql-client \
    redis-tools \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR $APP_HOME

COPY requirements.txt .

RUN pip install --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

RUN useradd -ms /bin/bash angelamcp
RUN chown -R angelamcp:angelamcp $APP_HOME

USER angelamcp

CMD ["python", "-m", "src.main"]
</file>

<file path="docs/Setup.md">
# MACP Setup Guide

Complete step-by-step installation guide for Debian/Ubuntu Linux.

## üìã System Requirements

- **OS**: Debian 11+ or Ubuntu 20.04+
- **Python**: 3.10 or higher
- **RAM**: 8GB minimum (16GB recommended)
- **Storage**: 10GB free space
- **Claude Code**: Already installed and configured

## üîß Step 1: System Dependencies

```bash
# Update system packages
sudo apt update && sudo apt upgrade -y

# Install Python and development tools
sudo apt install -y python3.10 python3.10-dev python3.10-venv python3-pip

# Install PostgreSQL
sudo apt install -y postgresql postgresql-contrib postgresql-client

# Install Redis
sudo apt install -y redis-server

# Install additional dependencies
sudo apt install -y git curl wget build-essential libpq-dev

# Verify installations
python3.10 --version
psql --version
redis-cli --version
claude --version  # Should show Claude Code version
```

## üóÑÔ∏è Step 2: Database Setup

### PostgreSQL Configuration

```bash
# Start PostgreSQL service
sudo systemctl start postgresql
sudo systemctl enable postgresql

# Create database and user
sudo -u postgres psql << EOF
CREATE USER macp_user WITH PASSWORD 'your_secure_password';
CREATE DATABASE macp_db OWNER macp_user;
GRANT ALL PRIVILEGES ON DATABASE macp_db TO macp_user;
\q
EOF

# Test connection
PGPASSWORD='your_secure_password' psql -h localhost -U macp_user -d macp_db -c '\l'
```

### Redis Configuration

```bash
# Start Redis service
sudo systemctl start redis-server
sudo systemctl enable redis-server

# Configure Redis for persistence (optional but recommended)
sudo bash -c 'echo "appendonly yes" >> /etc/redis/redis.conf'
sudo systemctl restart redis-server

# Test Redis
redis-cli ping  # Should return PONG
```

## üêç Step 3: Python Environment

```bash
# Create project directory
mkdir -p ~/projects/macp
cd ~/projects/macp

# Clone the repository (or create structure)
git clone https://github.com/yourusername/macp.git .

# Create virtual environment
python3.10 -m venv venv

# Activate virtual environment
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip setuptools wheel
```

## üì¶ Step 4: Install Dependencies

Create `requirements.txt`:

```bash
cat > requirements.txt << 'EOF'
# Core Dependencies
pydantic==2.5.0
pydantic-settings==2.1.0
python-dotenv==1.0.0

# API Clients
openai==1.12.0
google-generativeai==0.3.2
anthropic==0.18.1

# Database
sqlalchemy==2.0.25
asyncpg==0.29.0
psycopg2-binary==2.9.9
redis==5.0.1
alembic==1.13.1

# Async Support
asyncio==3.4.3
aiohttp==3.9.3
aiofiles==23.2.1

# Terminal UI
rich==13.7.0
prompt-toolkit==3.0.43
click==8.1.7

# Utilities
pyyaml==6.0.1
structlog==24.1.0
tenacity==8.2.3

# Development
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0
ruff==0.1.14
mypy==1.8.0
black==23.12.1

# Documentation
mkdocs==1.5.3
mkdocs-material==9.5.3
EOF

# Install all dependencies
pip install -r requirements.txt
```

## ‚öôÔ∏è Step 5: Configuration

### Create Environment File

```bash
# Copy example environment file
cp .env.example .env

# Or create new one
cat > .env << 'EOF'
# Application Settings
APP_NAME=MACP
APP_ENV=production
LOG_LEVEL=INFO

# Claude Code Configuration
CLAUDE_CODE_PATH=/usr/local/bin/claude
CLAUDE_CODE_TIMEOUT=300

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=o3-mini
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7

# Google Gemini Configuration
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-2.5-pro-preview-06-05
GEMINI_MAX_TOKENS=4096
GEMINI_TEMPERATURE=0.7

# Database Configuration
DATABASE_URL=postgresql://macp_user:your_secure_password@localhost:5432/macp_db
REDIS_URL=redis://localhost:6379/0

# API Rate Limiting
RATE_LIMIT_OPENAI=60  # requests per minute
RATE_LIMIT_GEMINI=60  # requests per minute

# Session Configuration
SESSION_TIMEOUT=3600  # 1 hour
MAX_CONVERSATION_LENGTH=100  # messages

# Feature Flags
ENABLE_COST_TRACKING=true
ENABLE_PARALLEL_EXECUTION=true
ENABLE_DEBATE_MODE=true
EOF

# Secure the environment file
chmod 600 .env
```

### Verify API Keys

```bash
# Create verification script
cat > scripts/verify_setup.py << 'EOF'
#!/usr/bin/env python3
"""Verify MACP setup and API connectivity."""

import os
import sys
import subprocess
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def check_claude_code():
    """Check Claude Code installation."""
    try:
        result = subprocess.run(['claude', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Claude Code is installed")
            return True
    except FileNotFoundError:
        pass
    print("‚ùå Claude Code not found. Please install it first.")
    return False

def check_env_vars():
    """Check required environment variables."""
    required = [
        'OPENAI_API_KEY',
        'GOOGLE_API_KEY',
        'DATABASE_URL',
        'REDIS_URL'
    ]
    
    missing = []
    for var in required:
        if not os.getenv(var):
            missing.append(var)
            print(f"‚ùå Missing: {var}")
        else:
            print(f"‚úÖ Found: {var}")
    
    return len(missing) == 0

def check_database():
    """Check database connectivity."""
    try:
        import psycopg2
        from urllib.parse import urlparse
        
        db_url = os.getenv('DATABASE_URL')
        url = urlparse(db_url)
        
        conn = psycopg2.connect(
            host=url.hostname,
            port=url.port,
            user=url.username,
            password=url.password,
            database=url.path[1:]
        )
        conn.close()
        print("‚úÖ PostgreSQL connection successful")
        return True
    except Exception as e:
        print(f"‚ùå PostgreSQL connection failed: {e}")
        return False

def check_redis():
    """Check Redis connectivity."""
    try:
        import redis
        r = redis.from_url(os.getenv('REDIS_URL'))
        r.ping()
        print("‚úÖ Redis connection successful")
        return True
    except Exception as e:
        print(f"‚ùå Redis connection failed: {e}")
        return False

if __name__ == "__main__":
    print("MACP Setup Verification\n" + "="*50)
    
    checks = [
        check_claude_code(),
        check_env_vars(),
        check_database(),
        check_redis()
    ]
    
    if all(checks):
        print("\n‚úÖ All checks passed! MACP is ready to use.")
        sys.exit(0)
    else:
        print("\n‚ùå Some checks failed. Please fix the issues above.")
        sys.exit(1)
EOF

chmod +x scripts/verify_setup.py

# Run verification
python scripts/verify_setup.py
```

## üóÉÔ∏è Step 6: Database Initialization

```bash
# Create database schema
cat > scripts/init_db.py << 'EOF'
#!/usr/bin/env python3
"""Initialize MACP database schema."""

import asyncio
from sqlalchemy import create_engine
from src.persistence.models import Base
from config.settings import settings

def init_database():
    """Create all database tables."""
    engine = create_engine(settings.DATABASE_URL)
    Base.metadata.create_all(bind=engine)
    print("‚úÖ Database schema created successfully")

if __name__ == "__main__":
    init_database()
EOF

# Run database initialization
python scripts/init_db.py
```

## üöÄ Step 7: First Run

```bash
# Make sure virtual environment is activated
source venv/bin/activate

# Run MACP
python -m src.main

# Or use the Makefile
make run
```

## üß™ Step 8: Verify Installation

```bash
# Run tests
make test

# Test individual agents
python scripts/test_agents.py

# Check logs
tail -f logs/macp.log
```

## üêõ Troubleshooting

### Common Issues

1. **Claude Code not found**
   ```bash
   # Verify Claude is in PATH
   which claude
   
   # Add to PATH if needed
   echo 'export PATH="$PATH:/path/to/claude"' >> ~/.bashrc
   source ~/.bashrc
   ```

2. **PostgreSQL connection refused**
   ```bash
   # Check PostgreSQL is running
   sudo systemctl status postgresql
   
   # Check pg_hba.conf allows local connections
   sudo nano /etc/postgresql/14/main/pg_hba.conf
   # Ensure you have: local all all md5
   ```

3. **Redis connection refused**
   ```bash
   # Check Redis is running
   sudo systemctl status redis-server
   
   # Test with redis-cli
   redis-cli ping
   ```

4. **API key errors**
   ```bash
   # Verify keys are set
   python -c "import os; print(os.getenv('OPENAI_API_KEY')[:10] + '...')"
   ```

## üìö Next Steps

1. Read [ARCHITECTURE.md](./docs/ARCHITECTURE.md) to understand the system design
2. Check [docs/examples/](./docs/examples/) for usage examples
3. Configure your preferred settings in `.env`
4. Start building with MACP!

---

Need help? Check our [FAQ](./docs/FAQ.md) or open an issue on GitHub.
</file>

<file path="docs/Tree.md">
# MACP Project Structure

```
AngelaMCP/
‚îú‚îÄ‚îÄ README.md                 # Main project documentation
‚îú‚îÄ‚îÄ SETUP.md                  # Detailed setup instructions
‚îú‚îÄ‚îÄ ARCHITECTURE.md           # System architecture documentation
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ .env.example             # Example environment configuration
‚îú‚îÄ‚îÄ .gitignore               # Git ignore rules
‚îú‚îÄ‚îÄ pyproject.toml           # Project configuration (ruff, pytest)
‚îú‚îÄ‚îÄ Makefile                 # Build automation
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py          # Pydantic settings management
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Model configurations
‚îÇ   ‚îî‚îÄ‚îÄ prompts/             # System prompts for agents
‚îÇ       ‚îú‚îÄ‚îÄ debate.yaml
‚îÇ       ‚îî‚îÄ‚îÄ templates.yaml
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ cli.py               # CLI interface
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/              # Agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py          # Base agent interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude_agent.py  # Claude Code wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_agent.py  # OpenAI o3-mini
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gemini_agent.py  # Gemini 2.5-pro
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator/        # Core orchestration logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manager.py       # Main orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debate.py        # Debate protocol
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ voting.py        # Voting mechanism
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ task_queue.py    # Async task management
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ persistence/         # Database layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py        # SQLAlchemy models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py      # DB connection management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repositories.py  # Data access layer
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ui/                  # Terminal interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ display.py       # Rich terminal UI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streaming.py     # Real-time output
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ input_handler.py # User input management
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logger.py        # Logging configuration
‚îÇ       ‚îú‚îÄ‚îÄ exceptions.py    # Custom exceptions
‚îÇ       ‚îî‚îÄ‚îÄ helpers.py       # Helper functions
‚îÇ
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py         # Pytest configuration
‚îÇ   ‚îú‚îÄ‚îÄ unit/               # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ integration/        # Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/           # Test data
‚îÇ
‚îú‚îÄ‚îÄ scripts/                 # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup_db.py         # Database initialization
‚îÇ   ‚îú‚îÄ‚îÄ migrate.py          # Database migrations
‚îÇ   ‚îî‚îÄ‚îÄ test_agents.py      # Agent connectivity test
‚îÇ
‚îú‚îÄ‚îÄ docker/                  # Docker configuration
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ postgres/
‚îÇ       ‚îî‚îÄ‚îÄ init.sql        # DB initialization
‚îÇ
‚îî‚îÄ‚îÄ docs/                    # Additional documentation
    ‚îú‚îÄ‚îÄ API.md              # API documentation
    ‚îú‚îÄ‚îÄ CONTRIBUTING.md     # Contribution guidelines
    ‚îî‚îÄ‚îÄ examples/           # Usage examples
```
</file>

<file path="scripts/init_db.py">
#!/usr/bin/env python3
"""
Database initialization script for AngelaMCP.

This creates all database tables and sets up the initial schema.
I'm making this robust with proper error handling and verification.
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.exc import OperationalError
import asyncpg
from config.settings import settings
from src.persistence.models import Base
from src.utils.logger import setup_logging, get_logger

setup_logging()
logger = get_logger("db_init")


async def create_database_if_not_exists():
    """Create the database if it doesn't exist."""
    try:

        db_name = settings.database_url.path.strip("/")
        
        postgres_db_url = settings.database_url.with_path("/postgres")
        
        logger.info(f"Checking if database '{db_name}' exists...")

        try:
            conn = await asyncpg.connect(str(postgres_db_url))
            
            exists = await conn.fetchval(
                "SELECT 1 FROM pg_database WHERE datname = $1", db_name
            )

            if not exists:
                logger.info(f"Creating database '{db_name}'...")
                await conn.execute(f'CREATE DATABASE "{db_name}"')
                logger.info(f"‚úÖ Database '{db_name}' created successfully")
            else:
                logger.info(f"‚úÖ Database '{db_name}' already exists")

            await conn.close()
        except Exception as e:
            logger.error(f"Failed to create or check database: {e}")
            raise
    except Exception as e:
        logger.error(f"Database creation check failed: {e}")
        pass 


async def create_tables():
    """Create all database tables asynchronously."""
    try:
        logger.info("Creating database tables asynchronously...")

        engine = create_async_engine(str(settings.database_url))

        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        logger.info("‚úÖ All database tables created successfully")

        async with engine.connect() as conn:
            tables_query = text("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' ORDER BY table_name")
            result = await conn.execute(tables_query)
            tables = result.fetchall()

            logger.info("Verified tables in database:")
            for table in tables:
                logger.info(f"  - {table[0]}")

        await engine.dispose()

    except OperationalError as e:
        logger.error(f"Database connection failed: {e}")
        logger.error("Make sure PostgreSQL is running in Docker and connection details in .env are correct.")
        raise
    except Exception as e:
        logger.error(f"Table creation failed: {e}")
        raise

async def verify_database_setup():
    """Verify database setup is working correctly."""
    try:
        logger.info("Verifying database setup...")

        from src.persistence.database import DatabaseManager

        db_manager = DatabaseManager()
        await db_manager.initialize()

        async with db_manager.get_session() as session:
            result = await session.execute(text("SELECT current_timestamp"))
            timestamp = result.scalar()
            logger.info(f"‚úÖ Database verification successful - Current time: {timestamp}")

        await db_manager.close()

    except Exception as e:
        logger.error(f"Database verification failed: {e}")
        raise


async def main():
    """Main initialization function."""
    try:
        logger.info(" Starting AngelaMCP database initialization...")

        await create_tables()
        await verify_database_setup()

        logger.info("üëª Database initialization completed successfully!")

        print("\n" + "="*60)
        print("‚úÖ AngelaMCP Database Setup Complete!")
        print("="*60)
        
        print("Database URL:", settings.database_url)
        
        print("Next steps:")
        print("1. Run: make verify")
        print("2. Run: make run")
        print("="*60)


    except Exception as e:
        logger.error(f"Database initialization failed: {e}")
        print("\n" + "="*60)
        print("‚ùå Database Setup Failed!")
        print("="*60)
        print(f"Error: {e}")
        print("\nTroubleshooting:")
        print("1. Is Docker running? (docker ps)")
        print("2. Did 'make docker-up' complete successfully?")
        print("3. Are database credentials in .env correct?")
        print("4. Is your DATABASE_URL in the format: postgresql+asyncpg://... ?")
        print("="*60)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="scripts/verify_setup.py">
#!/usr/bin/env python3
"""
Setup verification script for AngelaMCP.

This checks all components and dependencies to ensure the Dockerized
environment is working correctly.
"""

import asyncio
import sys
import os
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any
import inspect # We need this to check for async functions

# --- Add project root to path ---
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# --- Color codes for terminal output ---
class Colors:
    GREEN = "\033[92m"
    RED = "\033[91m"
    YELLOW = "\033[93m"
    BLUE = "\033[94m"
    CYAN = "\033[96m"
    MAGENTA = "\033[95m"
    END = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"

# --- Main Verifier Class ---
class AngelaMCPVerifier:
    def __init__(self):
        self.results = []
        # Import settings here so path is set
        try:
            from config.settings import settings
            self.settings = settings
        except Exception as e:
            print(f"{Colors.RED}‚ùå Critical Error: Could not load settings. Check .env and config/settings.py.{Colors.END}")
            print(f"   Details: {e}")
            sys.exit(1)

    # --- THIS IS THE CORRECTED run_check METHOD ---
    async def run_check(self, name: str, check_func, *args, **kwargs) -> bool:
        """Runs a single check and prints the result, handling both sync and async functions."""
        print(f"  {Colors.CYAN}Checking {name}...{Colors.END}", end='', flush=True)
        padding = 40 - len(name)
        print("." * padding, end='', flush=True)
        
        try:
            # Check if the function is async and await it if so
            if inspect.iscoroutinefunction(check_func):
                status, message = await check_func(*args, **kwargs)
            else:
                status, message = check_func(*args, **kwargs) # Run it synchronously
            
            if status == "PASS":
                print(f" {Colors.GREEN}[  OK  ]{Colors.END}")
                if message: print(f"    {Colors.GREEN}‚îî‚îÄ> {message}{Colors.END}")
                self.results.append(True)
                return True
            elif status == "WARN":
                print(f" {Colors.YELLOW}[ WARN ]{Colors.END}")
                if message: print(f"    {Colors.YELLOW}‚îî‚îÄ> {message}{Colors.END}")
                self.results.append(True) # Warnings don't cause failure
                return True
            else: # FAIL
                print(f" {Colors.RED}[ FAIL ]{Colors.END}")
                if message: print(f"    {Colors.RED}‚îî‚îÄ> {message}{Colors.END}")
                self.results.append(False)
                return False
        except Exception as e:
            print(f" {Colors.RED}[ ERROR ]{Colors.END}")
            print(f"    {Colors.RED}‚îî‚îÄ> An unexpected error occurred: {e}{Colors.END}")
            self.results.append(False)
            return False

    def check_python_version(self) -> Tuple[str, str]:
        """Checks Python version."""
        v = sys.version_info
        if v >= (3, 10):
            return "PASS", f"Python {v.major}.{v.minor}.{v.micro}"
        return "FAIL", f"Python version is {v.major}.{v.minor}. Require 3.10+"

    def check_dependencies(self) -> Tuple[str, str]:
        """Checks if required packages are installed in the venv."""
        try:
            with open(project_root / "requirements.txt") as f:
                reqs = [line.strip().split('==')[0] for line in f if line.strip() and not line.startswith('#')]
            
            import_map = {'psycopg2-binary': 'psycopg2', 'google-generativeai': 'google.genai'}
            
            missing = []
            for req in reqs:
                try:
                    __import__(import_map.get(req, req))
                except ImportError:
                    missing.append(req)

            if not missing:
                return "PASS", f"{len(reqs)} dependencies are installed."
            return "FAIL", f"Missing packages: {', '.join(missing)}. Run 'make install'."
        except FileNotFoundError:
            return "FAIL", "requirements.txt not found."
    
    def check_env_file(self) -> Tuple[str, str]:
        """Checks for .env file."""
        if not (project_root / ".env").exists():
            return "FAIL", "'.env' file not found. Copy '.env.example' and fill it out."
        return "PASS", "'.env' file found."

    def check_api_keys(self) -> Tuple[str, str]:
        """Checks if API keys seem to be configured."""
        missing_keys = []
        if 'your-' in str(self.settings.openai_api_key) or not str(self.settings.openai_api_key):
            missing_keys.append("OPENAI_API_KEY")
        if 'your-' in str(self.settings.google_api_key) or not str(self.settings.google_api_key):
            missing_keys.append("GOOGLE_API_KEY")
        
        if not missing_keys:
            return "PASS", "OpenAI & Google API keys seem to be set."
        return "WARN", f"API keys might be missing or placeholders: {', '.join(missing_keys)}"

    def check_docker_running(self) -> Tuple[str, str]:
        """Checks if the Docker daemon is running."""
        try:
            result = subprocess.run(["docker", "ps"], capture_output=True, check=True)
            return "PASS", "Docker daemon is running."
        except (FileNotFoundError, subprocess.CalledProcessError):
            return "FAIL", "Docker daemon is not running or 'docker' command is not in PATH."

    def check_project_containers(self) -> Tuple[str, str]:
        """Checks if the required project containers are running."""
        required = ["angelamcp", "angelamcp_postgres", "angelamcp_redis"]
        try:
            result = subprocess.run(["docker", "ps", "--format", "{{.Names}}"], capture_output=True, text=True, check=True)
            running_containers = result.stdout.strip().split('\n')
            
            missing = [name for name in required if name not in running_containers]
            
            if not missing:
                return "PASS", "All project containers are running."
            return "FAIL", f"Missing containers: {', '.join(missing)}. Run 'make docker-up'."
        except (FileNotFoundError, subprocess.CalledProcessError):
            return "FAIL", "Could not check Docker containers. Is Docker running?"

    # --- THIS IS THE CORRECTED DB CONNECTION CHECK ---
    async def check_db_connection(self) -> Tuple[str, str]:
        """Checks database connectivity via asyncpg."""
        try:
            # The asyncpg library does not understand "+asyncpg", so we remove it for the test.
            # SQLAlchemy still needs it, but this direct test does not.
            connect_url = str(self.settings.database_url).replace("+asyncpg", "")
            
            import asyncpg
            conn = await asyncio.wait_for(asyncpg.connect(connect_url), timeout=5)
            version = await conn.fetchval("SELECT version()")
            await conn.close()
            return "PASS", f"Connected to PostgreSQL ({version.split(' ')[1]})."
        except Exception as e:
            return "FAIL", f"Could not connect to database in Docker. Details: {e}"

    async def check_redis_connection(self) -> Tuple[str, str]:
        """Checks Redis connectivity."""
        try:
            import redis.asyncio as redis
            r = redis.from_url(str(self.settings.redis_url))
            await asyncio.wait_for(r.ping(), timeout=5)
            await r.aclose() # Use aclose() for newer versions
            return "PASS", "Connected to Redis."
        except Exception as e:
            return "FAIL", f"Could not connect to Redis in Docker. Details: {e}"

def print_header():
    """Prints the cool header."""
    print(f"{Colors.BOLD}{Colors.MAGENTA}")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£†‚†§‚†∂‚†∂‚†∂‚†§‚†§‚£Ä‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚°§‚†ö‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†¶‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∞‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†π‚£Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚†á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ß‚†Ä‚†Ä‚†Ä‚†Ä")
    print("AngelaMCP ‚îÄ ‚îÇ‚†Ä‚¢Ä‚£†‚†§‚†∂‚†í‚†í‚†í‚†í‚†∂‚†•‚†§‚£Ä‚°Ä‚†Ä‚¢Ä‚£Ä‚††‚†§‚†∂‚†í‚†í‚†í‚†∂‚†¨‚£ë‚°Ä‚†Ä‚îÇ ‚îÄ Verifier")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†∏‚°Ñ‚†∏‚°Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢¶‚£¨‚†û‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢à‚°ó‚†Ä‚¢∏‚†Ä‚†Ä‚†Ä‚†Ä")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚†¶‚£ù‚†í‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†ö‚£Å‚°§‚†ö‚†Ä‚†Ä‚†Ä‚†Ä")
    print("‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†â‚†í‚†í‚†¶‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†§‚†¥‚†í‚†ã‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä")
    print(f"{Colors.END}")

def print_success_footer():
    """Prints the very cool success message."""
    print(f"\n{Colors.BOLD}{Colors.GREEN}==============================================================={Colors.END}")
    print(f"{Colors.BOLD}{Colors.GREEN}‚ñà‚ñÄ‚ñÄ‚ÄÉ‚ñà‚ñë‚ñà‚ÄÉ‚ñà‚ñÄ‚ñÄ‚ÄÉ‚ñà‚ñÄ‚ñÄ‚ÄÉ‚ñÑ‚ñÄ‚ñà‚ÄÉ‚ñà‚ñÄ‚ÄÉ‚ñà‚ñÄ‚ÄÉ ‚ÄÉ‚ñà‚ñÄ‚ÄÉ‚ñÑ‚ñÄ‚ñà‚ÄÉ‚ñà‚ñÑ‚ñà‚ÄÉ‚ñà‚ñÄ‚ñÄ‚ÄÉ ‚ÄÉ‚ñà‚ñÄ‚ñÑ‚ñÄ‚ñà‚ÄÉ‚ñà‚ñÄ‚ñÄ‚ÄÉ‚ñÄ‚ñà‚ñÄ‚ÄÉ‚ñà‚ñÄ‚ñà{Colors.END}")
    print(f"{Colors.BOLD}{Colors.GREEN}‚ñà‚ñÑ‚ñÑ‚ÄÉ‚ñà‚ñÄ‚ñà‚ÄÉ‚ñà‚ñà‚ñÑ‚ÄÉ‚ñà‚ñà‚ñÑ‚ÄÉ‚ñà‚ñÄ‚ñà‚ÄÉ‚ñÑ‚ñà‚ÄÉ‚ñÑ‚ñà‚ÄÉ ‚ÄÉ‚ñÑ‚ñà‚ÄÉ‚ñà‚ñÄ‚ñà‚ÄÉ‚ñë‚ñà‚ñë‚ÄÉ‚ñà‚ñà‚ñÑ‚ÄÉ ‚ÄÉ‚ñà‚ñë‚ñà‚ñë‚ñà‚ÄÉ‚ñà‚ñà‚ñÑ‚ÄÉ‚ñë‚ñà‚ñë‚ÄÉ‚ñà‚ñÑ‚ñà{Colors.END}")
    print(f"{Colors.BOLD}{Colors.GREEN}==============================================================={Colors.END}")
    print(f"\n{Colors.CYAN}All systems operational. AngelaMCP is ready for deployment.{Colors.END}\n")
    print("  To start the application, run: " + f"{Colors.YELLOW}make run{Colors.END}")
    print("  To run as an MCP server, use: " + f"{Colors.YELLOW}make run-mcp{Colors.END}\n")

def print_failure_footer():
    """Prints the failure message."""
    print(f"\n{Colors.BOLD}{Colors.RED}==============================================================={Colors.END}")
    print(f"{Colors.BOLD}{Colors.RED}                             SETUP FAILED                             {Colors.END}")
    print(f"{Colors.BOLD}{Colors.RED}==============================================================={Colors.END}")
    print(f"\n{Colors.YELLOW}Please fix the [ FAIL ] checks above before proceeding.{Colors.END}\n")


async def main():
    """Main verification function."""
    print_header()
    time.sleep(1) # For dramatic effect
    
    verifier = AngelaMCPVerifier()

    print(f"\n{Colors.BOLD}{Colors.UNDERLINE}Phase 1: Local Environment Checks{Colors.END}")
    await verifier.run_check("Python Version", verifier.check_python_version)
    await verifier.run_check("Python Dependencies", verifier.check_dependencies)
    await verifier.run_check("Environment File (.env)", verifier.check_env_file)
    await verifier.run_check("API Key Configuration", verifier.check_api_keys)
    
    print(f"\n{Colors.BOLD}{Colors.UNDERLINE}Phase 2: Docker Environment Checks{Colors.END}")
    await verifier.run_check("Docker Service", verifier.check_docker_running)
    await verifier.run_check("Project Containers", verifier.check_project_containers)

    print(f"\n{Colors.BOLD}{Colors.UNDERLINE}Phase 3: Service Connectivity Checks{Colors.END}")
    await verifier.run_check("Database (PostgreSQL) Connection", verifier.check_db_connection)
    await verifier.run_check("Cache (Redis) Connection", verifier.check_redis_connection)

    if all(verifier.results):
        print_success_footer()
        return True
    else:
        print_failure_footer()
        return False


if __name__ == "__main__":
    is_success = False
    try:
        is_success = asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}Verification interrupted by user.{Colors.END}")
        sys.exit(1)
    
    sys.exit(0 if is_success else 1)
</file>

<file path="src/agents/__init__.py">
"""Agent implementations for AngelaMCP."""

from .base import BaseAgent, AgentResponse
from .claude_agent import ClaudeCodeAgent
from .openai_agent import OpenAIAgent
from .gemini_agent import GeminiAgent

__all__ = ["BaseAgent", "AgentResponse", "ClaudeCodeAgent", "OpenAIAgent", "GeminiAgent"]
</file>

<file path="src/agents/base.py">
"""
Base agent interface for AngelaMCP.

This defines the common interface that all AI agents must implement.
I'm using ABC to ensure consistent implementation across Claude, OpenAI, and Gemini.
"""

import asyncio
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Union
from pydantic import BaseModel, Field

from src.utils.logger import get_logger


class AgentType(str, Enum):
    """Types of AI agents in the system."""
    CLAUDE = "claude"
    OPENAI = "openai"
    GEMINI = "gemini"


class TaskType(str, Enum):
    """Types of tasks agents can handle."""
    GENERAL = "general"
    CODE_GENERATION = "code_generation"
    CODE_REVIEW = "code_review"
    ANALYSIS = "analysis"
    RESEARCH = "research"
    DOCUMENTATION = "documentation"
    CREATIVE = "creative"
    DEBATE = "debate"


class AgentRole(str, Enum):
    """Roles agents can play in collaboration."""
    PRIMARY = "primary"
    REVIEWER = "reviewer"
    RESEARCHER = "researcher"
    SPECIALIST = "specialist"
    CRITIC = "critic"
    PROPOSER = "proposer"
    DEBATER = "debater"


@dataclass
class TaskContext:
    """Context information for task execution."""
    task_type: TaskType = TaskType.GENERAL
    agent_role: Optional[AgentRole] = None
    conversation_id: Optional[str] = None
    session_id: Optional[str] = None
    user_preferences: Dict[str, Any] = field(default_factory=dict)
    constraints: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def model_copy(self) -> 'TaskContext':
        """Create a copy of the context."""
        return TaskContext(
            task_type=self.task_type,
            agent_role=self.agent_role,
            conversation_id=self.conversation_id,
            session_id=self.session_id,
            user_preferences=self.user_preferences.copy(),
            constraints=self.constraints.copy(),
            metadata=self.metadata.copy()
        )


@dataclass
class AgentResponse:
    """Response from an AI agent."""
    agent_type: AgentType
    content: str
    confidence: float = 0.8
    execution_time_ms: float = 0.0
    token_usage: Dict[str, int] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None


class BaseAgent(ABC):
    """
    Abstract base class for all AI agents.
    
    All agents (Claude, OpenAI, Gemini) must implement this interface.
    """
    
    def __init__(
        self, 
        agent_type: AgentType, 
        name: str, 
        capabilities: List[str] = None
    ):
        self.agent_type = agent_type
        self.name = name
        self.capabilities = capabilities or []
        self.logger = get_logger(f"agents.{agent_type.value}")
        self._last_response_time: Optional[float] = None
        self._total_requests = 0
        self._failed_requests = 0
    
    def __str__(self) -> str:
        return f"{self.name} ({self.agent_type.value})"
    
    def __repr__(self) -> str:
        return f"<{self.__class__.__name__}: {self.agent_type.value}>"
    
    @abstractmethod
    async def generate(self, prompt: str, context: TaskContext) -> AgentResponse:
        """
        Generate a response to the given prompt.
        
        Args:
            prompt: The input prompt/question
            context: Task context and metadata
            
        Returns:
            AgentResponse with the generated content
        """
        pass
    
    @abstractmethod
    async def critique(self, content: str, original_task: str, context: TaskContext) -> AgentResponse:
        """
        Critique another agent's response or solution.
        
        Args:
            content: The content to critique
            original_task: The original task description
            context: Task context
            
        Returns:
            AgentResponse with critique and suggestions
        """
        pass
    
    @abstractmethod
    async def propose_solution(self, task_description: str, constraints: List[str], context: TaskContext) -> AgentResponse:
        """
        Propose a solution for a given task.
        
        Args:
            task_description: Description of the task
            constraints: Any constraints or requirements
            context: Task context
            
        Returns:
            AgentResponse with proposed solution
        """
        pass
    
    async def health_check(self) -> Dict[str, Any]:
        """
        Check agent health and availability.
        
        Returns:
            Dict with health status information
        """
        try:
            start_time = time.time()
            
            # Basic test - should be overridden by implementations
            test_context = TaskContext(task_type=TaskType.GENERAL)
            response = await self.generate("Hello", test_context)
            
            response_time = time.time() - start_time
            self._last_response_time = response_time
            
            return {
                "status": "healthy",
                "agent_type": self.agent_type.value,
                "name": self.name,
                "response_time": response_time,
                "total_requests": self._total_requests,
                "failed_requests": self._failed_requests,
                "success_rate": ((self._total_requests - self._failed_requests) / max(1, self._total_requests)) * 100,
                "capabilities": self.capabilities,
                "last_check": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Health check failed: {e}")
            return {
                "status": "unhealthy",
                "agent_type": self.agent_type.value,
                "name": self.name,
                "error": str(e),
                "last_check": time.time()
            }
    
    def supports_capability(self, capability: str) -> bool:
        """Check if agent supports a specific capability."""
        return capability in self.capabilities
    
    def get_stats(self) -> Dict[str, Any]:
        """Get agent performance statistics."""
        return {
            "total_requests": self._total_requests,
            "failed_requests": self._failed_requests,
            "success_rate": ((self._total_requests - self._failed_requests) / max(1, self._total_requests)) * 100,
            "last_response_time": self._last_response_time,
            "capabilities": self.capabilities
        }
    
    async def _track_request(self, func, *args, **kwargs):
        """Track request statistics."""
        self._total_requests += 1
        try:
            result = await func(*args, **kwargs)
            return result
        except Exception as e:
            self._failed_requests += 1
            raise e


# Global agent registry for easy access
class AgentRegistry:
    """
    Registry for managing all active agents.
    
    I'm implementing a centralized registry to track all active agents
    and provide easy access for the orchestration system.
    """
    
    def __init__(self):
        self._agents: Dict[str, BaseAgent] = {}
        self._agent_types: Dict[AgentType, List[BaseAgent]] = {
            agent_type: [] for agent_type in AgentType
        }
        self.logger = get_logger("agents.registry")
    
    def register(self, agent: BaseAgent) -> None:
        """Register a new agent."""
        if agent.name in self._agents:
            raise ValueError(f"Agent with name '{agent.name}' already registered")
        
        self._agents[agent.name] = agent
        self._agent_types[agent.agent_type].append(agent)
        
        self.logger.info(f"Registered agent: {agent}")
    
    def unregister(self, agent_name: str) -> None:
        """Unregister an agent."""
        if agent_name not in self._agents:
            raise ValueError(f"Agent '{agent_name}' not found")
        
        agent = self._agents[agent_name]
        del self._agents[agent_name]
        self._agent_types[agent.agent_type].remove(agent)
        
        self.logger.info(f"Unregistered agent: {agent_name}")
    
    def get_agent(self, agent_name: str) -> Optional[BaseAgent]:
        """Get agent by name."""
        return self._agents.get(agent_name)
    
    def get_agents_by_type(self, agent_type: AgentType) -> List[BaseAgent]:
        """Get all agents of a specific type."""
        return self._agent_types[agent_type].copy()
    
    def get_all_agents(self) -> List[BaseAgent]:
        """Get all registered agents."""
        return list(self._agents.values())
    
    def get_available_agents(self, capability: Optional[str] = None) -> List[BaseAgent]:
        """Get agents that support a specific capability."""
        if capability is None:
            return self.get_all_agents()
        
        return [
            agent for agent in self._agents.values()
            if agent.supports_capability(capability)
        ]
    
    async def health_check_all(self) -> Dict[str, Dict[str, Any]]:
        """Perform health check on all registered agents."""
        results = {}
        
        for agent_name, agent in self._agents.items():
            try:
                results[agent_name] = await agent.health_check()
            except Exception as e:
                results[agent_name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        return results
    
    async def shutdown_all(self) -> None:
        """Shutdown all registered agents."""
        for agent in self._agents.values():
            try:
                if hasattr(agent, 'shutdown'):
                    await agent.shutdown()
            except Exception as e:
                self.logger.error(f"Error shutting down {agent.name}: {e}")


# Global registry instance
agent_registry = AgentRegistry()
</file>

<file path="src/agents/gemini_agent.py">
"""
Gemini Agent implementation for AngelaMCP using the new Google Gen AI SDK.

This agent specializes in research, documentation, and creative problem-solving.
I'm using the latest google-genai SDK for better performance and features.
"""

import asyncio
import time
from typing import List, Dict, Any, Optional

# New Gemini SDK imports
from google import genai
from google.genai import types

from src.agents.base import BaseAgent, AgentType, AgentResponse, TaskContext, TaskType
from src.utils.logger import get_logger
from src.utils.exceptions import AgentError
from config.settings import settings

logger = get_logger("agents.gemini")


class GeminiAgent(BaseAgent):
    """
    Gemini agent using the new Google Gen AI SDK.

    Specialized in:
    - Research and analysis
    - Documentation generation
    - Creative problem-solving
    - Best practices research
    """

    def __init__(self):
        super().__init__(
            agent_type=AgentType.GEMINI,
            name="Gemini Research Specialist",
            capabilities=[
                "research_analysis",
                "documentation",
                "creative_solutions",
                "best_practices",
                "comprehensive_analysis"
            ]
        )

        # Initialize the new Gemini client
        self.client = genai.Client(api_key=settings.google_api_key)
        self.model = settings.gemini_model
        self.max_retries = settings.gemini_max_retries
        self.retry_delay = settings.gemini_retry_delay

        # Default generation config
        self.default_config = types.GenerateContentConfig(
            max_output_tokens=settings.gemini_max_output_tokens,
            temperature=settings.gemini_temperature,
            top_p=settings.gemini_top_p,
            top_k=settings.gemini_top_k,
            stop_sequences=[],
        )

        logger.info(f"Initialized Gemini agent with model: {self.model}")

    async def generate(self, prompt: str, context: TaskContext) -> AgentResponse:
        """Generate response using Gemini with the new SDK."""
        start_time = time.time()

        try:
            # Build generation config
            config = self._build_config(context)

            # Add system instruction based on task type
            system_instruction = self._get_system_instruction(context)
            if system_instruction:
                config.system_instruction = system_instruction

            logger.debug(f"Generating response for prompt: {prompt[:100]}...")

            # Generate content with retry logic
            for attempt in range(self.max_retries + 1):
                try:
                    response = self.client.models.generate_content(
                        model=self.model,
                        contents=prompt,
                        config=config
                    )

                    # Extract response text
                    if response.candidates and response.candidates[0].content.parts:
                        content = response.candidates[0].content.parts[0].text

                        # Calculate metrics
                        execution_time = time.time() - start_time
                        input_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0)
                        output_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0)

                        return AgentResponse(
                            agent_type=self.agent_type,
                            content=content,
                            confidence=self._calculate_confidence(response),
                            execution_time_ms=execution_time * 1000,
                            token_usage={
                                "input_tokens": input_tokens,
                                "output_tokens": output_tokens,
                                "total_tokens": input_tokens + output_tokens
                            },
                            metadata={
                                "model": self.model,
                                "finish_reason": response.candidates[0].finish_reason if response.candidates else None,
                                "safety_ratings": self._extract_safety_ratings(response),
                                "attempt": attempt + 1
                            }
                        )
                    else:
                        raise AgentError("No valid response content from Gemini")

                except Exception as e:
                    if attempt < self.max_retries:
                        logger.warning(f"Gemini API error (attempt {attempt + 1}): {e}")
                        await asyncio.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff
                        continue
                    else:
                        raise AgentError(f"Gemini API failed after {self.max_retries + 1} attempts: {e}")

        except Exception as e:
            logger.error(f"Gemini generation failed: {e}", exc_info=True)
            raise AgentError(f"Gemini generation error: {e}")

    async def critique(self, content: str, original_task: str, context: TaskContext) -> AgentResponse:
        """Provide comprehensive critique using Gemini's analytical capabilities."""
        critique_prompt = f"""Please provide a thorough critique of the following solution for the task: "{original_task}"

Solution to review:
{content}

Provide a comprehensive analysis including:

**Research-Based Assessment:**
- How well does this align with current best practices?
- What recent developments or trends are relevant?
- Are there established patterns or frameworks being followed?

**Creative Alternatives:**
- What innovative approaches could be considered?
- Are there unconventional but effective solutions?
- How could this be enhanced with creative thinking?

**Documentation Quality:**
- Is the solution well-documented and explained?
- What additional documentation would be helpful?
- How clear is the approach for future maintenance?

**Comprehensive Analysis:**
- Long-term implications and sustainability
- Scalability considerations
- Integration with broader ecosystem
- Knowledge gaps that should be addressed

**Research Recommendations:**
- What additional research would strengthen this solution?
- Are there relevant case studies or examples?
- What resources should be consulted for improvement?

Focus on providing deep, research-backed insights that go beyond surface-level review."""

        # Update context for critique task
        critique_context = context.model_copy()
        critique_context.task_type = TaskType.CODE_REVIEW
        critique_context.agent_role = "research_analyst"

        return await self.generate(critique_prompt, critique_context)

    async def propose_solution(self, task_description: str, constraints: List[str],
                             context: TaskContext) -> AgentResponse:
        """Propose innovative solutions using Gemini's research capabilities."""
        constraints_text = "\n".join(f"- {constraint}" for constraint in constraints) if constraints else "None specified"

        solution_prompt = f"""Conduct comprehensive research and propose an innovative solution for:

**Task:** {task_description}

**Constraints:**
{constraints_text}

Please provide a research-driven solution including:

**1. Background Research:**
- Current state of the art in this domain
- Historical context and evolution
- Key players and established solutions
- Recent innovations and emerging trends

**2. Comprehensive Solution Design:**
- Step-by-step implementation strategy
- Technical architecture and design patterns
- Resource allocation and timeline considerations
- Integration points and dependencies

**3. Innovation Opportunities:**
- Novel applications of existing technologies
- Creative combinations of different approaches
- Potential for breakthrough solutions
- Future-oriented considerations

**4. Risk-Benefit Analysis:**
- Comprehensive risk assessment
- Mitigation strategies and contingency plans
- Expected benefits and success metrics
- Long-term implications and sustainability

**5. Implementation Roadmap:**
- Phased approach with milestones
- Critical success factors
- Monitoring and evaluation criteria
- Adaptation and evolution strategies

**6. Knowledge Resources:**
- Key references and documentation
- Communities and expert networks
- Tools and frameworks to leverage
- Continuous learning recommendations

Think outside the box while maintaining practical applicability."""

        # Update context for solution proposal
        solution_context = context.model_copy()
        solution_context.task_type = TaskType.ANALYSIS
        solution_context.agent_role = "research_specialist"

        return await self.generate(solution_prompt, solution_context)

    async def research_topic(self, topic: str, focus_areas: List[str], context: TaskContext) -> AgentResponse:
        """Conduct comprehensive research using Gemini's analytical capabilities."""
        focus_text = "\n".join(f"- {area}" for area in focus_areas) if focus_areas else "Comprehensive overview"

        research_prompt = f"""Conduct an in-depth research analysis on the following topic:

**Research Topic:** {topic}

**Focus Areas:**
{focus_text}

Please provide comprehensive research covering:

**1. Foundational Understanding:**
- Core concepts and definitions
- Historical context and evolution
- Theoretical foundations and principles
- Key stakeholders and ecosystem

**2. Current State Analysis:**
- Present landscape and major players
- Current trends and developments
- Market dynamics and adoption patterns
- Regulatory and policy considerations

**3. Technical Deep Dive:**
- Underlying technologies and methodologies
- Implementation approaches and frameworks
- Standards, protocols, and best practices
- Tools, platforms, and infrastructure

**4. Comparative Analysis:**
- Alternative approaches and solutions
- Competitive landscape analysis
- Strengths, weaknesses, and trade-offs
- Use case scenarios and applicability

**5. Future Outlook:**
- Emerging trends and innovations
- Predicted developments and evolution
- Opportunities and challenges ahead
- Strategic implications and recommendations

**6. Practical Applications:**
- Real-world implementations and case studies
- Success stories and failure lessons
- Common pitfalls and how to avoid them
- Best practices and guidelines

**7. Resource Compilation:**
- Essential tools and frameworks
- Key documentation and references
- Learning resources and communities
- Standards and specifications

Provide authoritative, well-researched information with practical insights."""

        # Update context for research task
        research_context = context.model_copy()
        research_context.task_type = TaskType.RESEARCH
        research_context.agent_role = "researcher"

        return await self.generate(research_prompt, research_context)

    async def generate_documentation(self, topic: str, audience: str, context: TaskContext) -> AgentResponse:
        """Generate comprehensive documentation using Gemini."""
        doc_prompt = f"""Create comprehensive documentation for:

**Topic:** {topic}
**Target Audience:** {audience}

Please generate documentation that includes:

**1. Executive Summary:**
- High-level overview
- Key benefits and value proposition
- Quick start guide

**2. Detailed Content:**
- Step-by-step instructions
- Technical specifications
- Configuration examples
- Best practices

**3. Visual Elements:**
- Diagrams and flowcharts (described in text)
- Code examples and snippets
- Configuration files
- Screenshots descriptions

**4. Troubleshooting:**
- Common issues and solutions
- Debugging approaches
- FAQ section
- Support resources

**5. Advanced Topics:**
- Customization options
- Integration guides
- Performance optimization
- Security considerations

**6. References:**
- Additional resources
- Related documentation
- Standards and specifications
- Community resources

Make it comprehensive, well-structured, and appropriate for the target audience."""

        doc_context = context.model_copy()
        doc_context.task_type = TaskType.DOCUMENTATION
        doc_context.agent_role = "technical_writer"

        return await self.generate(doc_prompt, doc_context)

    async def health_check(self) -> Dict[str, Any]:
        """Check Gemini agent health and connectivity."""
        try:
            start_time = time.time()

            # Simple test request
            response = self.client.models.generate_content(
                model=self.model,
                contents="Hello, this is a health check. Please respond with 'OK'.",
                config=types.GenerateContentConfig(
                    max_output_tokens=10,
                    temperature=0.0
                )
            )

            response_time = time.time() - start_time

            return {
                "status": "healthy",
                "model": self.model,
                "response_time": response_time,
                "last_check": time.time(),
                "capabilities": self.capabilities
            }

        except Exception as e:
            logger.error(f"Gemini health check failed: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "last_check": time.time()
            }

    def _build_config(self, context: TaskContext) -> types.GenerateContentConfig:
        """Build generation config based on context."""
        config = types.GenerateContentConfig(
            max_output_tokens=self.default_config.max_output_tokens,
            temperature=self.default_config.temperature,
            top_p=self.default_config.top_p,
            top_k=self.default_config.top_k,
        )

        # Adjust config based on task type
        if context.task_type == TaskType.CODE_GENERATION:
            config.temperature = 0.3  # More deterministic for code
        elif context.task_type == TaskType.CREATIVE:
            config.temperature = 0.9  # More creative
        elif context.task_type == TaskType.RESEARCH:
            config.temperature = 0.7  # Balanced for research

        return config

    def _get_system_instruction(self, context: TaskContext) -> Optional[str]:
        """Get system instruction based on context."""
        instructions = {
            TaskType.RESEARCH: "You are a comprehensive research specialist. Provide thorough, well-researched analysis with citations and practical insights.",
            TaskType.DOCUMENTATION: "You are a technical documentation expert. Create clear, comprehensive documentation that is well-structured and user-friendly.",
            TaskType.CREATIVE: "You are a creative problem-solver. Think outside the box and propose innovative solutions while maintaining practical applicability.",
            TaskType.ANALYSIS: "You are an analytical expert. Provide deep, systematic analysis with multiple perspectives and actionable insights.",
            TaskType.CODE_REVIEW: "You are a research-oriented code reviewer. Focus on best practices, innovative approaches, and comprehensive improvement suggestions."
        }

        return instructions.get(context.task_type)

    def _calculate_confidence(self, response) -> float:
        """Calculate confidence score based on response quality."""
        try:
            # Base confidence
            confidence = 0.8

            # Adjust based on safety ratings
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]

                # Check finish reason
                if hasattr(candidate, 'finish_reason'):
                    if candidate.finish_reason == "STOP":
                        confidence += 0.1
                    elif candidate.finish_reason in ["MAX_TOKENS", "SAFETY"]:
                        confidence -= 0.2

                # Check content length (longer usually indicates more thoughtful response)
                if hasattr(candidate, 'content') and candidate.content.parts:
                    content_length = len(candidate.content.parts[0].text)
                    if content_length > 1000:
                        confidence += 0.1

            return min(1.0, max(0.0, confidence))

        except Exception:
            return 0.7  # Default confidence

    def _extract_safety_ratings(self, response) -> List[Dict[str, Any]]:
        """Extract safety ratings from response."""
        try:
            safety_ratings = []
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'safety_ratings'):
                    for rating in candidate.safety_ratings:
                        safety_ratings.append({
                            "category": rating.category,
                            "probability": rating.probability
                        })
            return safety_ratings
        except Exception:
            return []
</file>

<file path="src/agents/openai_agent.py">
"""
OpenAI Agent implementation for AngelaMCP.

This agent specializes in code review, analysis, and quality assessment.
I'm implementing this as the "code reviewer" with focus on best practices and security.
"""

import asyncio
import time
import json
from typing import List, Dict, Any, Optional

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion

from src.agents.base import BaseAgent, AgentType, AgentResponse, TaskContext, TaskType
from src.utils.logger import get_logger
from src.utils.exceptions import AgentError
from config.settings import settings


class OpenAIAgent(BaseAgent):
    """
    OpenAI agent specializing in code review and analysis.
    
    Capabilities:
    - Code quality assessment
    - Security analysis  
    - Performance optimization
    - Best practices review
    - Detailed technical analysis
    """
    
    def __init__(self):
        super().__init__(
            agent_type=AgentType.OPENAI,
            name="OpenAI Code Reviewer",
            capabilities=[
                "code_review",
                "security_analysis",
                "performance_optimization", 
                "best_practices",
                "technical_analysis",
                "quality_assessment"
            ]
        )
        
        # Initialize OpenAI client
        self.client = AsyncOpenAI(api_key=settings.openai_api_key)
        self.model = settings.openai_model
        self.max_tokens = settings.openai_max_tokens
        self.temperature = settings.openai_temperature
        self.max_retries = settings.openai_max_retries
        self.retry_delay = settings.openai_retry_delay
        
        self.logger.info(f"Initialized OpenAI agent with model: {self.model}")
    
    async def generate(self, prompt: str, context: TaskContext) -> AgentResponse:
        """Generate response using OpenAI with retry logic."""
        start_time = time.time()
        
        try:
            # Build messages based on context
            messages = self._build_messages(prompt, context)
            
            # Generate with retry logic
            for attempt in range(self.max_retries + 1):
                try:
                    response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        max_tokens=self.max_tokens,
                        temperature=self._get_temperature(context),
                        top_p=settings.openai_top_p,
                        frequency_penalty=settings.openai_frequency_penalty,
                        presence_penalty=settings.openai_presence_penalty,
                        timeout=settings.openai_timeout
                    )
                    
                    # Extract response content
                    content = response.choices[0].message.content
                    if not content:
                        raise AgentError("Empty response from OpenAI")
                    
                    execution_time = time.time() - start_time
                    
                    return AgentResponse(
                        agent_type=self.agent_type,
                        content=content,
                        confidence=self._calculate_confidence(response),
                        execution_time_ms=execution_time * 1000,
                        token_usage={
                            "prompt_tokens": response.usage.prompt_tokens,
                            "completion_tokens": response.usage.completion_tokens,
                            "total_tokens": response.usage.total_tokens
                        },
                        metadata={
                            "model": self.model,
                            "finish_reason": response.choices[0].finish_reason,
                            "attempt": attempt + 1,
                            "context": context.task_type.value
                        }
                    )
                    
                except Exception as e:
                    if attempt < self.max_retries:
                        self.logger.warning(f"OpenAI API error (attempt {attempt + 1}): {e}")
                        await asyncio.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff
                        continue
                    else:
                        raise AgentError(f"OpenAI API failed after {self.max_retries + 1} attempts: {e}")
        
        except Exception as e:
            self.logger.error(f"OpenAI generation failed: {e}", exc_info=True)
            raise AgentError(f"OpenAI generation error: {e}")
    
    async def critique(self, content: str, original_task: str, context: TaskContext) -> AgentResponse:
        """Provide detailed code review and critique."""
        critique_prompt = f"""Please provide a thorough critique of the following solution for the task: "{original_task}"

Solution to review:
{content}

Provide a detailed analysis including:

**Strengths:**
- What aspects of the solution work well
- Good practices demonstrated
- Effective approaches used

**Weaknesses and Issues:**
- Technical problems or bugs
- Logic errors or edge cases missed
- Performance concerns
- Security vulnerabilities
- Code quality issues

**Specific Improvements:**
- Concrete suggestions for fixing issues
- Alternative approaches to consider
- Best practices that should be applied
- Code refactoring recommendations

**Security Analysis:**
- Potential security vulnerabilities
- Input validation concerns
- Authentication and authorization issues
- Data handling security

**Performance Assessment:**
- Performance bottlenecks
- Optimization opportunities
- Scalability considerations
- Resource usage analysis

**Overall Assessment:**
- Summary of solution quality
- Readiness for production use
- Priority of recommended changes
- Risk assessment

Focus on being constructive, specific, and actionable in your feedback."""

        critique_context = context.model_copy()
        critique_context.task_type = TaskType.CODE_REVIEW
        critique_context.agent_role = "reviewer"
        
        return await self.generate(critique_prompt, critique_context)
    
    async def propose_solution(self, task_description: str, constraints: List[str], context: TaskContext) -> AgentResponse:
        """Propose a solution with focus on best practices and quality."""
        constraints_text = "\n".join(f"- {constraint}" for constraint in constraints) if constraints else "None specified"
        
        solution_prompt = f"""Analyze the following task and propose a comprehensive solution with emphasis on code quality, security, and best practices:

**Task:** {task_description}

**Constraints:**
{constraints_text}

Please provide a structured solution including:

**1. Problem Analysis:**
- Break down the key requirements
- Identify potential challenges
- Consider edge cases and corner scenarios
- Security implications

**2. Proposed Approach:**
- High-level solution strategy
- Technology choices and rationale
- Architecture considerations
- Security-first design principles

**3. Implementation Plan:**
- Step-by-step implementation approach
- Key components and their responsibilities
- Error handling strategy
- Testing approach

**4. Code Quality Considerations:**
- Design patterns to apply
- Code organization principles
- Documentation standards
- Maintainability factors

**5. Security Assessment:**
- Security requirements and considerations
- Potential vulnerabilities to address
- Authentication and authorization needs
- Data protection measures

**6. Performance Optimization:**
- Performance requirements
- Optimization strategies
- Scalability considerations
- Resource management

**7. Risk Assessment:**
- Technical risks and mitigation strategies
- Alternative approaches if main solution fails
- Monitoring and alerting needs

**8. Testing Strategy:**
- Unit testing approach
- Integration testing plan
- Security testing requirements
- Performance testing needs

Focus on creating a practical, well-reasoned solution that prioritizes security, performance, and maintainability."""

        solution_context = context.model_copy()
        solution_context.task_type = TaskType.ANALYSIS
        
        return await self.generate(solution_prompt, solution_context)
    
    async def research_topic(self, topic: str, focus_areas: List[str], context: TaskContext) -> AgentResponse:
        """Research a technical topic with focus on best practices."""
        focus_text = "\n".join(f"- {area}" for area in focus_areas) if focus_areas else "General overview"
        
        research_prompt = f"""Conduct comprehensive research on the following topic with emphasis on practical implementation and best practices:

**Topic:** {topic}

**Focus Areas:**
{focus_text}

Please provide a thorough research analysis including:

**1. Overview:**
- Definition and key concepts
- Current state and relevance
- Important context and background
- Industry standards

**2. Technical Details:**
- Core technologies and methodologies
- Implementation approaches
- Standards and best practices
- Common pitfalls and how to avoid them

**3. Comparative Analysis:**
- Alternative solutions or approaches
- Pros and cons of different methods
- Use case scenarios for each approach
- Performance and security implications

**4. Best Practices:**
- Industry-standard approaches
- Security considerations
- Performance optimization
- Maintainability guidelines

**5. Implementation Guidance:**
- Step-by-step implementation approaches
- Common integration patterns
- Testing strategies
- Monitoring and debugging

**6. Security Considerations:**
- Security best practices
- Common vulnerabilities
- Protection strategies
- Compliance requirements

**7. Performance Optimization:**
- Performance best practices
- Optimization techniques
- Scalability considerations
- Resource management

**8. Resources and Tools:**
- Recommended tools and frameworks
- Learning resources
- Standards and specifications
- Community resources

Provide accurate, up-to-date information with practical insights for secure and efficient implementation."""

        research_context = context.model_copy()
        research_context.task_type = TaskType.RESEARCH
        research_context.agent_role = "researcher"
        
        return await self.generate(research_prompt, research_context)
    
    async def analyze_security(self, code: str, language: str, context: TaskContext) -> AgentResponse:
        """Perform detailed security analysis."""
        security_prompt = f"""Perform a comprehensive security analysis of the following {language} code:

```{language}
{code}
```

Please provide detailed security assessment covering:

**1. Vulnerability Assessment:**
- Identify potential security vulnerabilities
- OWASP Top 10 considerations
- Language-specific security issues
- Configuration security problems

**2. Input Validation:**
- Input validation weaknesses
- Injection attack vectors (SQL, XSS, etc.)
- Parameter tampering risks
- Data sanitization issues

**3. Authentication & Authorization:**
- Authentication mechanism review
- Authorization logic assessment
- Session management security
- Access control implementation

**4. Data Protection:**
- Data encryption at rest and in transit
- Sensitive data handling
- Privacy considerations
- Data leakage risks

**5. Error Handling:**
- Information disclosure through errors
- Exception handling security
- Logging security considerations
- Debug information exposure

**6. Infrastructure Security:**
- Deployment security considerations
- Configuration management security
- Dependency security assessment
- Environment security

**7. Remediation Recommendations:**
- Specific fixes for identified issues
- Security implementation best practices
- Secure coding guidelines
- Testing recommendations

**8. Risk Assessment:**
- Risk level classification (Critical/High/Medium/Low)
- Impact analysis
- Exploitability assessment
- Mitigation priority

Focus on providing actionable security improvements and specific remediation steps."""

        security_context = context.model_copy()
        security_context.task_type = TaskType.CODE_REVIEW
        security_context.metadata["security_analysis"] = True
        
        return await self.generate(security_prompt, security_context)
    
    async def optimize_performance(self, code: str, language: str, context: TaskContext) -> AgentResponse:
        """Analyze and suggest performance optimizations."""
        optimization_prompt = f"""Analyze the following {language} code for performance optimization opportunities:

```{language}
{code}
```

Please provide comprehensive performance analysis including:

**1. Performance Analysis:**
- Identify performance bottlenecks
- Algorithmic complexity assessment
- Resource usage analysis
- Memory consumption patterns

**2. Optimization Opportunities:**
- Algorithm optimization suggestions
- Data structure improvements
- Caching strategies
- Database query optimization

**3. Scalability Assessment:**
- Horizontal scaling considerations
- Vertical scaling opportunities
- Load handling capabilities
- Resource contention points

**4. Memory Optimization:**
- Memory leak detection
- Memory usage optimization
- Garbage collection considerations
- Resource cleanup improvements

**5. I/O Optimization:**
- File I/O improvements
- Network I/O optimization
- Database access optimization
- Caching implementation

**6. Concurrency Improvements:**
- Parallel processing opportunities
- Async/await optimization
- Thread safety considerations
- Lock contention reduction

**7. Implementation Recommendations:**
- Specific code improvements
- Performance monitoring suggestions
- Benchmarking strategies
- Testing approaches

**8. Trade-off Analysis:**
- Performance vs readability
- Performance vs maintainability
- Memory vs CPU trade-offs
- Optimization priority recommendations

Provide specific, actionable performance improvements with measurable impact estimates."""

        optimization_context = context.model_copy()
        optimization_context.task_type = TaskType.ANALYSIS
        optimization_context.metadata["performance_analysis"] = True
        
        return await self.generate(optimization_prompt, optimization_context)
    
    async def health_check(self) -> Dict[str, Any]:
        """Check OpenAI agent health and connectivity."""
        try:
            start_time = time.time()
            
            # Simple test request
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": "Respond with 'OK' for health check."}
                ],
                max_tokens=5,
                temperature=0.0
            )
            
            response_time = time.time() - start_time
            
            return {
                "status": "healthy",
                "model": self.model,
                "response_time": response_time,
                "token_usage": {
                    "prompt": response.usage.prompt_tokens,
                    "completion": response.usage.completion_tokens,
                    "total": response.usage.total_tokens
                },
                "capabilities": self.capabilities,
                "last_check": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"OpenAI health check failed: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "model": self.model,
                "last_check": time.time()
            }
    
    def _build_messages(self, prompt: str, context: TaskContext) -> List[Dict[str, str]]:
        """Build message list based on context."""
        system_message = self._get_system_message(context)
        
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        
        messages.append({"role": "user", "content": prompt})
        
        return messages
    
    def _get_system_message(self, context: TaskContext) -> Optional[str]:
        """Get system message based on task context."""
        system_messages = {
            TaskType.CODE_REVIEW: "You are an expert code reviewer specializing in security, performance, and best practices. Provide thorough, constructive feedback with specific, actionable recommendations.",
            TaskType.ANALYSIS: "You are a senior technical analyst. Provide comprehensive analysis with focus on security, performance, and architectural best practices.",
            TaskType.RESEARCH: "You are a technical researcher specializing in best practices, security, and performance optimization. Provide thorough, practical research with actionable insights.",
            TaskType.GENERAL: "You are a technical expert focused on code quality, security, and best practices. Provide detailed, practical guidance."
        }
        
        return system_messages.get(context.task_type)
    
    def _get_temperature(self, context: TaskContext) -> float:
        """Get temperature based on task context."""
        # Lower temperature for code review and analysis
        if context.task_type in [TaskType.CODE_REVIEW, TaskType.ANALYSIS]:
            return 0.3
        # Higher temperature for creative tasks
        elif context.task_type == TaskType.CREATIVE:
            return 0.8
        # Default temperature
        else:
            return self.temperature
    
    def _calculate_confidence(self, response: ChatCompletion) -> float:
        """Calculate confidence score based on response quality."""
        confidence = 0.8  # Base confidence
        
        # Adjust based on finish reason
        finish_reason = response.choices[0].finish_reason
        if finish_reason == "stop":
            confidence += 0.1
        elif finish_reason == "length":
            confidence -= 0.1
        
        # Adjust based on response length (longer usually indicates more thorough analysis)
        content_length = len(response.choices[0].message.content or "")
        if content_length > 1000:
            confidence += 0.1
        elif content_length < 100:
            confidence -= 0.2
        
        return min(1.0, max(0.0, confidence))
</file>

<file path="src/orchestration/__init__.py">
"""
Orchestration package for AngelaMCP.

This package contains the orchestration engine that manages multi-agent collaboration,
task routing, debate protocols, and consensus building across Claude Code, OpenAI, and Gemini agents.
"""

from .orchestrator import TaskOrchestrator, OrchestrationEngine
from .debate import DebateProtocol, DebateRound, DebateResult
from .voting import VotingSystem, VoteResult, ConsensusBuilder

__all__ = [
    "TaskOrchestrator",
    "OrchestrationEngine", 
    "DebateProtocol",
    "DebateRound",
    "DebateResult",
    "VotingSystem",
    "VoteResult",
    "ConsensusBuilder"
]
</file>

<file path="src/orchestration/debate.py">
"""
Debate protocol for AngelaMCP multi-agent collaboration.

This module implements structured debate protocols that allow agents to engage
in productive disagreement, present arguments, counter-arguments, and reach
consensus through iterative discussion. I'm building a system that leverages
the different strengths and perspectives of each agent type.
"""

import asyncio
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
from pydantic import BaseModel, Field

from src.agents.base import BaseAgent, AgentResponse, TaskContext, AgentRole, agent_registry
from src.utils.logger import get_logger, log_context, AsyncPerformanceLogger

logger = get_logger("orchestration.debate")


class DebateRole(str, Enum):
    """Roles agents can play in a debate."""
    PROPOSER = "proposer"        # Agent proposing initial solution
    CHALLENGER = "challenger"    # Agent challenging the proposal
    MODERATOR = "moderator"      # Agent moderating the debate
    SYNTHESIZER = "synthesizer"  # Agent creating final synthesis


class ArgumentType(str, Enum):
    """Types of arguments in a debate."""
    INITIAL_PROPOSAL = "initial_proposal"
    COUNTER_ARGUMENT = "counter_argument"
    REBUTTAL = "rebuttal"
    CLARIFICATION = "clarification"
    EVIDENCE = "evidence"
    SYNTHESIS = "synthesis"


@dataclass
class DebateArgument:
    """Represents a single argument in a debate."""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    agent_name: str = ""
    agent_type: str = ""
    role: DebateRole = DebateRole.PROPOSER
    argument_type: ArgumentType = ArgumentType.INITIAL_PROPOSAL
    content: str = ""
    confidence_score: Optional[float] = None
    evidence: List[str] = field(default_factory=list)
    addresses_argument_id: Optional[str] = None
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DebateRound:
    """Represents a single round of debate."""
    round_number: int
    arguments: List[DebateArgument] = field(default_factory=list)
    round_summary: str = ""
    consensus_score: float = 0.0
    round_duration_ms: float = 0.0


@dataclass
class DebateResult:
    """Final result of a debate session."""
    debate_id: str
    task_id: str
    success: bool
    final_consensus: str
    confidence_score: float
    rounds: List[DebateRound] = field(default_factory=list)
    participating_agents: List[str] = field(default_factory=list)
    total_duration_ms: float = 0.0
    total_cost_usd: float = 0.0
    total_tokens: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None


class DebateProtocol:
    """
    Structured debate protocol for multi-agent collaboration.
    
    I'm implementing a sophisticated debate system that allows agents to:
    1. Present initial proposals
    2. Challenge and counter-argue
    3. Provide evidence and clarification
    4. Reach consensus through structured discussion
    """
    
    def __init__(self, orchestrator, task: 'OrchestrationTask'):
        self.orchestrator = orchestrator
        self.task = task
        self.logger = get_logger("orchestration.debate_protocol")
        
        # Debate configuration
        self.max_rounds = task.max_debate_rounds
        self.consensus_threshold = task.consensus_threshold
        self.timeout_seconds = task.timeout_seconds or 300  # 5 minutes default
        
        # Debate state
        self.debate_id = str(uuid.uuid4())
        self.rounds: List[DebateRound] = []
        self.arguments: List[DebateArgument] = []
        self.participating_agents: List[BaseAgent] = []
        
        # Performance tracking
        self.start_time = 0.0
        self.total_cost = 0.0
        self.total_tokens = 0
        
    def _select_debate_agents(self) -> List[BaseAgent]:
        """Select agents for debate based on diversity and capability."""
        available_agents = agent_registry.get_all_agents()
        
        if len(available_agents) < 2:
            raise ValueError("At least 2 agents required for debate")
        
        # Prefer diverse agent types for better debate
        selected_agents = []
        agent_types_used = set()
        
        # First, select one agent of each type if available
        for agent in available_agents:
            if agent.agent_type not in agent_types_used and len(selected_agents) < 3:
                selected_agents.append(agent)
                agent_types_used.add(agent.agent_type)
        
        # If we need more agents, add based on performance
        if len(selected_agents) < 3 and len(available_agents) > len(selected_agents):
            remaining_agents = [
                agent for agent in available_agents 
                if agent not in selected_agents
            ]
            # Sort by performance and add best remaining
            remaining_agents.sort(
                key=lambda a: a.performance_metrics.get("success_rate", 0.5),
                reverse=True
            )
            selected_agents.extend(remaining_agents[:3 - len(selected_agents)])
        
        self.logger.info(f"Selected {len(selected_agents)} agents for debate: {[a.name for a in selected_agents]}")
        return selected_agents
    
    def _assign_debate_roles(self, agents: List[BaseAgent]) -> Dict[BaseAgent, DebateRole]:
        """Assign roles to agents based on their strengths."""
        roles = {}
        
        if len(agents) >= 2:
            # Assign proposer to agent best suited for the task type
            weights = self.orchestrator._agent_weights
            best_proposer = max(
                agents,
                key=lambda a: weights.get(a.agent_type, {}).get(self.task.task_type, 0.5)
            )
            roles[best_proposer] = DebateRole.PROPOSER
            
            # Assign challenger to a different agent type if possible
            remaining_agents = [a for a in agents if a != best_proposer]
            challenger = next(
                (a for a in remaining_agents if a.agent_type != best_proposer.agent_type),
                remaining_agents[0] if remaining_agents else None
            )
            if challenger:
                roles[challenger] = DebateRole.CHALLENGER
            
            # Assign synthesizer if we have a third agent
            if len(agents) >= 3:
                synthesizer_candidates = [a for a in agents if a not in roles]
                if synthesizer_candidates:
                    roles[synthesizer_candidates[0]] = DebateRole.SYNTHESIZER
        
        return roles
    
    async def _get_initial_proposal(self, proposer: BaseAgent) -> DebateArgument:
        """Get initial proposal from the proposer agent."""
        context = TaskContext(
            task_id=f"{self.task.task_id}_proposal",
            task_type=self.task.task_type,
            conversation_id=self.task.conversation_id,
            session_id=self.task.session_id,
            agent_role=AgentRole.PRIMARY,
            max_tokens=self.task.max_tokens,
            timeout_seconds=self.task.timeout_seconds
        )
        
        proposal_prompt = f"""You are participating in a structured debate about this task:

{self.task.description}

As the PROPOSER, please provide your initial solution or approach. Your proposal should include:

1. **Core Solution**: Your main approach to solving this task
2. **Reasoning**: Why you believe this is the best approach
3. **Evidence**: Any supporting arguments or examples
4. **Potential Concerns**: What challenges or limitations you acknowledge
5. **Implementation Details**: Specific steps or considerations

Be thorough but concise. This proposal will be challenged by other agents, so make it robust and well-reasoned."""

        response = await proposer.generate(proposal_prompt, context)
        
        # Track cost and tokens
        if response.cost_usd:
            self.total_cost += response.cost_usd
        if response.tokens_used:
            self.total_tokens += response.tokens_used
        
        return DebateArgument(
            agent_name=proposer.name,
            agent_type=proposer.agent_type.value,
            role=DebateRole.PROPOSER,
            argument_type=ArgumentType.INITIAL_PROPOSAL,
            content=response.content,
            confidence_score=response.confidence_score,
            metadata={"response_metadata": response.metadata}
        )
    
    async def _get_counter_argument(self, challenger: BaseAgent, proposal: DebateArgument) -> DebateArgument:
        """Get counter-argument from challenger agent."""
        context = TaskContext(
            task_id=f"{self.task.task_id}_challenge",
            task_type=self.task.task_type,
            conversation_id=self.task.conversation_id,
            session_id=self.task.session_id,
            agent_role=AgentRole.REVIEWER,
            max_tokens=self.task.max_tokens,
            timeout_seconds=self.task.timeout_seconds
        )
        
        challenge_prompt = f"""You are participating in a structured debate. Here is the original task:

{self.task.description}

The PROPOSER has suggested this solution:

{proposal.content}

As the CHALLENGER, please provide a thorough critique and alternative approach. Your response should include:

1. **Critical Analysis**: What are the weaknesses or problems with the proposed solution?
2. **Alternative Approach**: What different solution or modifications do you recommend?
3. **Supporting Evidence**: Why is your approach better or more suitable?
4. **Risk Assessment**: What risks does the original proposal have that your approach avoids?
5. **Implementation Advantages**: How is your approach easier, faster, or more reliable?

Be constructive but rigorous in your critique. Present a compelling alternative that addresses the task better."""

        response = await challenger.generate(challenge_prompt, context)
        
        # Track cost and tokens
        if response.cost_usd:
            self.total_cost += response.cost_usd
        if response.tokens_used:
            self.total_tokens += response.tokens_used
        
        return DebateArgument(
            agent_name=challenger.name,
            agent_type=challenger.agent_type.value,
            role=DebateRole.CHALLENGER,
            argument_type=ArgumentType.COUNTER_ARGUMENT,
            content=response.content,
            confidence_score=response.confidence_score,
            addresses_argument_id=proposal.id,
            metadata={"response_metadata": response.metadata}
        )
    
    async def _get_rebuttal(self, proposer: BaseAgent, proposal: DebateArgument, 
                          counter_argument: DebateArgument) -> DebateArgument:
        """Get rebuttal from original proposer."""
        context = TaskContext(
            task_id=f"{self.task.task_id}_rebuttal",
            task_type=self.task.task_type,
            conversation_id=self.task.conversation_id,
            session_id=self.task.session_id,
            agent_role=AgentRole.PRIMARY,
            max_tokens=self.task.max_tokens,
            timeout_seconds=self.task.timeout_seconds
        )
        
        rebuttal_prompt = f"""You are continuing a structured debate. Here is the original task:

{self.task.description}

Your original proposal was:

{proposal.content}

The CHALLENGER responded with this critique and alternative:

{counter_argument.content}

As the original PROPOSER, please provide a rebuttal that:

1. **Defends Your Approach**: Address the specific criticisms raised
2. **Identifies Flaws**: Point out problems with the challenger's alternative
3. **Provides Evidence**: Offer concrete examples or reasoning for your position
4. **Refined Solution**: Improve your original proposal based on valid points
5. **Comparative Analysis**: Explain why your approach is still superior overall

Be open to valid criticism but defend your position where you believe it's stronger."""

        response = await proposer.generate(rebuttal_prompt, context)
        
        # Track cost and tokens
        if response.cost_usd:
            self.total_cost += response.cost_usd
        if response.tokens_used:
            self.total_tokens += response.tokens_used
        
        return DebateArgument(
            agent_name=proposer.name,
            agent_type=proposer.agent_type.value,
            role=DebateRole.PROPOSER,
            argument_type=ArgumentType.REBUTTAL,
            content=response.content,
            confidence_score=response.confidence_score,
            addresses_argument_id=counter_argument.id,
            metadata={"response_metadata": response.metadata}
        )
    
    async def _synthesize_consensus(self, synthesizer: BaseAgent, 
                                  arguments: List[DebateArgument]) -> DebateArgument:
        """Create synthesis from all arguments."""
        context = TaskContext(
            task_id=f"{self.task.task_id}_synthesis",
            task_type=self.task.task_type,
            conversation_id=self.task.conversation_id,
            session_id=self.task.session_id,
            agent_role=AgentRole.SPECIALIST,
            max_tokens=self.task.max_tokens,
            timeout_seconds=self.task.timeout_seconds
        )
        
        # Build comprehensive prompt with all arguments
        arguments_text = ""
        for i, arg in enumerate(arguments, 1):
            arguments_text += f"""
=== Argument {i} ({arg.role.value} - {arg.argument_type.value}) ===
Agent: {arg.agent_name}
Content: {arg.content}

"""
        
        synthesis_prompt = f"""You are synthesizing a debate about this task:

{self.task.description}

Here are all the arguments presented during the debate:

{arguments_text}

As the SYNTHESIZER, create a final consensus solution that:

1. **Integrated Solution**: Combines the best elements from all perspectives
2. **Addresses Concerns**: Resolves the main criticisms and concerns raised
3. **Balanced Approach**: Incorporates valid points from all participants
4. **Implementation Plan**: Provides a clear, actionable solution
5. **Risk Mitigation**: Addresses potential problems identified in the debate
6. **Consensus Reasoning**: Explains how this synthesis resolves the disagreements

Create a solution that all participants could reasonably accept while solving the original task effectively."""

        response = await synthesizer.generate(synthesis_prompt, context)
        
        # Track cost and tokens
        if response.cost_usd:
            self.total_cost += response.cost_usd
        if response.tokens_used:
            self.total_tokens += response.tokens_used
        
        return DebateArgument(
            agent_name=synthesizer.name,
            agent_type=synthesizer.agent_type.value,
            role=DebateRole.SYNTHESIZER,
            argument_type=ArgumentType.SYNTHESIS,
            content=response.content,
            confidence_score=response.confidence_score,
            metadata={"response_metadata": response.metadata}
        )
    
    def _calculate_consensus_score(self, arguments: List[DebateArgument]) -> float:
        """Calculate consensus score based on argument patterns."""
        if len(arguments) < 2:
            return 0.0
        
        # Simple heuristic: higher confidence scores indicate stronger consensus
        confidence_scores = [arg.confidence_score for arg in arguments if arg.confidence_score is not None]
        
        if not confidence_scores:
            return 0.5  # Default moderate consensus
        
        # Average confidence across all arguments
        avg_confidence = sum(confidence_scores) / len(confidence_scores)
        
        # Bonus for synthesis arguments (indicates resolution)
        synthesis_bonus = 0.2 if any(arg.argument_type == ArgumentType.SYNTHESIS for arg in arguments) else 0.0
        
        # Penalty for many counter-arguments (indicates ongoing disagreement)
        counter_penalty = len([arg for arg in arguments if arg.argument_type == ArgumentType.COUNTER_ARGUMENT]) * 0.1
        
        consensus_score = min(1.0, max(0.0, avg_confidence + synthesis_bonus - counter_penalty))
        
        return consensus_score
    
    async def execute_debate(self) -> 'TaskResult':
        """Execute the full debate protocol."""
        from .orchestrator import TaskResult, OrchestrationStrategy
        
        self.start_time = time.time()
        
        with log_context(self.debate_id, self.task.session_id):
            self.logger.info(f"Starting debate for task {self.task.task_id}")
            
            try:
                # Select and assign agents
                agents = self._select_debate_agents()
                roles = self._assign_debate_roles(agents)
                self.participating_agents = agents
                
                if len(roles) < 2:
                    raise ValueError("Need at least 2 agents with assigned roles for debate")
                
                proposer = next(agent for agent, role in roles.items() if role == DebateRole.PROPOSER)
                challenger = next(agent for agent, role in roles.items() if role == DebateRole.CHALLENGER)
                synthesizer = next((agent for agent, role in roles.items() if role == DebateRole.SYNTHESIZER), None)
                
                # Execute debate rounds
                for round_num in range(1, self.max_rounds + 1):
                    round_start = time.time()
                    round_arguments = []
                    
                    self.logger.info(f"Starting debate round {round_num}")
                    
                    if round_num == 1:
                        # Initial proposal
                        async with AsyncPerformanceLogger(self.logger, f"debate_proposal", round=round_num):
                            proposal = await self._get_initial_proposal(proposer)
                        round_arguments.append(proposal)
                        self.arguments.append(proposal)
                        
                        # Counter-argument
                        async with AsyncPerformanceLogger(self.logger, f"debate_challenge", round=round_num):
                            counter = await self._get_counter_argument(challenger, proposal)
                        round_arguments.append(counter)
                        self.arguments.append(counter)
                    else:
                        # Subsequent rounds: rebuttals and clarifications
                        last_argument = self.arguments[-1]
                        if last_argument.role == DebateRole.CHALLENGER:
                            # Proposer rebuts
                            async with AsyncPerformanceLogger(self.logger, f"debate_rebuttal", round=round_num):
                                rebuttal = await self._get_rebuttal(proposer, self.arguments[0], last_argument)
                            round_arguments.append(rebuttal)
                            self.arguments.append(rebuttal)
                        else:
                            # Challenger provides follow-up
                            async with AsyncPerformanceLogger(self.logger, f"debate_followup", round=round_num):
                                followup = await self._get_counter_argument(challenger, last_argument)
                            round_arguments.append(followup)
                            self.arguments.append(followup)
                    
                    # Calculate round consensus
                    consensus_score = self._calculate_consensus_score(round_arguments)
                    round_duration = (time.time() - round_start) * 1000
                    
                    # Create round summary
                    round_summary = f"Round {round_num}: {len(round_arguments)} arguments, consensus: {consensus_score:.2f}"
                    
                    debate_round = DebateRound(
                        round_number=round_num,
                        arguments=round_arguments,
                        round_summary=round_summary,
                        consensus_score=consensus_score,
                        round_duration_ms=round_duration
                    )
                    self.rounds.append(debate_round)
                    
                    self.logger.info(f"Completed round {round_num} - Consensus: {consensus_score:.2f}")
                    
                    # Check for early consensus
                    if consensus_score >= self.consensus_threshold:
                        self.logger.info(f"Early consensus reached at round {round_num}")
                        break
                
                # Final synthesis if we have a synthesizer
                final_content = ""
                final_confidence = 0.0
                
                if synthesizer and len(self.arguments) >= 2:
                    self.logger.info("Creating final synthesis")
                    async with AsyncPerformanceLogger(self.logger, "debate_synthesis"):
                        synthesis = await self._synthesize_consensus(synthesizer, self.arguments)
                    self.arguments.append(synthesis)
                    final_content = synthesis.content
                    final_confidence = synthesis.confidence_score or 0.0
                else:
                    # Use the last argument as final result
                    last_arg = self.arguments[-1] if self.arguments else None
                    if last_arg:
                        final_content = last_arg.content
                        final_confidence = last_arg.confidence_score or 0.0
                
                # Calculate final consensus
                final_consensus_score = self._calculate_consensus_score(self.arguments)
                total_duration = (time.time() - self.start_time) * 1000
                
                # Create debate result
                debate_result = DebateResult(
                    debate_id=self.debate_id,
                    task_id=self.task.task_id,
                    success=final_consensus_score >= self.consensus_threshold,
                    final_consensus=final_content,
                    confidence_score=final_confidence,
                    rounds=self.rounds,
                    participating_agents=[agent.name for agent in self.participating_agents],
                    total_duration_ms=total_duration,
                    total_cost_usd=self.total_cost,
                    total_tokens=self.total_tokens,
                    metadata={
                        "final_consensus_score": final_consensus_score,
                        "total_arguments": len(self.arguments),
                        "roles_assigned": {agent.name: role.value for agent, role in roles.items()}
                    }
                )
                
                # Convert to TaskResult
                task_result = TaskResult(
                    task_id=self.task.task_id,
                    success=debate_result.success,
                    content=debate_result.final_consensus,
                    execution_time_ms=debate_result.total_duration_ms,
                    total_cost_usd=debate_result.total_cost_usd,
                    total_tokens=debate_result.total_tokens,
                    strategy_used=OrchestrationStrategy.DEBATE,
                    metadata={
                        "debate_result": debate_result,
                        "consensus_score": final_consensus_score,
                        "rounds_completed": len(self.rounds),
                        "agents_participated": debate_result.participating_agents
                    }
                )
                
                self.logger.info(
                    f"Debate completed - Success: {debate_result.success}, "
                    f"Consensus: {final_consensus_score:.2f}, "
                    f"Rounds: {len(self.rounds)}, "
                    f"Cost: ${self.total_cost:.4f}"
                )
                
                return task_result
                
            except Exception as e:
                total_duration = (time.time() - self.start_time) * 1000
                self.logger.error(f"Debate execution failed: {e}")
                
                return TaskResult(
                    task_id=self.task.task_id,
                    success=False,
                    content="",
                    execution_time_ms=total_duration,
                    total_cost_usd=self.total_cost,
                    total_tokens=self.total_tokens,
                    strategy_used=OrchestrationStrategy.DEBATE,
                    error_message=str(e),
                    metadata={
                        "debate_id": self.debate_id,
                        "rounds_completed": len(self.rounds),
                        "arguments_made": len(self.arguments)
                    }
                )
</file>

<file path="src/orchestration/orchestrator.py">
"""
Task orchestrator for AngelaMCP multi-agent collaboration.

This module implements the core orchestration engine that manages task distribution,
agent coordination, and workflow execution across Claude Code, OpenAI, and Gemini agents.
I'm building a production-grade system that can handle complex multi-step tasks with
intelligent agent selection and fallback mechanisms.
"""

import asyncio
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Union, Callable
from pydantic import BaseModel, Field

from src.agents.base import (
    BaseAgent, AgentType, AgentResponse, TaskContext, TaskType, AgentRole,
    agent_registry
)
from src.agents.claude_agent import ClaudeCodeAgent
from src.agents.openai_agent import OpenAIAgent
from src.agents.gemini_agent import GeminiAgent
from src.persistence.database import DatabaseManager
from src.persistence.models import Conversation, Message, TaskExecution
from src.utils.logger import get_logger, log_context, AsyncPerformanceLogger
from config.settings import settings

logger = get_logger("orchestration.orchestrator")


class TaskPriority(str, Enum):
    """Task priority levels for orchestration."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    URGENT = "urgent"


class TaskStatus(str, Enum):
    """Task execution status."""
    PENDING = "pending"
    ASSIGNED = "assigned"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class OrchestrationStrategy(str, Enum):
    """Strategies for agent orchestration."""
    SINGLE_AGENT = "single_agent"          # Use best agent for task
    PARALLEL = "parallel"                  # Run multiple agents in parallel
    SEQUENTIAL = "sequential"              # Run agents in sequence
    DEBATE = "debate"                      # Enable debate between agents
    CONSENSUS = "consensus"                # Require consensus from multiple agents


@dataclass
class TaskResult:
    """Result of a task execution."""
    task_id: str
    success: bool
    content: str
    agent_responses: List[AgentResponse] = field(default_factory=list)
    execution_time_ms: float = 0.0
    total_cost_usd: float = 0.0
    total_tokens: int = 0
    strategy_used: Optional[OrchestrationStrategy] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None


class OrchestrationTask(BaseModel):
    """Task definition for orchestration."""
    task_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    description: str = Field(description="Task description or prompt")
    task_type: TaskType = Field(description="Type of task")
    priority: TaskPriority = Field(default=TaskPriority.MEDIUM)
    
    # Strategy configuration
    strategy: OrchestrationStrategy = Field(default=OrchestrationStrategy.SINGLE_AGENT)
    preferred_agents: List[AgentType] = Field(default_factory=list)
    required_capabilities: List[str] = Field(default_factory=list)
    constraints: List[str] = Field(default_factory=list)
    
    # Collaboration settings
    enable_debate: bool = Field(default=False)
    max_debate_rounds: int = Field(default=3)
    require_consensus: bool = Field(default=False)
    consensus_threshold: float = Field(default=0.7)
    
    # Resource constraints
    max_tokens: Optional[int] = Field(None)
    max_cost_usd: Optional[float] = Field(None)
    timeout_seconds: Optional[int] = Field(None)
    
    # Context
    conversation_id: Optional[str] = Field(None)
    session_id: Optional[str] = Field(None)
    context_data: Dict[str, Any] = Field(default_factory=dict)


class TaskOrchestrator:
    """
    Core task orchestrator for managing multi-agent collaboration.
    
    I'm implementing intelligent task routing, agent selection, and coordination
    to maximize the effectiveness of multi-agent problem solving.
    """
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
        self.logger = get_logger("orchestration.task_orchestrator")
        
        # Task tracking
        self._active_tasks: Dict[str, OrchestrationTask] = {}
        self._task_results: Dict[str, TaskResult] = {}
        
        # Agent selection weights (higher = preferred)
        self._agent_weights = {
            AgentType.CLAUDE_CODE: {
                TaskType.CODE_GENERATION: 1.0,
                TaskType.CODE_REVIEW: 0.8,
                TaskType.DEBUGGING: 1.0,
                TaskType.TESTING: 0.9,
                TaskType.DOCUMENTATION: 0.7,
                TaskType.ANALYSIS: 0.6,
                TaskType.RESEARCH: 0.4,
                TaskType.COLLABORATION: 0.8,
                TaskType.CUSTOM: 0.7
            },
            AgentType.OPENAI: {
                TaskType.CODE_GENERATION: 0.7,
                TaskType.CODE_REVIEW: 1.0,
                TaskType.DEBUGGING: 0.8,
                TaskType.TESTING: 0.7,
                TaskType.DOCUMENTATION: 0.8,
                TaskType.ANALYSIS: 1.0,
                TaskType.RESEARCH: 0.8,
                TaskType.COLLABORATION: 0.9,
                TaskType.CUSTOM: 0.8
            },
            AgentType.GEMINI: {
                TaskType.CODE_GENERATION: 0.6,
                TaskType.CODE_REVIEW: 0.7,
                TaskType.DEBUGGING: 0.6,
                TaskType.TESTING: 0.6,
                TaskType.DOCUMENTATION: 0.9,
                TaskType.ANALYSIS: 0.8,
                TaskType.RESEARCH: 1.0,
                TaskType.COLLABORATION: 0.7,
                TaskType.CUSTOM: 0.9
            }
        }
        
        # Performance tracking
        self._total_tasks = 0
        self._successful_tasks = 0
        self._total_cost = 0.0
        self._start_time = time.time()
        
        self.logger.info("Task orchestrator initialized")
    
    def _select_best_agent(self, task: OrchestrationTask) -> Optional[BaseAgent]:
        """Select the best agent for a given task."""
        available_agents = agent_registry.get_all_agents()
        
        if not available_agents:
            self.logger.error("No agents available for task execution")
            return None
        
        # Filter by preferred agents if specified
        if task.preferred_agents:
            available_agents = [
                agent for agent in available_agents
                if agent.agent_type in task.preferred_agents
            ]
        
        # Filter by required capabilities
        if task.required_capabilities:
            available_agents = [
                agent for agent in available_agents
                if all(agent.supports_capability(cap) for cap in task.required_capabilities)
            ]
        
        if not available_agents:
            self.logger.warning("No agents match task requirements")
            return None
        
        # Score agents based on task type and capabilities
        agent_scores = []
        for agent in available_agents:
            base_score = self._agent_weights.get(agent.agent_type, {}).get(task.task_type, 0.5)
            
            # Bonus for required capabilities
            capability_bonus = 0.1 * len([
                cap for cap in task.required_capabilities
                if agent.supports_capability(cap)
            ])
            
            # Performance-based adjustment
            metrics = agent.performance_metrics
            if metrics["total_requests"] > 0:
                success_rate = (metrics["total_requests"] - 
                              metrics.get("failed_requests", 0)) / metrics["total_requests"]
                performance_bonus = (success_rate - 0.8) * 0.2  # Bonus for >80% success rate
            else:
                performance_bonus = 0
            
            total_score = base_score + capability_bonus + performance_bonus
            agent_scores.append((agent, total_score))
            
            self.logger.debug(
                f"Agent {agent.name} score: {total_score:.3f} "
                f"(base: {base_score}, capability: {capability_bonus}, "
                f"performance: {performance_bonus})"
            )
        
        # Sort by score and return best agent
        agent_scores.sort(key=lambda x: x[1], reverse=True)
        best_agent = agent_scores[0][0]
        
        self.logger.info(
            f"Selected agent {best_agent.name} for task {task.task_id} "
            f"(score: {agent_scores[0][1]:.3f})"
        )
        
        return best_agent
    
    def _select_multiple_agents(self, task: OrchestrationTask, count: int) -> List[BaseAgent]:
        """Select multiple agents for parallel or collaborative execution."""
        available_agents = agent_registry.get_all_agents()
        
        if len(available_agents) < count:
            self.logger.warning(f"Only {len(available_agents)} agents available, requested {count}")
            return available_agents
        
        # Use similar scoring logic as single agent selection
        agent_scores = []
        for agent in available_agents:
            base_score = self._agent_weights.get(agent.agent_type, {}).get(task.task_type, 0.5)
            
            # Add diversity bonus to encourage different agent types
            type_penalty = sum(1 for scored_agent, _ in agent_scores 
                             if scored_agent.agent_type == agent.agent_type) * 0.1
            
            total_score = base_score - type_penalty
            agent_scores.append((agent, total_score))
        
        # Sort and take top agents
        agent_scores.sort(key=lambda x: x[1], reverse=True)
        selected_agents = [agent for agent, _ in agent_scores[:count]]
        
        self.logger.info(
            f"Selected {len(selected_agents)} agents for collaborative task: "
            f"{[agent.name for agent in selected_agents]}"
        )
        
        return selected_agents
    
    async def _execute_single_agent(self, task: OrchestrationTask, agent: BaseAgent) -> TaskResult:
        """Execute task with a single agent."""
        start_time = time.time()
        
        try:
            # Create task context
            context = TaskContext(
                task_id=task.task_id,
                task_type=task.task_type,
                conversation_id=task.conversation_id,
                session_id=task.session_id,
                agent_role=AgentRole.PRIMARY,
                max_tokens=task.max_tokens,
                timeout_seconds=task.timeout_seconds,
                context_data=task.context_data
            )
            
            # Execute task
            async with AsyncPerformanceLogger(
                self.logger, f"single_agent_execution", 
                task_id=task.task_id, agent=agent.name
            ):
                response = await agent.generate(task.description, context)
            
            execution_time = (time.time() - start_time) * 1000
            
            # Create result
            result = TaskResult(
                task_id=task.task_id,
                success=response.success,
                content=response.content,
                agent_responses=[response],
                execution_time_ms=execution_time,
                total_cost_usd=response.cost_usd or 0.0,
                total_tokens=response.tokens_used or 0,
                strategy_used=OrchestrationStrategy.SINGLE_AGENT,
                metadata={
                    "agent_used": agent.name,
                    "agent_type": agent.agent_type.value
                },
                error_message=response.error_message
            )
            
            return result
            
        except Exception as e:
            execution_time = (time.time() - start_time) * 1000
            self.logger.error(f"Single agent execution failed: {e}")
            
            return TaskResult(
                task_id=task.task_id,
                success=False,
                content="",
                execution_time_ms=execution_time,
                strategy_used=OrchestrationStrategy.SINGLE_AGENT,
                error_message=str(e),
                metadata={"agent_used": agent.name}
            )
    
    async def _execute_parallel_agents(self, task: OrchestrationTask, agents: List[BaseAgent]) -> TaskResult:
        """Execute task with multiple agents in parallel."""
        start_time = time.time()
        
        try:
            # Create tasks for each agent
            agent_tasks = []
            for i, agent in enumerate(agents):
                context = TaskContext(
                    task_id=f"{task.task_id}_parallel_{i}",
                    task_type=task.task_type,
                    conversation_id=task.conversation_id,
                    session_id=task.session_id,
                    agent_role=AgentRole.PRIMARY,
                    max_tokens=task.max_tokens,
                    timeout_seconds=task.timeout_seconds,
                    context_data=task.context_data
                )
                
                agent_tasks.append(agent.generate(task.description, context))
            
            # Execute all tasks in parallel
            async with AsyncPerformanceLogger(
                self.logger, f"parallel_agent_execution",
                task_id=task.task_id, agent_count=len(agents)
            ):
                responses = await asyncio.gather(*agent_tasks, return_exceptions=True)
            
            execution_time = (time.time() - start_time) * 1000
            
            # Process responses
            valid_responses = []
            total_cost = 0.0
            total_tokens = 0
            
            for i, response in enumerate(responses):
                if isinstance(response, Exception):
                    self.logger.error(f"Agent {agents[i].name} failed: {response}")
                    continue
                
                valid_responses.append(response)
                if response.cost_usd:
                    total_cost += response.cost_usd
                if response.tokens_used:
                    total_tokens += response.tokens_used
            
            if not valid_responses:
                return TaskResult(
                    task_id=task.task_id,
                    success=False,
                    content="All parallel agents failed",
                    execution_time_ms=execution_time,
                    strategy_used=OrchestrationStrategy.PARALLEL,
                    error_message="All agents failed"
                )
            
            # Combine responses (take the best one or combine them)
            best_response = max(valid_responses, key=lambda r: 1 if r.success else 0)
            
            # Create combined content
            if len(valid_responses) > 1:
                combined_content = f"Combined results from {len(valid_responses)} agents:\n\n"
                for i, response in enumerate(valid_responses):
                    combined_content += f"=== Agent {agents[i].name} ===\n{response.content}\n\n"
            else:
                combined_content = best_response.content
            
            result = TaskResult(
                task_id=task.task_id,
                success=any(r.success for r in valid_responses),
                content=combined_content,
                agent_responses=valid_responses,
                execution_time_ms=execution_time,
                total_cost_usd=total_cost,
                total_tokens=total_tokens,
                strategy_used=OrchestrationStrategy.PARALLEL,
                metadata={
                    "agents_used": [agent.name for agent in agents],
                    "successful_responses": len([r for r in valid_responses if r.success]),
                    "total_responses": len(valid_responses)
                }
            )
            
            return result
            
        except Exception as e:
            execution_time = (time.time() - start_time) * 1000
            self.logger.error(f"Parallel agent execution failed: {e}")
            
            return TaskResult(
                task_id=task.task_id,
                success=False,
                content="",
                execution_time_ms=execution_time,
                strategy_used=OrchestrationStrategy.PARALLEL,
                error_message=str(e)
            )
    
    async def execute_task(self, task: OrchestrationTask) -> TaskResult:
        """Execute a task using the appropriate orchestration strategy."""
        self._active_tasks[task.task_id] = task
        self._total_tasks += 1
        
        with log_context(task.task_id, task.session_id):
            self.logger.info(
                f"Executing task {task.task_id} with strategy {task.strategy.value}",
                extra={
                    "task_type": task.task_type.value,
                    "priority": task.priority.value,
                    "strategy": task.strategy.value
                }
            )
            
            try:
                if task.strategy == OrchestrationStrategy.SINGLE_AGENT:
                    agent = self._select_best_agent(task)
                    if not agent:
                        raise ValueError("No suitable agent found for task")
                    result = await self._execute_single_agent(task, agent)
                
                elif task.strategy == OrchestrationStrategy.PARALLEL:
                    agents = self._select_multiple_agents(task, 2)
                    if not agents:
                        raise ValueError("No agents available for parallel execution")
                    result = await self._execute_parallel_agents(task, agents)
                
                elif task.strategy == OrchestrationStrategy.DEBATE:
                    # Import here to avoid circular imports
                    from .debate import DebateProtocol
                    debate = DebateProtocol(self, task)
                    result = await debate.execute_debate()
                
                else:
                    # Fallback to single agent
                    self.logger.warning(f"Strategy {task.strategy.value} not implemented, using single agent")
                    agent = self._select_best_agent(task)
                    if not agent:
                        raise ValueError("No suitable agent found for task")
                    result = await self._execute_single_agent(task, agent)
                
                # Store result and update metrics
                self._task_results[task.task_id] = result
                if result.success:
                    self._successful_tasks += 1
                self._total_cost += result.total_cost_usd
                
                # Persist to database
                await self._persist_task_execution(task, result)
                
                self.logger.info(
                    f"Task {task.task_id} completed - Success: {result.success}, "
                    f"Cost: ${result.total_cost_usd:.4f}, Time: {result.execution_time_ms:.1f}ms"
                )
                
                return result
                
            except Exception as e:
                self.logger.error(f"Task execution failed: {e}")
                result = TaskResult(
                    task_id=task.task_id,
                    success=False,
                    content="",
                    error_message=str(e)
                )
                self._task_results[task.task_id] = result
                return result
            
            finally:
                # Clean up active task
                self._active_tasks.pop(task.task_id, None)
    
    async def _persist_task_execution(self, task: OrchestrationTask, result: TaskResult) -> None:
        """Persist task execution results to database."""
        try:
            async with self.db.get_session() as session:
                # Create task execution record
                task_execution = TaskExecution(
                    id=str(uuid.uuid4()),
                    conversation_id=task.conversation_id,
                    task_type=task.task_type.value,
                    task_description=task.description[:1000],  # Truncate for storage
                    orchestration_strategy=task.strategy.value,
                    agents_used=[resp.agent_type for resp in result.agent_responses],
                    success=result.success,
                    execution_time_ms=result.execution_time_ms,
                    total_cost_usd=result.total_cost_usd,
                    total_tokens_used=result.total_tokens,
                    result_content=result.content[:5000],  # Truncate for storage
                    error_message=result.error_message,
                    metadata_json=result.metadata
                )
                
                session.add(task_execution)
                await session.commit()
                
        except Exception as e:
            self.logger.error(f"Failed to persist task execution: {e}")
    
    def get_task_result(self, task_id: str) -> Optional[TaskResult]:
        """Get result for a completed task."""
        return self._task_results.get(task_id)
    
    def get_active_tasks(self) -> List[OrchestrationTask]:
        """Get list of currently active tasks."""
        return list(self._active_tasks.values())
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get orchestrator performance metrics."""
        uptime = time.time() - self._start_time
        success_rate = self._successful_tasks / max(self._total_tasks, 1)
        
        return {
            "total_tasks": self._total_tasks,
            "successful_tasks": self._successful_tasks,
            "success_rate": success_rate,
            "total_cost_usd": self._total_cost,
            "average_cost_per_task": self._total_cost / max(self._total_tasks, 1),
            "uptime_seconds": uptime,
            "tasks_per_minute": (self._total_tasks / max(uptime / 60, 1)),
            "active_tasks_count": len(self._active_tasks)
        }


class OrchestrationEngine:
    """
    High-level orchestration engine that manages the overall workflow.
    
    I'm providing a simplified interface for complex multi-agent operations
    while handling all the orchestration complexity internally.
    """
    
    def __init__(self, db_manager: DatabaseManager):
        self.orchestrator = TaskOrchestrator(db_manager)
        self.logger = get_logger("orchestration.engine")
        
    async def process_request(
        self,
        prompt: str,
        task_type: TaskType = TaskType.CUSTOM,
        strategy: OrchestrationStrategy = OrchestrationStrategy.SINGLE_AGENT,
        **kwargs
    ) -> TaskResult:
        """Process a user request with intelligent orchestration."""
        
        task = OrchestrationTask(
            description=prompt,
            task_type=task_type,
            strategy=strategy,
            **kwargs
        )
        
        return await self.orchestrator.execute_task(task)
    
    async def analyze_and_route(self, prompt: str, **kwargs) -> TaskResult:
        """Automatically analyze prompt and route to best strategy."""
        
        # Simple heuristics for task type detection
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ["code", "function", "class", "implement", "write"]):
            task_type = TaskType.CODE_GENERATION
            strategy = OrchestrationStrategy.SINGLE_AGENT
        elif any(word in prompt_lower for word in ["review", "check", "analyze", "critique"]):
            task_type = TaskType.CODE_REVIEW
            strategy = OrchestrationStrategy.PARALLEL
        elif any(word in prompt_lower for word in ["debug", "error", "fix", "problem"]):
            task_type = TaskType.DEBUGGING
            strategy = OrchestrationStrategy.SINGLE_AGENT
        elif any(word in prompt_lower for word in ["research", "investigate", "study", "learn"]):
            task_type = TaskType.RESEARCH
            strategy = OrchestrationStrategy.PARALLEL
        elif any(word in prompt_lower for word in ["compare", "debate", "discuss", "opinion"]):
            task_type = TaskType.ANALYSIS
            strategy = OrchestrationStrategy.DEBATE
        else:
            task_type = TaskType.CUSTOM
            strategy = OrchestrationStrategy.SINGLE_AGENT
        
        self.logger.info(f"Auto-routed task: {task_type.value} with {strategy.value}")
        
        return await self.process_request(prompt, task_type, strategy, **kwargs)
    
    def get_status(self) -> Dict[str, Any]:
        """Get overall orchestration engine status."""
        return {
            "orchestrator_metrics": self.orchestrator.get_performance_metrics(),
            "available_agents": len(agent_registry.get_all_agents()),
            "agent_health": {
                agent.name: "healthy" if agent else "unknown"
                for agent in agent_registry.get_all_agents()
            }
        }
</file>

<file path="src/orchestration/voting.py">
"""
Voting system for AngelaMCP multi-agent consensus building.

This module implements weighted voting mechanisms that allow agents to evaluate
solutions, proposals, and reach consensus through democratic processes with
special provisions for Claude Code's veto power as specified in the roadmap.
"""

import asyncio
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
from pydantic import BaseModel, Field

from src.agents.base import BaseAgent, AgentResponse, TaskContext, AgentRole, AgentType, agent_registry
from src.utils.logger import get_logger, log_context, AsyncPerformanceLogger

logger = get_logger("orchestration.voting")


class VoteType(str, Enum):
    """Types of votes that can be cast."""
    APPROVE = "approve"
    REJECT = "reject"
    ABSTAIN = "abstain"
    VETO = "veto"  # Special vote type for Claude Code


class VotingMethod(str, Enum):
    """Different voting methods available."""
    SIMPLE_MAJORITY = "simple_majority"
    WEIGHTED_MAJORITY = "weighted_majority"
    UNANIMOUS = "unanimous"
    CLAUDE_VETO = "claude_veto"  # Claude Code has veto power


@dataclass
class Vote:
    """Represents a single vote from an agent."""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    agent_name: str = ""
    agent_type: str = ""
    vote_type: VoteType = VoteType.ABSTAIN
    confidence: float = 0.5
    reasoning: str = ""
    evidence: List[str] = field(default_factory=list)
    weight: float = 1.0
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class VoteResult:
    """Result of a voting session."""
    vote_id: str
    task_id: str
    success: bool
    final_decision: VoteType
    confidence_score: float
    votes: List[Vote] = field(default_factory=list)
    total_weight: float = 0.0
    approve_weight: float = 0.0
    reject_weight: float = 0.0
    abstain_weight: float = 0.0
    has_veto: bool = False
    veto_reason: str = ""
    voting_method: VotingMethod = VotingMethod.SIMPLE_MAJORITY
    duration_ms: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


class VotingSystem:
    """
    Weighted voting system for multi-agent consensus building.
    
    I'm implementing a sophisticated voting mechanism that:
    1. Allows weighted votes based on agent expertise
    2. Supports different voting methods
    3. Provides Claude Code with veto power as specified
    4. Gathers detailed reasoning for each vote
    """
    
    def __init__(self):
        self.logger = get_logger("orchestration.voting_system")
        
        # Default agent weights for different voting contexts
        self._default_weights = {
            AgentType.CLAUDE_CODE: 1.5,  # Higher weight + veto power
            AgentType.OPENAI: 1.0,       # Standard weight
            AgentType.GEMINI: 1.0        # Standard weight
        }
        
        # Task-specific weight adjustments
        self._task_weight_modifiers = {
            "code_generation": {
                AgentType.CLAUDE_CODE: 1.3,
                AgentType.OPENAI: 0.9,
                AgentType.GEMINI: 0.8
            },
            "code_review": {
                AgentType.CLAUDE_CODE: 1.1,
                AgentType.OPENAI: 1.2,
                AgentType.GEMINI: 0.9
            },
            "research": {
                AgentType.CLAUDE_CODE: 0.8,
                AgentType.OPENAI: 1.0,
                AgentType.GEMINI: 1.3
            },
            "analysis": {
                AgentType.CLAUDE_CODE: 0.9,
                AgentType.OPENAI: 1.2,
                AgentType.GEMINI: 1.1
            }
        }
    
    def _calculate_agent_weight(self, agent: BaseAgent, context: str = "general") -> float:
        """Calculate voting weight for an agent based on context."""
        base_weight = self._default_weights.get(agent.agent_type, 1.0)
        
        # Apply task-specific modifiers
        modifiers = self._task_weight_modifiers.get(context, {})
        modifier = modifiers.get(agent.agent_type, 1.0)
        
        # Performance-based adjustment
        metrics = agent.performance_metrics
        if metrics["total_requests"] > 10:  # Only adjust if we have enough data
            success_rate = (metrics["total_requests"] - 
                          metrics.get("failed_requests", 0)) / metrics["total_requests"]
            performance_modifier = 0.8 + (success_rate * 0.4)  # Range: 0.8 to 1.2
        else:
            performance_modifier = 1.0
        
        final_weight = base_weight * modifier * performance_modifier
        
        self.logger.debug(
            f"Agent {agent.name} weight: {final_weight:.2f} "
            f"(base: {base_weight}, modifier: {modifier}, performance: {performance_modifier})"
        )
        
        return final_weight
    
    async def _collect_vote(self, agent: BaseAgent, proposal: str, context: TaskContext, 
                          weight: float) -> Vote:
        """Collect a vote from a single agent."""
        
        # Special prompt for Claude Code with veto power
        if agent.agent_type == AgentType.CLAUDE_CODE:
            vote_prompt = f"""You are participating in a voting session to evaluate this proposal:

PROPOSAL:
{proposal}

ORIGINAL TASK:
{context.context_data.get('original_task', 'Not specified')}

As Claude Code, you have SPECIAL VETO POWER in this voting system. You can:

1. **APPROVE** - You support this proposal
2. **REJECT** - You oppose this proposal  
3. **VETO** - You strongly oppose and want to prevent this proposal (overrides all other votes)
4. **ABSTAIN** - You have no strong opinion

Please provide your vote with detailed reasoning:

**VOTE**: [APPROVE/REJECT/VETO/ABSTAIN]

**CONFIDENCE**: [0.0 to 1.0]

**REASONING**: 
- Explain your evaluation of the proposal
- Identify strengths and weaknesses
- Consider implementation feasibility
- Assess potential risks or benefits
- If voting VETO, explain why this proposal should be blocked

**EVIDENCE**:
- List specific points that support your vote
- Reference concrete examples or standards
- Cite relevant best practices or concerns

Your vote carries extra weight ({weight:.2f}x) and your VETO can override majority approval."""

        else:
            vote_prompt = f"""You are participating in a voting session to evaluate this proposal:

PROPOSAL:
{proposal}

ORIGINAL TASK:
{context.context_data.get('original_task', 'Not specified')}

Please cast your vote with detailed reasoning:

**VOTE**: [APPROVE/REJECT/ABSTAIN]

**CONFIDENCE**: [0.0 to 1.0] - How confident are you in this assessment?

**REASONING**: 
- Evaluate the proposal's merit and feasibility
- Consider strengths, weaknesses, and trade-offs
- Assess alignment with the original task requirements
- Identify potential risks or implementation challenges

**EVIDENCE**:
- List specific points that support your vote
- Reference concrete examples or standards
- Provide technical or practical justifications

Your vote weight is {weight:.2f} in this decision process."""
        
        try:
            async with AsyncPerformanceLogger(
                self.logger, f"collect_vote", 
                agent=agent.name, task_id=context.task_id
            ):
                response = await agent.generate(vote_prompt, context)
            
            # Parse the vote response
            vote = self._parse_vote_response(agent, response, weight)
            
            self.logger.info(
                f"Collected vote from {agent.name}: {vote.vote_type.value} "
                f"(confidence: {vote.confidence:.2f}, weight: {vote.weight:.2f})"
            )
            
            return vote
            
        except Exception as e:
            self.logger.error(f"Failed to collect vote from {agent.name}: {e}")
            
            # Return abstain vote as fallback
            return Vote(
                agent_name=agent.name,
                agent_type=agent.agent_type.value,
                vote_type=VoteType.ABSTAIN,
                confidence=0.0,
                reasoning=f"Failed to collect vote: {e}",
                weight=weight,
                metadata={"error": str(e)}
            )
    
    def _parse_vote_response(self, agent: BaseAgent, response: AgentResponse, weight: float) -> Vote:
        """Parse agent response into a structured vote."""
        content = response.content.lower()
        
        # Extract vote type
        vote_type = VoteType.ABSTAIN  # Default
        if "veto" in content and agent.agent_type == AgentType.CLAUDE_CODE:
            vote_type = VoteType.VETO
        elif "approve" in content or "support" in content:
            vote_type = VoteType.APPROVE
        elif "reject" in content or "oppose" in content:
            vote_type = VoteType.REJECT
        elif "abstain" in content:
            vote_type = VoteType.ABSTAIN
        
        # Extract confidence (look for patterns like "confidence: 0.8" or "0.8/1.0")
        confidence = 0.5  # Default
        import re
        confidence_patterns = [
            r"confidence[:\s]+([0-9]*\.?[0-9]+)",
            r"([0-9]*\.?[0-9]+)\s*/\s*1\.?0?",
            r"([0-9]*\.?[0-9]+)\s*confidence"
        ]
        
        for pattern in confidence_patterns:
            match = re.search(pattern, content)
            if match:
                try:
                    confidence = float(match.group(1))
                    confidence = max(0.0, min(1.0, confidence))  # Clamp to [0,1]
                    break
                except ValueError:
                    continue
        
        # Extract reasoning and evidence sections
        reasoning = ""
        evidence = []
        
        # Look for reasoning section
        reasoning_match = re.search(r"reasoning[:\s]+(.*?)(?:evidence|$)", content, re.DOTALL | re.IGNORECASE)
        if reasoning_match:
            reasoning = reasoning_match.group(1).strip()
        
        # Look for evidence section
        evidence_match = re.search(r"evidence[:\s]+(.*?)$", content, re.DOTALL | re.IGNORECASE)
        if evidence_match:
            evidence_text = evidence_match.group(1).strip()
            # Split by bullet points or dashes
            evidence = [
                item.strip() 
                for item in re.split(r'[-‚Ä¢*]\s*', evidence_text)
                if item.strip()
            ]
        
        # If we couldn't parse structured sections, use the full content as reasoning
        if not reasoning:
            reasoning = response.content
        
        return Vote(
            agent_name=agent.name,
            agent_type=agent.agent_type.value,
            vote_type=vote_type,
            confidence=confidence,
            reasoning=reasoning,
            evidence=evidence,
            weight=weight,
            metadata={
                "response_success": response.success,
                "response_tokens": response.tokens_used,
                "response_cost": response.cost_usd
            }
        )
    
    async def conduct_vote(self, proposal: str, agents: List[BaseAgent], 
                         method: VotingMethod = VotingMethod.CLAUDE_VETO,
                         context: Optional[TaskContext] = None,
                         task_context: str = "general") -> VoteResult:
        """Conduct a voting session among the specified agents."""
        
        vote_id = str(uuid.uuid4())
        start_time = time.time()
        
        if context is None:
            context = TaskContext(
                task_id=vote_id,
                task_type="voting"
            )
        
        # Add original proposal to context
        context.context_data["original_task"] = context.context_data.get("original_task", proposal)
        
        with log_context(vote_id, context.session_id):
            self.logger.info(
                f"Starting vote {vote_id} with {len(agents)} agents using {method.value}"
            )
            
            try:
                # Calculate weights for all agents
                agent_weights = {
                    agent: self._calculate_agent_weight(agent, task_context) 
                    for agent in agents
                }
                
                # Collect votes from all agents in parallel
                vote_tasks = [
                    self._collect_vote(agent, proposal, context, agent_weights[agent])
                    for agent in agents
                ]
                
                async with AsyncPerformanceLogger(
                    self.logger, "collect_all_votes", 
                    vote_id=vote_id, agent_count=len(agents)
                ):
                    votes = await asyncio.gather(*vote_tasks, return_exceptions=True)
                
                # Filter out exceptions and create valid votes list
                valid_votes = []
                for i, vote in enumerate(votes):
                    if isinstance(vote, Exception):
                        self.logger.error(f"Vote collection failed for {agents[i].name}: {vote}")
                        # Create abstain vote as fallback
                        fallback_vote = Vote(
                            agent_name=agents[i].name,
                            agent_type=agents[i].agent_type.value,
                            vote_type=VoteType.ABSTAIN,
                            confidence=0.0,
                            reasoning=f"Vote collection failed: {vote}",
                            weight=agent_weights[agents[i]],
                            metadata={"error": str(vote)}
                        )
                        valid_votes.append(fallback_vote)
                    else:
                        valid_votes.append(vote)
                
                # Calculate vote result
                result = self._calculate_vote_result(
                    vote_id, context.task_id, valid_votes, method, start_time
                )
                
                self.logger.info(
                    f"Vote {vote_id} completed - Decision: {result.final_decision.value} "
                    f"(confidence: {result.confidence_score:.2f}, "
                    f"approve: {result.approve_weight:.1f}, "
                    f"reject: {result.reject_weight:.1f}, "
                    f"veto: {result.has_veto})"
                )
                
                return result
                
            except Exception as e:
                duration = (time.time() - start_time) * 1000
                self.logger.error(f"Vote conduct failed: {e}")
                
                return VoteResult(
                    vote_id=vote_id,
                    task_id=context.task_id,
                    success=False,
                    final_decision=VoteType.REJECT,
                    confidence_score=0.0,
                    voting_method=method,
                    duration_ms=duration,
                    metadata={"error": str(e)}
                )
    
    def _calculate_vote_result(self, vote_id: str, task_id: str, votes: List[Vote], 
                             method: VotingMethod, start_time: float) -> VoteResult:
        """Calculate the final result of a vote."""
        
        # Calculate weight totals
        total_weight = sum(vote.weight for vote in votes)
        approve_weight = sum(vote.weight for vote in votes if vote.vote_type == VoteType.APPROVE)
        reject_weight = sum(vote.weight for vote in votes if vote.vote_type == VoteType.REJECT)
        abstain_weight = sum(vote.weight for vote in votes if vote.vote_type == VoteType.ABSTAIN)
        
        # Check for veto
        veto_votes = [vote for vote in votes if vote.vote_type == VoteType.VETO]
        has_veto = len(veto_votes) > 0
        veto_reason = ""
        
        if has_veto:
            veto_reason = "; ".join([vote.reasoning for vote in veto_votes])
        
        # Determine final decision based on method
        final_decision = VoteType.REJECT  # Default
        confidence_score = 0.0
        
        if method == VotingMethod.CLAUDE_VETO:
            if has_veto:
                final_decision = VoteType.VETO
                confidence_score = max([vote.confidence for vote in veto_votes])
            elif approve_weight > reject_weight:
                final_decision = VoteType.APPROVE
                # Weight confidence by vote weights
                approve_votes = [vote for vote in votes if vote.vote_type == VoteType.APPROVE]
                if approve_votes:
                    weighted_confidence = sum(vote.confidence * vote.weight for vote in approve_votes)
                    confidence_score = weighted_confidence / approve_weight
            else:
                final_decision = VoteType.REJECT
                reject_votes = [vote for vote in votes if vote.vote_type == VoteType.REJECT]
                if reject_votes:
                    weighted_confidence = sum(vote.confidence * vote.weight for vote in reject_votes)
                    confidence_score = weighted_confidence / max(reject_weight, 0.1)
        
        elif method == VotingMethod.SIMPLE_MAJORITY:
            approve_count = len([vote for vote in votes if vote.vote_type == VoteType.APPROVE])
            reject_count = len([vote for vote in votes if vote.vote_type == VoteType.REJECT])
            
            if approve_count > reject_count:
                final_decision = VoteType.APPROVE
            else:
                final_decision = VoteType.REJECT
            
            # Simple average confidence
            relevant_votes = [vote for vote in votes if vote.vote_type == final_decision]
            if relevant_votes:
                confidence_score = sum(vote.confidence for vote in relevant_votes) / len(relevant_votes)
        
        elif method == VotingMethod.WEIGHTED_MAJORITY:
            if approve_weight > reject_weight:
                final_decision = VoteType.APPROVE
                confidence_score = approve_weight / max(total_weight - abstain_weight, 0.1)
            else:
                final_decision = VoteType.REJECT
                confidence_score = reject_weight / max(total_weight - abstain_weight, 0.1)
        
        elif method == VotingMethod.UNANIMOUS:
            # All non-abstaining votes must agree
            non_abstain_votes = [vote for vote in votes if vote.vote_type != VoteType.ABSTAIN]
            if non_abstain_votes:
                first_vote_type = non_abstain_votes[0].vote_type
                if all(vote.vote_type == first_vote_type for vote in non_abstain_votes):
                    final_decision = first_vote_type
                    confidence_score = sum(vote.confidence for vote in non_abstain_votes) / len(non_abstain_votes)
                else:
                    final_decision = VoteType.REJECT  # No consensus
                    confidence_score = 0.0
        
        duration = (time.time() - start_time) * 1000
        
        return VoteResult(
            vote_id=vote_id,
            task_id=task_id,
            success=final_decision in [VoteType.APPROVE],
            final_decision=final_decision,
            confidence_score=confidence_score,
            votes=votes,
            total_weight=total_weight,
            approve_weight=approve_weight,
            reject_weight=reject_weight,
            abstain_weight=abstain_weight,
            has_veto=has_veto,
            veto_reason=veto_reason,
            voting_method=method,
            duration_ms=duration,
            metadata={
                "total_votes": len(votes),
                "vote_breakdown": {
                    "approve": len([v for v in votes if v.vote_type == VoteType.APPROVE]),
                    "reject": len([v for v in votes if v.vote_type == VoteType.REJECT]),
                    "abstain": len([v for v in votes if v.vote_type == VoteType.ABSTAIN]),
                    "veto": len([v for v in votes if v.vote_type == VoteType.VETO])
                }
            }
        )


class ConsensusBuilder:
    """
    High-level consensus building system that combines voting with iteration.
    
    I'm implementing a system that can refine proposals based on feedback
    and seek consensus through multiple rounds of voting and revision.
    """
    
    def __init__(self, voting_system: VotingSystem):
        self.voting_system = voting_system
        self.logger = get_logger("orchestration.consensus_builder")
    
    async def build_consensus(self, initial_proposal: str, agents: List[BaseAgent],
                            max_iterations: int = 3,
                            consensus_threshold: float = 0.8,
                            context: Optional[TaskContext] = None) -> VoteResult:
        """Build consensus through iterative voting and proposal refinement."""
        
        current_proposal = initial_proposal
        iteration = 0
        
        self.logger.info(f"Starting consensus building with {len(agents)} agents")
        
        while iteration < max_iterations:
            iteration += 1
            self.logger.info(f"Consensus iteration {iteration}")
            
            # Conduct vote on current proposal
            vote_result = await self.voting_system.conduct_vote(
                current_proposal, agents, VotingMethod.CLAUDE_VETO, context
            )
            
            # Check if we achieved consensus
            if vote_result.success and vote_result.confidence_score >= consensus_threshold:
                self.logger.info(f"Consensus achieved in iteration {iteration}")
                return vote_result
            
            # If vetoed, we need major changes
            if vote_result.has_veto:
                self.logger.info("Proposal vetoed, seeking major revision")
                break
            
            # If we have more iterations, refine the proposal based on feedback
            if iteration < max_iterations:
                self.logger.info("Refining proposal based on feedback")
                current_proposal = await self._refine_proposal(
                    current_proposal, vote_result, agents, context
                )
        
        # Return the final vote result
        self.logger.info(
            f"Consensus building completed after {iteration} iterations. "
            f"Final decision: {vote_result.final_decision.value}"
        )
        
        return vote_result
    
    async def _refine_proposal(self, original_proposal: str, vote_result: VoteResult,
                             agents: List[BaseAgent], context: Optional[TaskContext]) -> str:
        """Refine proposal based on voting feedback."""
        
        # Collect feedback from agents who rejected or had concerns
        feedback_items = []
        for vote in vote_result.votes:
            if vote.vote_type in [VoteType.REJECT, VoteType.VETO]:
                feedback_items.append(f"**{vote.agent_name}**: {vote.reasoning}")
        
        if not feedback_items:
            return original_proposal  # No specific feedback to incorporate
        
        feedback_text = "\n".join(feedback_items)
        
        # Use the best-performing agent to refine the proposal
        best_agent = max(agents, key=lambda a: a.performance_metrics.get("success_rate", 0.5))
        
        refinement_prompt = f"""Please refine this proposal based on the feedback received:

ORIGINAL PROPOSAL:
{original_proposal}

FEEDBACK FROM VOTING:
{feedback_text}

Please create an improved version that addresses the concerns raised while maintaining the core intent. The refined proposal should:

1. Address the specific issues mentioned in the feedback
2. Incorporate valid suggestions and improvements
3. Resolve conflicts and contradictions
4. Maintain feasibility and practicality
5. Keep the core objectives intact

Provide only the refined proposal without additional commentary."""

        try:
            if context:
                context.task_id = f"{context.task_id}_refinement"
            else:
                context = TaskContext(task_id="proposal_refinement")
            
            response = await best_agent.generate(refinement_prompt, context)
            
            if response.success:
                self.logger.info(f"Proposal refined by {best_agent.name}")
                return response.content
            else:
                self.logger.warning("Proposal refinement failed, using original")
                return original_proposal
                
        except Exception as e:
            self.logger.error(f"Error refining proposal: {e}")
            return original_proposal
</file>

<file path="src/orchestrator/__init__.py">
"""Orchestration engine for multi-agent collaboration."""

from .manager import TaskOrchestrator
from .debate import DebateProtocol
from .voting import VotingSystem
from .task_queue import AsyncTaskQueue

__all__ = ["TaskOrchestrator", "DebateProtocol", "VotingSystem", "AsyncTaskQueue"]
</file>

<file path="src/orchestrator/collaboration.py">
"""
Main Collaboration Orchestrator for AngelaMCP.

This module coordinates the complete multi-agent collaboration flow:
1. Initialize agents (Claude Code, OpenAI, Gemini)
2. Conduct structured debate
3. Run weighted voting
4. Return final collaborative result

This is the core of what makes AngelaMCP different from single-agent tools.
"""

import asyncio
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime

from src.agents.base import BaseAgent, TaskContext, TaskType, AgentType
from src.agents.claude_agent import ClaudeCodeAgent
from src.agents.openai_agent import OpenAIAgent
from src.agents.gemini_agent import GeminiAgent
from src.orchestrator.debate import DebateProtocol, DebateResult
from src.orchestrator.voting import VotingSystem, VotingResult
from src.utils.logger import get_logger, AsyncPerformanceLogger
from config.settings import settings

logger = get_logger("orchestrator.collaboration")


class CollaborationMode(str, Enum):
    """Different modes of collaboration."""
    FULL_DEBATE = "full_debate"  # Complete debate + voting
    QUICK_VOTE = "quick_vote"    # Skip critique phase, go straight to voting
    CLAUDE_LEAD = "claude_lead"  # Claude Code proposes, others review
    PARALLEL_WORK = "parallel_work"  # All agents work independently, best result wins


@dataclass
class CollaborationRequest:
    """Request for multi-agent collaboration."""
    task_description: str
    mode: CollaborationMode = CollaborationMode.FULL_DEBATE
    require_consensus: bool = True
    timeout_minutes: int = 10
    context: Optional[Dict[str, Any]] = None


@dataclass
class CollaborationResult:
    """Complete result of multi-agent collaboration."""
    collaboration_id: str
    request: CollaborationRequest
    success: bool
    
    # Final outcome
    final_solution: Optional[str] = None
    chosen_agent: Optional[str] = None
    
    # Process details
    debate_result: Optional[DebateResult] = None
    voting_result: Optional[VotingResult] = None
    
    # Metrics
    total_duration: float = 0.0
    agents_participated: List[str] = field(default_factory=list)
    consensus_reached: bool = False
    
    # Execution summary
    summary: str = ""
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class CollaborationOrchestrator:
    """
    Main orchestrator for multi-agent collaboration.
    
    This is the heart of AngelaMCP - it coordinates Claude Code (with its MCP tools),
    OpenAI (reviewer), and Gemini (researcher) to work together on tasks.
    """
    
    def __init__(
        self,
        claude_agent: Optional[ClaudeCodeAgent] = None,
        openai_agent: Optional[OpenAIAgent] = None,
        gemini_agent: Optional[GeminiAgent] = None,
        status_callback: Optional[Callable[[str], None]] = None
    ):
        """
        Initialize the collaboration orchestrator.
        
        Args:
            claude_agent: Claude Code agent instance
            openai_agent: OpenAI agent instance  
            gemini_agent: Gemini agent instance
            status_callback: Optional callback for real-time status updates
        """
        self.logger = get_logger("collaboration")
        self.status_callback = status_callback
        
        # Initialize agents
        self.claude_agent = claude_agent or ClaudeCodeAgent()
        self.openai_agent = openai_agent or OpenAIAgent()
        self.gemini_agent = gemini_agent or GeminiAgent()
        
        # Initialize collaboration components
        self.debate_protocol = DebateProtocol(
            timeout_per_phase=settings.debate_timeout,
            max_rounds=settings.debate_max_rounds
        )
        
        self.voting_system = VotingSystem(
            claude_vote_weight=settings.claude_vote_weight,
            enable_claude_veto=settings.claude_veto_enabled,
            voting_timeout=settings.voting_timeout
        )
        
        # Track active collaborations
        self._active_collaborations: Dict[str, Dict[str, Any]] = {}
        
        self.logger.info("üé≠ Collaboration orchestrator initialized with 3 agents")
    
    def _update_status(self, message: str) -> None:
        """Update status via callback if available."""
        if self.status_callback:
            self.status_callback(message)
        self.logger.info(message)
    
    async def collaborate(self, request: CollaborationRequest) -> CollaborationResult:
        """
        Execute a complete multi-agent collaboration.
        
        Args:
            request: The collaboration request
            
        Returns:
            CollaborationResult with final outcome and process details
        """
        collaboration_id = str(uuid.uuid4())
        start_time = time.time()
        
        self._update_status(f"üöÄ Starting collaboration {collaboration_id[:8]} on: {request.task_description[:60]}...")
        
        # Track collaboration
        self._active_collaborations[collaboration_id] = {
            "id": collaboration_id,
            "request": request,
            "start_time": start_time,
            "status": "initializing"
        }
        
        try:
            async with AsyncPerformanceLogger(self.logger, "collaboration_full", task_id=collaboration_id):
                
                # Create task context
                context = TaskContext(
                    task_id=collaboration_id,
                    task_type=self._determine_task_type(request.task_description),
                    session_id=collaboration_id,
                    requires_collaboration=True,
                    enable_debate=True,
                    max_debate_rounds=settings.debate_max_rounds,
                    timeout_seconds=request.timeout_minutes * 60,
                    context_data=request.context or {}
                )
                
                # Get list of participating agents
                agents = [self.claude_agent, self.openai_agent, self.gemini_agent]
                participating_agents = [agent.name for agent in agents]
                
                # Execute collaboration based on mode
                if request.mode == CollaborationMode.FULL_DEBATE:
                    result = await self._full_debate_collaboration(
                        request, agents, context, collaboration_id
                    )
                elif request.mode == CollaborationMode.CLAUDE_LEAD:
                    result = await self._claude_lead_collaboration(
                        request, agents, context, collaboration_id
                    )
                else:
                    # Default to full debate for now
                    result = await self._full_debate_collaboration(
                        request, agents, context, collaboration_id
                    )
                
                total_duration = time.time() - start_time
                
                # Create final result
                final_result = CollaborationResult(
                    collaboration_id=collaboration_id,
                    request=request,
                    success=result["success"],
                    final_solution=result.get("solution"),
                    chosen_agent=result.get("chosen_agent"),
                    debate_result=result.get("debate_result"),
                    voting_result=result.get("voting_result"),
                    total_duration=total_duration,
                    agents_participated=participating_agents,
                    consensus_reached=result.get("consensus_reached", False),
                    summary=result.get("summary", ""),
                    error_message=result.get("error"),
                    metadata={
                        "mode": request.mode,
                        "agents_count": len(agents),
                        "timeout_minutes": request.timeout_minutes
                    }
                )
                
                self._update_status(f"‚úÖ Collaboration completed: {final_result.chosen_agent or 'No winner'} in {total_duration:.1f}s")
                return final_result
                
        except Exception as e:
            total_duration = time.time() - start_time
            self.logger.error(f"‚ùå Collaboration {collaboration_id[:8]} failed: {e}")
            
            return CollaborationResult(
                collaboration_id=collaboration_id,
                request=request,
                success=False,
                total_duration=total_duration,
                agents_participated=[agent.name for agent in [self.claude_agent, self.openai_agent, self.gemini_agent]],
                error_message=str(e),
                metadata={"error_type": type(e).__name__}
            )
        
        finally:
            # Cleanup
            if collaboration_id in self._active_collaborations:
                del self._active_collaborations[collaboration_id]
    
    async def _full_debate_collaboration(
        self,
        request: CollaborationRequest,
        agents: List[BaseAgent],
        context: TaskContext,
        collaboration_id: str
    ) -> Dict[str, Any]:
        """Execute full debate + voting collaboration."""
        
        self._update_status(f"[{collaboration_id[:8]}] üé™ Starting full debate mode")
        
        # Phase 1: Conduct Debate
        self._update_status(f"[{collaboration_id[:8]}] üí¨ Phase 1: Agent debate in progress...")
        debate_result = await self.debate_protocol.conduct_debate(
            topic=request.task_description,
            agents=agents,
            context=context,
            require_all_agents=False  # Continue even if some agents fail
        )
        
        if not debate_result.success:
            return {
                "success": False,
                "error": f"Debate failed: {debate_result.error_message}",
                "debate_result": debate_result
            }
        
        # Phase 2: Conduct Voting
        self._update_status(f"[{collaboration_id[:8]}] üó≥Ô∏è Phase 2: Voting on proposals...")
        voting_result = await self.voting_system.conduct_voting(
            debate_result=debate_result,
            agents=agents,
            context=context
        )
        
        if not voting_result.success:
            return {
                "success": False,
                "error": f"Voting failed: {voting_result.error_message}",
                "debate_result": debate_result,
                "voting_result": voting_result
            }
        
        # Phase 3: Final Implementation (if we have a winner)
        final_solution = None
        if voting_result.winner and voting_result.winning_proposal:
            self._update_status(f"[{collaboration_id[:8]}] üèÜ Phase 3: Implementing {voting_result.winner}'s solution...")
            
            # If Claude Code won, we can potentially execute the solution
            if voting_result.winner == self.claude_agent.name:
                # Claude Code can execute its own solution
                final_solution = voting_result.winning_proposal.content
            else:
                # For other agents, we use their proposal as-is
                final_solution = voting_result.winning_proposal.content
        
        # Create summary
        summary = f"""
**üé≠ Multi-Agent Collaboration Complete**

**Task:** {request.task_description[:100]}...

**Participants:** {', '.join([agent.name for agent in agents])}

**Debate Results:**
- Proposals generated: {len(debate_result.rounds[0].proposals) if debate_result.rounds else 0}
- Duration: {debate_result.total_duration:.1f}s

**Voting Results:**
{voting_result.voting_summary}

**Final Outcome:** {"‚úÖ " + voting_result.winner if voting_result.winner else "‚ùå No consensus reached"}
""".strip()
        
        return {
            "success": True,
            "solution": final_solution,
            "chosen_agent": voting_result.winner,
            "debate_result": debate_result,
            "voting_result": voting_result,
            "consensus_reached": voting_result.consensus_reached,
            "summary": summary
        }
    
    async def _claude_lead_collaboration(
        self,
        request: CollaborationRequest,
        agents: List[BaseAgent],
        context: TaskContext,
        collaboration_id: str
    ) -> Dict[str, Any]:
        """Execute Claude-lead collaboration (Claude proposes, others review)."""
        
        self._update_status(f"[{collaboration_id[:8]}] üëë Claude leading collaboration")
        
        # Phase 1: Claude Code creates initial solution
        self._update_status(f"[{collaboration_id[:8]}] üîß Claude Code implementing solution...")
        claude_response = await self.claude_agent.propose_solution(
            request.task_description, [], context
        )
        
        if not claude_response.success:
            return {
                "success": False,
                "error": f"Claude Code failed to create solution: {claude_response.error_message}"
            }
        
        # Phase 2: Other agents review and suggest improvements
        self._update_status(f"[{collaboration_id[:8]}] üëÄ Other agents reviewing Claude's work...")
        
        reviews = []
        for agent in agents:
            if agent.name == self.claude_agent.name:
                continue  # Skip Claude reviewing itself
            
            try:
                review = await agent.critique(
                    claude_response.content,
                    f"Solution from Claude Code for: {request.task_description}",
                    context
                )
                if review.success:
                    reviews.append({
                        "agent": agent.name,
                        "review": review.content
                    })
            except Exception as e:
                self.logger.warning(f"Review from {agent.name} failed: {e}")
        
        # Phase 3: Claude Code optionally refines based on feedback
        final_solution = claude_response.content
        if reviews:
            self._update_status(f"[{collaboration_id[:8]}] üîÑ Claude Code refining based on feedback...")
            
            # Create refinement prompt
            review_text = "\n\n".join([
                f"**Review from {r['agent']}:**\n{r['review']}"
                for r in reviews
            ])
            
            refinement_prompt = f"""Based on feedback from other AI agents, please refine your solution:

**Original Task:** {request.task_description}

**Your Current Solution:**
{claude_response.content}

**Feedback Received:**
{review_text}

Please provide an improved solution that addresses valid concerns while maintaining the core strengths of your approach."""
            
            try:
                refined_response = await self.claude_agent.generate(refinement_prompt, context)
                if refined_response.success:
                    final_solution = refined_response.content
            except Exception as e:
                self.logger.warning(f"Claude Code refinement failed: {e}")
        
        summary = f"""
**üëë Claude Code Lead Collaboration**

**Task:** {request.task_description[:100]}...

**Process:**
- Claude Code created initial solution
- {len(reviews)} agents provided reviews
- {"Solution refined based on feedback" if reviews else "No refinements needed"}

**Final Solution:** Implemented by Claude Code
""".strip()
        
        return {
            "success": True,
            "solution": final_solution,
            "chosen_agent": self.claude_agent.name,
            "consensus_reached": True,  # Claude Code is authoritative
            "summary": summary
        }
    
    def _determine_task_type(self, task_description: str) -> TaskType:
        """Determine the task type from description."""
        task_lower = task_description.lower()
        
        if any(word in task_lower for word in ["test", "testing", "unittest", "pytest"]):
            return TaskType.TESTING
        elif any(word in task_lower for word in ["debug", "error", "fix", "bug"]):
            return TaskType.DEBUGGING
        elif any(word in task_lower for word in ["review", "critique", "analyze"]):
            return TaskType.CODE_REVIEW
        elif any(word in task_lower for word in ["document", "docs", "readme", "comment"]):
            return TaskType.DOCUMENTATION
        elif any(word in task_lower for word in ["research", "investigate", "explore"]):
            return TaskType.RESEARCH
        elif any(word in task_lower for word in ["create", "build", "implement", "develop", "code"]):
            return TaskType.CODE_GENERATION
        else:
            return TaskType.CUSTOM
    
    async def quick_task(self, task_description: str, timeout_minutes: int = 5) -> CollaborationResult:
        """Execute a quick collaborative task with reasonable defaults."""
        request = CollaborationRequest(
            task_description=task_description,
            mode=CollaborationMode.FULL_DEBATE,
            timeout_minutes=timeout_minutes
        )
        return await self.collaborate(request)
    
    async def claude_lead_task(self, task_description: str, timeout_minutes: int = 3) -> CollaborationResult:
        """Execute a Claude-lead task where Claude implements and others review."""
        request = CollaborationRequest(
            task_description=task_description,
            mode=CollaborationMode.CLAUDE_LEAD,
            timeout_minutes=timeout_minutes
        )
        return await self.collaborate(request)
    
    def get_active_collaborations(self) -> Dict[str, Dict[str, Any]]:
        """Get information about active collaborations."""
        return {
            collab_id: {
                "task": info["request"].task_description[:100],
                "status": info["status"],
                "duration": time.time() - info["start_time"]
            }
            for collab_id, info in self._active_collaborations.items()
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Check health of all agents and collaboration system."""
        agent_health = {}
        
        # Check each agent
        for agent_name, agent in [
            ("claude_code", self.claude_agent),
            ("openai", self.openai_agent),
            ("gemini", self.gemini_agent)
        ]:
            try:
                health = await agent.health_check()
                agent_health[agent_name] = health
            except Exception as e:
                agent_health[agent_name] = {"status": "error", "error": str(e)}
        
        # Overall system health
        healthy_agents = sum(1 for h in agent_health.values() if h.get("status") == "healthy")
        
        return {
            "overall_status": "healthy" if healthy_agents >= 2 else "degraded" if healthy_agents >= 1 else "unhealthy",
            "healthy_agents": healthy_agents,
            "total_agents": 3,
            "agent_health": agent_health,
            "collaboration_features": {
                "debate_protocol": "available",
                "voting_system": "available",
                "claude_veto": settings.claude_veto_enabled
            }
        }


class CollaborationError(Exception):
    """Exception raised during collaboration operations."""
    pass
</file>

<file path="src/orchestrator/manager.py">
"""
Task Orchestrator for AngelaMCP multi-agent collaboration.

This is the core brain that coordinates between Claude Code, OpenAI, and Gemini agents.
I'm implementing a production-grade orchestration system with debate, voting, and consensus.
"""

import asyncio
import time
import uuid
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from enum import Enum

from src.agents.base import BaseAgent, AgentType, AgentResponse, TaskContext, TaskType
from src.agents.claude_agent import ClaudeCodeAgent
from src.agents.openai_agent import OpenAIAgent
from src.agents.gemini_agent import GeminiAgent
from src.persistence.database import DatabaseManager
from src.utils.logger import get_logger
from src.utils.exceptions import OrchestrationError
from config.settings import settings

logger = get_logger("orchestrator.manager")


class CollaborationStrategy(str, Enum):
    """Strategy for agent collaboration."""
    SINGLE_AGENT = "single_agent"
    PARALLEL = "parallel"
    DEBATE = "debate"
    CONSENSUS = "consensus"


class TaskComplexity(str, Enum):
    """Task complexity levels."""
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    EXPERT = "expert"


@dataclass
class CollaborationResult:
    """Result of a collaboration session."""
    success: bool
    final_solution: str
    agent_responses: List[Dict[str, Any]] = field(default_factory=list)
    consensus_score: float = 0.0
    debate_summary: Optional[str] = None
    execution_time: float = 0.0
    cost_breakdown: Optional[Dict[str, float]] = None
    strategy_used: Optional[CollaborationStrategy] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DebateRound:
    """Single round of debate between agents."""
    round_number: int
    topic: str
    responses: List[Dict[str, Any]] = field(default_factory=list)
    critiques: List[Dict[str, Any]] = field(default_factory=list)
    round_summary: Optional[str] = None


@dataclass
class DebateResult:
    """Result of a structured debate."""
    topic: str
    rounds: List[DebateRound] = field(default_factory=list)
    final_consensus: Optional[str] = None
    consensus_score: float = 0.0
    participant_votes: Dict[str, Any] = field(default_factory=dict)
    rounds_completed: int = 0


class TaskOrchestrator:
    """
    Main orchestrator for multi-agent collaboration.

    Coordinates Claude Code, OpenAI, and Gemini agents for complex tasks.
    """

    def __init__(
        self,
        claude_agent: ClaudeCodeAgent,
        openai_agent: OpenAIAgent,
        gemini_agent: GeminiAgent,
        db_manager: Optional[DatabaseManager] = None
    ):
        self.claude_agent = claude_agent
        self.openai_agent = openai_agent
        self.gemini_agent = gemini_agent
        self.db_manager = db_manager

        # Agent mapping
        self.agents: Dict[str, BaseAgent] = {
            "claude": claude_agent,
            "openai": openai_agent,
            "gemini": gemini_agent
        }

        # Voting weights (Claude Code is senior developer)
        self.voting_weights = {
            "claude": settings.claude_vote_weight,
            "openai": settings.openai_vote_weight,
            "gemini": settings.gemini_vote_weight
        }

        logger.info("Task orchestrator initialized with 3 agents")

    async def collaborate_on_task(
        self,
        task_description: str,
        agents: List[str] = None,
        strategy: str = "debate",
        max_rounds: int = 3,
        require_consensus: bool = True
    ) -> CollaborationResult:
        """
        Main collaboration method - orchestrates multiple agents on a task.
        """
        start_time = time.time()

        try:
            # Default to all agents if none specified
            if agents is None:
                agents = ["claude", "openai", "gemini"]

            # Validate agents
            valid_agents = [agent for agent in agents if agent in self.agents]
            if not valid_agents:
                raise OrchestrationError("No valid agents specified")

            logger.info(f"Starting collaboration: {strategy} with agents: {valid_agents}")

            # Determine strategy
            collaboration_strategy = CollaborationStrategy(strategy)

            # Execute based on strategy
            if collaboration_strategy == CollaborationStrategy.SINGLE_AGENT:
                result = await self._single_agent_execution(task_description, valid_agents[0])
            elif collaboration_strategy == CollaborationStrategy.PARALLEL:
                result = await self._parallel_execution(task_description, valid_agents)
            elif collaboration_strategy == CollaborationStrategy.DEBATE:
                result = await self._debate_execution(task_description, valid_agents, max_rounds)
            elif collaboration_strategy == CollaborationStrategy.CONSENSUS:
                result = await self._consensus_execution(task_description, valid_agents, require_consensus)
            else:
                raise OrchestrationError(f"Unknown strategy: {strategy}")

            # Calculate execution time
            result.execution_time = time.time() - start_time
            result.strategy_used = collaboration_strategy

            logger.info(f"Collaboration completed in {result.execution_time:.2f}s")
            return result

        except Exception as e:
            logger.error(f"Collaboration failed: {e}", exc_info=True)
            return CollaborationResult(
                success=False,
                final_solution=f"Collaboration failed: {str(e)}",
                execution_time=time.time() - start_time
            )

    async def start_debate(
        self,
        topic: str,
        agents: List[str] = None,
        max_rounds: int = 3,
        timeout_seconds: int = 300
    ) -> Dict[str, Any]:
        """Start a structured debate between agents."""
        start_time = time.time()

        try:
            if agents is None:
                agents = ["claude", "openai", "gemini"]

            valid_agents = [agent for agent in agents if agent in self.agents]
            if len(valid_agents) < 2:
                raise OrchestrationError("Need at least 2 agents for debate")

            logger.info(f"Starting debate on: {topic}")

            debate_result = DebateResult(topic=topic)

            # Run debate rounds
            for round_num in range(1, max_rounds + 1):
                round_result = await self._run_debate_round(
                    topic, valid_agents, round_num, timeout_seconds
                )
                debate_result.rounds.append(round_result)
                debate_result.rounds_completed = round_num

                # Check if consensus reached early
                if await self._check_early_consensus(debate_result.rounds):
                    logger.info(f"Early consensus reached after round {round_num}")
                    break

            # Generate final consensus
            final_consensus = await self._generate_final_consensus(debate_result)
            debate_result.final_consensus = final_consensus["summary"]
            debate_result.consensus_score = final_consensus["score"]
            debate_result.participant_votes = final_consensus["votes"]

            execution_time = time.time() - start_time

            return {
                "topic": topic,
                "rounds_completed": debate_result.rounds_completed,
                "rounds": [
                    {
                        "round": r.round_number,
                        "responses": r.responses,
                        "critiques": r.critiques,
                        "summary": r.round_summary
                    }
                    for r in debate_result.rounds
                ],
                "consensus": {
                    "summary": debate_result.final_consensus,
                    "score": debate_result.consensus_score,
                    "votes": debate_result.participant_votes
                },
                "execution_time": execution_time
            }

        except Exception as e:
            logger.error(f"Debate failed: {e}", exc_info=True)
            return {
                "topic": topic,
                "error": str(e),
                "execution_time": time.time() - start_time
            }

    async def analyze_task_complexity(self, task_description: str) -> Dict[str, Any]:
        """Analyze task complexity and recommend collaboration strategy."""
        try:
            # Simple complexity analysis based on keywords and length
            complexity_indicators = {
                "simple": ["hello", "test", "example", "simple"],
                "moderate": ["create", "build", "implement", "design"],
                "complex": ["system", "architecture", "integrate", "optimize"],
                "expert": ["enterprise", "scalable", "production", "security", "performance"]
            }

            task_lower = task_description.lower()
            task_length = len(task_description.split())

            # Calculate complexity score
            complexity_score = 0
            detected_level = TaskComplexity.SIMPLE

            for level, keywords in complexity_indicators.items():
                if any(keyword in task_lower for keyword in keywords):
                    if level == "simple":
                        complexity_score = max(complexity_score, 2)
                        detected_level = TaskComplexity.SIMPLE
                    elif level == "moderate":
                        complexity_score = max(complexity_score, 4)
                        detected_level = TaskComplexity.MODERATE
                    elif level == "complex":
                        complexity_score = max(complexity_score, 7)
                        detected_level = TaskComplexity.COMPLEX
                    elif level == "expert":
                        complexity_score = max(complexity_score, 9)
                        detected_level = TaskComplexity.EXPERT

            # Adjust based on length
            if task_length > 50:
                complexity_score += 1
            elif task_length > 20:
                complexity_score += 0.5

            complexity_score = min(10, complexity_score)

            # Recommend strategy
            if complexity_score <= 3:
                strategy = CollaborationStrategy.SINGLE_AGENT
                agents = ["claude"]
                estimated_time = "1-5 minutes"
            elif complexity_score <= 6:
                strategy = CollaborationStrategy.PARALLEL
                agents = ["claude", "openai"]
                estimated_time = "5-15 minutes"
            else:
                strategy = CollaborationStrategy.DEBATE
                agents = ["claude", "openai", "gemini"]
                estimated_time = "15-30 minutes"

            return {
                "complexity_score": complexity_score,
                "complexity_level": detected_level.value,
                "recommended_strategy": strategy.value,
                "recommended_agents": agents,
                "estimated_time": estimated_time,
                "technical_complexity": detected_level.value,
                "collaboration_benefit": "High" if complexity_score > 6 else "Medium" if complexity_score > 3 else "Low",
                "reasoning": f"Task analysis indicates {detected_level.value} complexity based on keywords and scope. "
                           f"Recommended approach: {strategy.value} with {len(agents)} agent(s)."
            }

        except Exception as e:
            logger.error(f"Task complexity analysis failed: {e}")
            return {
                "complexity_score": 5,
                "recommended_strategy": "parallel",
                "recommended_agents": ["claude", "openai"],
                "error": str(e)
            }

    async def _single_agent_execution(self, task_description: str, agent_name: str) -> CollaborationResult:
        """Execute task with single agent."""
        try:
            agent = self.agents[agent_name]
            context = TaskContext(
                task_type=TaskType.GENERAL,
                agent_role="primary"
            )

            response = await agent.generate(task_description, context)

            return CollaborationResult(
                success=True,
                final_solution=response.content,
                agent_responses=[{
                    "agent": agent_name,
                    "content": response.content,
                    "confidence": response.confidence,
                    "execution_time": response.execution_time_ms
                }],
                consensus_score=1.0
            )

        except Exception as e:
            return CollaborationResult(
                success=False,
                final_solution=f"Single agent execution failed: {str(e)}"
            )

    async def _parallel_execution(self, task_description: str, agents: List[str]) -> CollaborationResult:
        """Execute task with agents in parallel."""
        try:
            tasks = []
            context = TaskContext(task_type=TaskType.GENERAL)

            # Launch all agents in parallel
            for agent_name in agents:
                agent = self.agents[agent_name]
                task = agent.generate(task_description, context)
                tasks.append((agent_name, task))

            # Wait for all responses
            responses = []
            for agent_name, task in tasks:
                try:
                    response = await task
                    responses.append({
                        "agent": agent_name,
                        "content": response.content,
                        "confidence": response.confidence,
                        "execution_time": response.execution_time_ms
                    })
                except Exception as e:
                    logger.error(f"Agent {agent_name} failed: {e}")
                    responses.append({
                        "agent": agent_name,
                        "content": f"Agent failed: {str(e)}",
                        "confidence": 0.0,
                        "execution_time": 0
                    })

            # Select best response (Claude has priority, then by confidence)
            best_response = None
            for response in responses:
                if response["agent"] == "claude":
                    best_response = response
                    break

            if not best_response:
                best_response = max(responses, key=lambda r: r["confidence"])

            return CollaborationResult(
                success=True,
                final_solution=best_response["content"],
                agent_responses=responses,
                consensus_score=0.8  # Good but not perfect consensus
            )

        except Exception as e:
            return CollaborationResult(
                success=False,
                final_solution=f"Parallel execution failed: {str(e)}"
            )

    async def _debate_execution(self, task_description: str, agents: List[str], max_rounds: int) -> CollaborationResult:
        """Execute task using debate methodology."""
        try:
            # Start with initial proposals
            proposals = await self._gather_initial_proposals(task_description, agents)

            # Run debate rounds
            debate_history = []
            for round_num in range(1, max_rounds + 1):
                round_result = await self._run_debate_round(
                    task_description, agents, round_num, 300
                )
                debate_history.append(round_result)

            # Generate final consensus through voting
            final_result = await self._generate_voting_consensus(proposals, debate_history)

            return CollaborationResult(
                success=True,
                final_solution=final_result["solution"],
                agent_responses=proposals,
                consensus_score=final_result["consensus_score"],
                debate_summary=final_result["debate_summary"]
            )

        except Exception as e:
            return CollaborationResult(
                success=False,
                final_solution=f"Debate execution failed: {str(e)}"
            )

    async def _consensus_execution(self, task_description: str, agents: List[str], require_consensus: bool) -> CollaborationResult:
        """Execute task requiring consensus."""
        # For now, use debate as consensus mechanism
        return await self._debate_execution(task_description, agents, 2)

    async def _gather_initial_proposals(self, task_description: str, agents: List[str]) -> List[Dict[str, Any]]:
        """Gather initial proposals from all agents."""
        proposals = []
        context = TaskContext(task_type=TaskType.ANALYSIS, agent_role="proposer")

        for agent_name in agents:
            try:
                agent = self.agents[agent_name]
                response = await agent.propose_solution(task_description, [], context)
                proposals.append({
                    "agent": agent_name,
                    "content": response.content,
                    "confidence": response.confidence
                })
            except Exception as e:
                logger.error(f"Failed to get proposal from {agent_name}: {e}")
                proposals.append({
                    "agent": agent_name,
                    "content": f"Proposal failed: {str(e)}",
                    "confidence": 0.0
                })

        return proposals

    async def _run_debate_round(self, topic: str, agents: List[str], round_num: int, timeout: int) -> DebateRound:
        """Run a single round of debate."""
        round_result = DebateRound(round_number=round_num, topic=topic)

        # Each agent provides their position
        for agent_name in agents:
            try:
                agent = self.agents[agent_name]
                context = TaskContext(task_type=TaskType.DEBATE, agent_role="debater")

                prompt = f"Debate Round {round_num} on: {topic}\nProvide your position and arguments."
                response = await agent.generate(prompt, context)

                round_result.responses.append({
                    "agent": agent_name,
                    "content": response.content,
                    "confidence": response.confidence
                })

            except Exception as e:
                logger.error(f"Agent {agent_name} failed in debate round {round_num}: {e}")

        # Generate critiques
        for agent_name in agents:
            try:
                agent = self.agents[agent_name]
                context = TaskContext(task_type=TaskType.CODE_REVIEW, agent_role="critic")

                # Critique other agents' responses
                other_responses = [r for r in round_result.responses if r["agent"] != agent_name]
                critique_prompt = f"Critique the following positions on {topic}:\n"
                for resp in other_responses:
                    critique_prompt += f"\n{resp['agent']}: {resp['content'][:200]}...\n"

                critique = await agent.generate(critique_prompt, context)
                round_result.critiques.append({
                    "agent": agent_name,
                    "content": critique.content
                })

            except Exception as e:
                logger.error(f"Critique failed for {agent_name}: {e}")

        return round_result

    async def _check_early_consensus(self, rounds: List[DebateRound]) -> bool:
        """Check if early consensus has been reached."""
        # Simple implementation - check if recent responses are similar
        if len(rounds) < 2:
            return False

        # For now, assume no early consensus to allow full debate
        return False

    async def _generate_final_consensus(self, debate_result: DebateResult) -> Dict[str, Any]:
        """Generate final consensus from debate."""
        try:
            # Collect all final positions
            final_positions = []
            if debate_result.rounds:
                last_round = debate_result.rounds[-1]
                final_positions = last_round.responses

            # Weighted voting
            total_weight = 0
            weighted_score = 0
            votes = {}

            for position in final_positions:
                agent_name = position["agent"]
                confidence = position["confidence"]
                weight = self.voting_weights.get(agent_name, 1.0)

                votes[agent_name] = {
                    "position": position["content"][:200] + "...",
                    "confidence": confidence,
                    "weight": weight,
                    "weighted_score": confidence * weight
                }

                total_weight += weight
                weighted_score += confidence * weight

            consensus_score = weighted_score / total_weight if total_weight > 0 else 0.0

            # Generate summary (use Claude's position as base)
            summary = "No consensus reached"
            if final_positions:
                # Priority to Claude, then highest confidence
                claude_position = next((p for p in final_positions if p["agent"] == "claude"), None)
                if claude_position:
                    summary = claude_position["content"]
                else:
                    best_position = max(final_positions, key=lambda p: p["confidence"])
                    summary = best_position["content"]

            return {
                "summary": summary,
                "score": consensus_score,
                "votes": votes
            }

        except Exception as e:
            logger.error(f"Consensus generation failed: {e}")
            return {
                "summary": "Consensus generation failed",
                "score": 0.0,
                "votes": {}
            }

    async def _generate_voting_consensus(self, proposals: List[Dict[str, Any]], debate_history: List[DebateRound]) -> Dict[str, Any]:
        """Generate consensus through weighted voting."""
        try:
            # Use Claude's final proposal as the solution (senior developer authority)
            claude_proposal = next((p for p in proposals if p["agent"] == "claude"), None)

            if claude_proposal:
                solution = claude_proposal["content"]
                consensus_score = 0.9  # High confidence in Claude's solution
            else:
                # Fallback to highest confidence proposal
                best_proposal = max(proposals, key=lambda p: p["confidence"])
                solution = best_proposal["content"]
                consensus_score = best_proposal["confidence"]

            # Generate debate summary
            debate_summary = f"Completed {len(debate_history)} rounds of debate with {len(proposals)} agents."

            return {
                "solution": solution,
                "consensus_score": consensus_score,
                "debate_summary": debate_summary
            }

        except Exception as e:
            logger.error(f"Voting consensus failed: {e}")
            return {
                "solution": "Consensus failed",
                "consensus_score": 0.0,
                "debate_summary": f"Voting failed: {str(e)}"
            }
</file>

<file path="src/persistence/__init__.py">
"""Database and caching layer."""

from .database import DatabaseManager
from .models import Base, Conversation, Message, TaskExecution
from .cache import CacheManager

__all__ = ["DatabaseManager", "Base", "Conversation", "Message", "TaskExecution", "CacheManager"]
</file>

<file path="src/persistence/cache.py">
"""
Cache module for AngelaMCP.

Simple cache manager implementation.
"""

class CacheManager:
    """Simple cache manager for development."""
    
    def __init__(self):
        self._cache = {}
    
    async def get(self, key: str):
        """Get value from cache."""
        return self._cache.get(key)
    
    async def set(self, key: str, value, ttl: int = None):
        """Set value in cache."""
        self._cache[key] = value
    
    async def delete(self, key: str):
        """Delete key from cache."""
        self._cache.pop(key, None)
    
    async def clear(self):
        """Clear all cache."""
        self._cache.clear()
</file>

<file path="src/persistence/database.py">
"""
Database connection management for AngelaMCP.

This handles both PostgreSQL and Redis connections with proper async support.
I'm implementing production-grade connection pooling and error handling.
"""

import asyncio
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any, AsyncGenerator
import redis.asyncio as redis
from sqlalchemy.ext.asyncio import (
    create_async_engine, 
    AsyncSession, 
    async_sessionmaker,
    AsyncEngine
)
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy import text

from src.persistence.models import Base
from src.utils.logger import get_logger
from src.utils.exceptions import DatabaseError
from config.settings import settings


class DatabaseManager:
    """
    Manages database connections and sessions.
    
    Handles both PostgreSQL (main storage) and Redis (caching/sessions).
    I'm implementing proper connection pooling and error handling.
    """
    
    def __init__(self):
        self.logger = get_logger("database.manager")
        
        # PostgreSQL
        self.engine: Optional[AsyncEngine] = None
        self.session_factory: Optional[async_sessionmaker] = None
        
        # Redis
        self.redis_client: Optional[redis.Redis] = None
        
        # Connection status
        self._initialized = False
        self._postgres_healthy = False
        self._redis_healthy = False
    
    async def initialize(self) -> None:
        """Initialize database connections."""
        if self._initialized:
            self.logger.warning("Database manager already initialized")
            return
        
        try:
            self.logger.info("Initializing database connections...")
            
            # Initialize PostgreSQL
            await self._initialize_postgres()
            
            # Initialize Redis
            await self._initialize_redis()
            
            # Verify connections
            await self._verify_connections()
            
            self._initialized = True
            self.logger.info("Database manager initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}", exc_info=True)
            await self.close()
            raise DatabaseError(f"Failed to initialize database: {e}")
    
    async def _initialize_postgres(self) -> None:
        """Initialize PostgreSQL connection."""
        try:
            # Create async engine
            self.engine = create_async_engine(
                str(settings.database_url).replace("postgresql://", "postgresql+asyncpg://"),
                pool_size=settings.database_pool_size,
                max_overflow=settings.database_max_overflow,
                pool_timeout=settings.database_pool_timeout,
                echo=settings.database_echo,
                pool_pre_ping=True,  # Verify connections before use
                pool_recycle=3600,   # Recycle connections every hour
            )
            
            # Create session factory
            self.session_factory = async_sessionmaker(
                bind=self.engine,
                class_=AsyncSession,
                expire_on_commit=False
            )
            
            self.logger.info("PostgreSQL engine initialized")
            
        except Exception as e:
            raise DatabaseError(f"Failed to initialize PostgreSQL: {e}")
    
    async def _initialize_redis(self) -> None:
        """Initialize Redis connection."""
        try:
            self.redis_client = redis.from_url(
                str(settings.redis_url),
                max_connections=settings.redis_max_connections,
                decode_responses=settings.redis_decode_responses,
                socket_timeout=settings.redis_socket_timeout,
                socket_connect_timeout=settings.redis_connection_timeout,
                retry_on_timeout=True,
                health_check_interval=30
            )
            
            self.logger.info("Redis client initialized")
            
        except Exception as e:
            raise DatabaseError(f"Failed to initialize Redis: {e}")
    
    async def _verify_connections(self) -> None:
        """Verify database connections are working."""
        # Test PostgreSQL
        try:
            async with self.engine.begin() as conn:
                result = await conn.execute(text("SELECT 1"))
                assert result.scalar() == 1
            self._postgres_healthy = True
            self.logger.info("‚úÖ PostgreSQL connection verified")
        except Exception as e:
            self._postgres_healthy = False
            raise DatabaseError(f"PostgreSQL connection verification failed: {e}")
        
        # Test Redis
        try:
            await self.redis_client.ping()
            self._redis_healthy = True
            self.logger.info("‚úÖ Redis connection verified")
        except Exception as e:
            self._redis_healthy = False
            raise DatabaseError(f"Redis connection verification failed: {e}")
    
    @asynccontextmanager
    async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
        """Get a database session with automatic cleanup."""
        if not self._initialized or not self.session_factory:
            raise DatabaseError("Database not initialized")
        
        session = self.session_factory()
        try:
            yield session
            await session.commit()
        except Exception as e:
            await session.rollback()
            self.logger.error(f"Database session error: {e}")
            raise DatabaseError(f"Database operation failed: {e}")
        finally:
            await session.close()
    
    async def get_redis(self) -> redis.Redis:
        """Get Redis client."""
        if not self._initialized or not self.redis_client:
            raise DatabaseError("Redis not initialized")
        
        return self.redis_client
    
    async def create_tables(self) -> None:
        """Create all database tables."""
        if not self.engine:
            raise DatabaseError("Database engine not initialized")
        
        try:
            self.logger.info("Creating database tables...")
            async with self.engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
            self.logger.info("‚úÖ Database tables created")
        except Exception as e:
            raise DatabaseError(f"Failed to create tables: {e}")
    
    async def drop_tables(self) -> None:
        """Drop all database tables (use with caution!)."""
        if not self.engine:
            raise DatabaseError("Database engine not initialized")
        
        try:
            self.logger.warning("Dropping all database tables...")
            async with self.engine.begin() as conn:
                await conn.run_sync(Base.metadata.drop_all)
            self.logger.info("Database tables dropped")
        except Exception as e:
            raise DatabaseError(f"Failed to drop tables: {e}")
    
    async def health_check(self) -> Dict[str, Any]:
        """Check health of all database connections."""
        health_status = {
            "overall": "healthy",
            "postgres": {"status": "unknown"},
            "redis": {"status": "unknown"}
        }
        
        # Check PostgreSQL
        try:
            if self.engine:
                async with self.engine.begin() as conn:
                    start_time = asyncio.get_event_loop().time()
                    result = await conn.execute(text("SELECT version()"))
                    response_time = asyncio.get_event_loop().time() - start_time
                    
                    health_status["postgres"] = {
                        "status": "healthy",
                        "response_time": response_time,
                        "version": result.scalar()[:50] + "...",
                        "pool_size": self.engine.pool.size(),
                        "checked_out": self.engine.pool.checkedout()
                    }
                    self._postgres_healthy = True
            else:
                health_status["postgres"] = {"status": "not_initialized"}
        except Exception as e:
            health_status["postgres"] = {
                "status": "unhealthy",
                "error": str(e)
            }
            self._postgres_healthy = False
        
        # Check Redis
        try:
            if self.redis_client:
                start_time = asyncio.get_event_loop().time()
                await self.redis_client.ping()
                response_time = asyncio.get_event_loop().time() - start_time
                
                info = await self.redis_client.info()
                health_status["redis"] = {
                    "status": "healthy",
                    "response_time": response_time,
                    "version": info.get("redis_version", "unknown"),
                    "connected_clients": info.get("connected_clients", 0),
                    "used_memory": info.get("used_memory_human", "unknown")
                }
                self._redis_healthy = True
            else:
                health_status["redis"] = {"status": "not_initialized"}
        except Exception as e:
            health_status["redis"] = {
                "status": "unhealthy", 
                "error": str(e)
            }
            self._redis_healthy = False
        
        # Overall status
        if not self._postgres_healthy or not self._redis_healthy:
            health_status["overall"] = "degraded"
        
        if not self._postgres_healthy and not self._redis_healthy:
            health_status["overall"] = "unhealthy"
        
        return health_status
    
    async def execute_raw_query(self, query: str, params: Optional[Dict[str, Any]] = None) -> Any:
        """Execute a raw SQL query."""
        if not self.engine:
            raise DatabaseError("Database engine not initialized")
        
        try:
            async with self.engine.begin() as conn:
                result = await conn.execute(text(query), params or {})
                return result
        except SQLAlchemyError as e:
            raise DatabaseError(f"Query execution failed: {e}")
    
    async def get_table_stats(self) -> Dict[str, Any]:
        """Get statistics about database tables."""
        if not self.engine:
            raise DatabaseError("Database engine not initialized")
        
        try:
            stats = {}
            async with self.engine.begin() as conn:
                # Get table row counts
                tables = ["conversations", "messages", "task_executions", "debate_rounds", "agent_responses"]
                
                for table in tables:
                    result = await conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                    stats[table] = result.scalar()
                
                # Get database size
                result = await conn.execute(text("SELECT pg_size_pretty(pg_database_size(current_database()))"))
                stats["database_size"] = result.scalar()
                
            return stats
            
        except Exception as e:
            self.logger.error(f"Failed to get table stats: {e}")
            return {"error": str(e)}
    
    async def cleanup_old_data(self, days_old: int = 30) -> Dict[str, int]:
        """Clean up old data from the database."""
        if not self.engine:
            raise DatabaseError("Database engine not initialized")
        
        try:
            cleanup_stats = {}
            
            async with self.get_session() as session:
                # Clean up old conversations
                result = await session.execute(text("""
                    DELETE FROM conversations 
                    WHERE created_at < NOW() - INTERVAL '%s days'
                    AND status = 'completed'
                """), {"days": days_old})
                cleanup_stats["conversations_deleted"] = result.rowcount
                
                # Clean up old metrics
                result = await session.execute(text("""
                    DELETE FROM session_metrics 
                    WHERE timestamp < NOW() - INTERVAL '%s days'
                """), {"days": days_old})
                cleanup_stats["metrics_deleted"] = result.rowcount
            
            self.logger.info(f"Cleanup completed: {cleanup_stats}")
            return cleanup_stats
            
        except Exception as e:
            raise DatabaseError(f"Data cleanup failed: {e}")
    
    async def close(self) -> None:
        """Close all database connections."""
        try:
            # Close PostgreSQL
            if self.engine:
                await self.engine.dispose()
                self.engine = None
                self.session_factory = None
                self.logger.info("PostgreSQL connections closed")
            
            # Close Redis
            if self.redis_client:
                await self.redis_client.close()
                self.redis_client = None
                self.logger.info("Redis connections closed")
            
            self._initialized = False
            self._postgres_healthy = False
            self._redis_healthy = False
            
        except Exception as e:
            self.logger.error(f"Error closing database connections: {e}")
    
    def is_healthy(self) -> bool:
        """Check if database connections are healthy."""
        return self._initialized and self._postgres_healthy and self._redis_healthy
    
    @property
    def postgres_healthy(self) -> bool:
        """Check if PostgreSQL is healthy."""
        return self._postgres_healthy
    
    @property
    def redis_healthy(self) -> bool:
        """Check if Redis is healthy."""
        return self._redis_healthy
</file>

<file path="src/persistence/models.py">
"""
Database models for AngelaMCP.

This defines all the database tables and relationships.
I'm using SQLAlchemy with async support for production-grade persistence.
"""

import uuid
from datetime import datetime
from typing import Optional, Dict, Any, List
from sqlalchemy import (
    String, Text, DateTime, Float, Integer, Boolean, JSON,
    ForeignKey, Index, UniqueConstraint
)
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy.sql import func


Base = declarative_base()


class Conversation(Base):
    """
    Represents a conversation session with multiple agents.
    """
    __tablename__ = "conversations"
    
    # Primary key
    id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        primary_key=True, 
        default=lambda: str(uuid.uuid4())
    )
    
    # Basic info
    title: Mapped[Optional[str]] = mapped_column(String(255))
    description: Mapped[Optional[str]] = mapped_column(Text)
    status: Mapped[str] = mapped_column(String(50), default="active")  # active, completed, archived
    
    # Collaboration details
    collaboration_strategy: Mapped[Optional[str]] = mapped_column(String(50))  # debate, parallel, consensus
    participants: Mapped[List[str]] = mapped_column(JSON, default=list)  # List of agent names
    consensus_score: Mapped[Optional[float]] = mapped_column(Float)
    
    # Metadata
    metadata: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    user_preferences: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    
    # Timestamps
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), 
        server_default=func.now(), 
        onupdate=func.now()
    )
    completed_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True))
    
    # Relationships
    messages: Mapped[List["Message"]] = relationship(
        "Message", 
        back_populates="conversation",
        cascade="all, delete-orphan",
        order_by="Message.created_at"
    )
    task_executions: Mapped[List["TaskExecution"]] = relationship(
        "TaskExecution",
        back_populates="conversation",
        cascade="all, delete-orphan"
    )
    
    # Indexes
    __table_args__ = (
        Index("idx_conversations_status", "status"),
        Index("idx_conversations_created_at", "created_at"),
        Index("idx_conversations_strategy", "collaboration_strategy"),
    )
    
    def __repr__(self) -> str:
        return f"<Conversation(id={self.id}, title={self.title}, status={self.status})>"


class Message(Base):
    """
    Individual messages within a conversation.
    """
    __tablename__ = "messages"
    
    # Primary key
    id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        primary_key=True, 
        default=lambda: str(uuid.uuid4())
    )
    
    # Foreign keys
    conversation_id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        ForeignKey("conversations.id", ondelete="CASCADE"),
        nullable=False
    )
    
    # Message details
    role: Mapped[str] = mapped_column(String(50))  # user, assistant, system
    agent_type: Mapped[Optional[str]] = mapped_column(String(50))  # claude, openai, gemini
    agent_name: Mapped[Optional[str]] = mapped_column(String(100))
    
    # Content
    content: Mapped[str] = mapped_column(Text)
    content_type: Mapped[str] = mapped_column(String(50), default="text")  # text, code, json
    
    # Agent response metadata
    confidence: Mapped[Optional[float]] = mapped_column(Float)
    execution_time_ms: Mapped[Optional[float]] = mapped_column(Float)
    token_usage: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    
    # Context
    task_type: Mapped[Optional[str]] = mapped_column(String(50))
    agent_role: Mapped[Optional[str]] = mapped_column(String(50))
    
    # Metadata
    metadata: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    error_info: Mapped[Optional[Dict[str, Any]]] = mapped_column(JSON)
    
    # Timestamps
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    conversation: Mapped[Conversation] = relationship("Conversation", back_populates="messages")
    
    # Indexes
    __table_args__ = (
        Index("idx_messages_conversation_id", "conversation_id"),
        Index("idx_messages_agent_type", "agent_type"),
        Index("idx_messages_created_at", "created_at"),
        Index("idx_messages_role", "role"),
    )
    
    def __repr__(self) -> str:
        return f"<Message(id={self.id}, agent={self.agent_type}, role={self.role})>"


class TaskExecution(Base):
    """
    Records of task executions and their results.
    """
    __tablename__ = "task_executions"
    
    # Primary key
    id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        primary_key=True, 
        default=lambda: str(uuid.uuid4())
    )
    
    # Foreign keys
    conversation_id: Mapped[Optional[str]] = mapped_column(
        UUID(as_uuid=False), 
        ForeignKey("conversations.id", ondelete="CASCADE")
    )
    
    # Task details
    task_description: Mapped[str] = mapped_column(Text)
    task_type: Mapped[str] = mapped_column(String(50))
    complexity_score: Mapped[Optional[float]] = mapped_column(Float)
    
    # Execution details
    strategy_used: Mapped[str] = mapped_column(String(50))  # single_agent, parallel, debate, consensus
    participants: Mapped[List[str]] = mapped_column(JSON, default=list)
    
    # Results
    success: Mapped[bool] = mapped_column(Boolean, default=False)
    final_solution: Mapped[Optional[str]] = mapped_column(Text)
    consensus_score: Mapped[Optional[float]] = mapped_column(Float)
    
    # Performance metrics
    execution_time_ms: Mapped[Optional[float]] = mapped_column(Float)
    total_tokens: Mapped[Optional[int]] = mapped_column(Integer)
    total_cost_usd: Mapped[Optional[float]] = mapped_column(Float)
    
    # Metadata
    constraints: Mapped[List[str]] = mapped_column(JSON, default=list)
    metadata: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    error_info: Mapped[Optional[Dict[str, Any]]] = mapped_column(JSON)
    
    # Timestamps
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
    started_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True))
    completed_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True))
    
    # Relationships
    conversation: Mapped[Optional[Conversation]] = relationship("TaskExecution", back_populates="task_executions")
    debate_rounds: Mapped[List["DebateRound"]] = relationship(
        "DebateRound",
        back_populates="task_execution",
        cascade="all, delete-orphan"
    )
    agent_responses: Mapped[List["AgentResponse"]] = relationship(
        "AgentResponse",
        back_populates="task_execution", 
        cascade="all, delete-orphan"
    )
    
    # Indexes
    __table_args__ = (
        Index("idx_task_executions_conversation_id", "conversation_id"),
        Index("idx_task_executions_strategy", "strategy_used"),
        Index("idx_task_executions_success", "success"),
        Index("idx_task_executions_created_at", "created_at"),
    )
    
    def __repr__(self) -> str:
        return f"<TaskExecution(id={self.id}, strategy={self.strategy_used}, success={self.success})>"


class DebateRound(Base):
    """
    Individual rounds in a debate session.
    """
    __tablename__ = "debate_rounds"
    
    # Primary key
    id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        primary_key=True, 
        default=lambda: str(uuid.uuid4())
    )
    
    # Foreign keys
    task_execution_id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        ForeignKey("task_executions.id", ondelete="CASCADE"),
        nullable=False
    )
    
    # Round details
    round_number: Mapped[int] = mapped_column(Integer)
    topic: Mapped[str] = mapped_column(Text)
    participants: Mapped[List[str]] = mapped_column(JSON, default=list)
    
    # Results
    round_summary: Mapped[Optional[str]] = mapped_column(Text)
    consensus_reached: Mapped[bool] = mapped_column(Boolean, default=False)
    
    # Performance
    duration_ms: Mapped[Optional[float]] = mapped_column(Float)
    
    # Metadata
    metadata: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    
    # Timestamps
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
    started_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True))
    completed_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True))
    
    # Relationships
    task_execution: Mapped[TaskExecution] = relationship("TaskExecution", back_populates="debate_rounds")
    agent_responses: Mapped[List["AgentResponse"]] = relationship(
        "AgentResponse",
        back_populates="debate_round",
        cascade="all, delete-orphan"
    )
    
    # Indexes
    __table_args__ = (
        Index("idx_debate_rounds_task_execution_id", "task_execution_id"),
        Index("idx_debate_rounds_round_number", "round_number"),
        UniqueConstraint("task_execution_id", "round_number", name="uq_task_round"),
    )
    
    def __repr__(self) -> str:
        return f"<DebateRound(id={self.id}, round={self.round_number}, consensus={self.consensus_reached})>"


class AgentResponse(Base):
    """
    Individual agent responses within tasks and debates.
    """
    __tablename__ = "agent_responses"
    
    # Primary key
    id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        primary_key=True, 
        default=lambda: str(uuid.uuid4())
    )
    
    # Foreign keys
    task_execution_id: Mapped[Optional[str]] = mapped_column(
        UUID(as_uuid=False), 
        ForeignKey("task_executions.id", ondelete="CASCADE")
    )
    debate_round_id: Mapped[Optional[str]] = mapped_column(
        UUID(as_uuid=False), 
        ForeignKey("debate_rounds.id", ondelete="CASCADE")
    )
    
    # Agent details
    agent_type: Mapped[str] = mapped_column(String(50))  # claude, openai, gemini
    agent_name: Mapped[str] = mapped_column(String(100))
    
    # Response details
    response_type: Mapped[str] = mapped_column(String(50))  # proposal, critique, vote, analysis
    content: Mapped[str] = mapped_column(Text)
    confidence: Mapped[float] = mapped_column(Float, default=0.8)
    
    # Performance metrics
    execution_time_ms: Mapped[float] = mapped_column(Float, default=0.0)
    token_usage: Mapped[Dict[str, int]] = mapped_column(JSON, default=dict)
    cost_usd: Mapped[Optional[float]] = mapped_column(Float)
    
    # Context
    task_type: Mapped[Optional[str]] = mapped_column(String(50))
    agent_role: Mapped[Optional[str]] = mapped_column(String(50))
    
    # Metadata
    metadata: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    error_info: Mapped[Optional[Dict[str, Any]]] = mapped_column(JSON)
    
    # Timestamps
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    task_execution: Mapped[Optional[TaskExecution]] = relationship("TaskExecution", back_populates="agent_responses")
    debate_round: Mapped[Optional[DebateRound]] = relationship("DebateRound", back_populates="agent_responses")
    
    # Indexes
    __table_args__ = (
        Index("idx_agent_responses_task_execution_id", "task_execution_id"),
        Index("idx_agent_responses_debate_round_id", "debate_round_id"),
        Index("idx_agent_responses_agent_type", "agent_type"),
        Index("idx_agent_responses_response_type", "response_type"),
        Index("idx_agent_responses_created_at", "created_at"),
    )
    
    def __repr__(self) -> str:
        return f"<AgentResponse(id={self.id}, agent={self.agent_type}, type={self.response_type})>"


class SessionMetrics(Base):
    """
    Performance and usage metrics for monitoring.
    """
    __tablename__ = "session_metrics"
    
    # Primary key
    id: Mapped[str] = mapped_column(
        UUID(as_uuid=False), 
        primary_key=True, 
        default=lambda: str(uuid.uuid4())
    )
    
    # Session details
    session_id: Mapped[str] = mapped_column(String(255))
    metric_type: Mapped[str] = mapped_column(String(50))  # performance, usage, error, cost
    
    # Metric data
    metric_name: Mapped[str] = mapped_column(String(100))
    metric_value: Mapped[float] = mapped_column(Float)
    metric_unit: Mapped[str] = mapped_column(String(50))  # ms, tokens, usd, count
    
    # Context
    agent_type: Mapped[Optional[str]] = mapped_column(String(50))
    task_type: Mapped[Optional[str]] = mapped_column(String(50))
    
    # Metadata
    metadata: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    
    # Timestamps
    timestamp: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
    
    # Indexes
    __table_args__ = (
        Index("idx_session_metrics_session_id", "session_id"),
        Index("idx_session_metrics_type", "metric_type"),
        Index("idx_session_metrics_name", "metric_name"),
        Index("idx_session_metrics_timestamp", "timestamp"),
        Index("idx_session_metrics_agent_type", "agent_type"),
    )
    
    def __repr__(self) -> str:
        return f"<SessionMetrics(id={self.id}, type={self.metric_type}, name={self.metric_name})>"
</file>

<file path="src/persistence/repositories.py">
"""
Repository pattern implementation for AngelaMCP.
Provides data access layer for all database operations.
"""

import logging
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Union
from uuid import UUID, uuid4

from sqlalchemy import desc, func, and_, or_
from sqlalchemy.orm import Session
from sqlalchemy.ext.asyncio import AsyncSession

from src.models.database import (
    Conversation, Message, TaskExecution, AgentProposal,
    TaskStatus, AgentType, MessageRole
)


class BaseRepository:
    """Base repository with common database operations."""
    
    def __init__(self, session: AsyncSession):
        self.session = session
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def commit(self) -> None:
        """Commit current transaction."""
        await self.session.commit()
    
    async def rollback(self) -> None:
        """Rollback current transaction."""
        await self.session.rollback()
    
    async def refresh(self, instance) -> None:
        """Refresh instance from database."""
        await self.session.refresh(instance)


class ConversationRepository(BaseRepository):
    """Repository for conversation management."""
    
    async def create_conversation(
        self,
        session_id: UUID,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Conversation:
        """Create a new conversation."""
        conversation = Conversation(
            session_id=session_id,
            metadata=metadata or {}
        )
        
        self.session.add(conversation)
        await self.commit()
        await self.refresh(conversation)
        
        self.logger.info(f"Created conversation {conversation.id} for session {session_id}")
        return conversation
    
    async def get_conversation(self, conversation_id: UUID) -> Optional[Conversation]:
        """Get conversation by ID."""
        result = await self.session.get(Conversation, conversation_id)
        return result
    
    async def get_active_conversation(self, session_id: UUID) -> Optional[Conversation]:
        """Get active conversation for session."""
        result = await self.session.execute(
            self.session.query(Conversation)
            .filter(
                and_(
                    Conversation.session_id == session_id,
                    Conversation.ended_at.is_(None)
                )
            )
            .order_by(desc(Conversation.started_at))
            .limit(1)
        )
        return result.scalar_one_or_none()
    
    async def end_conversation(self, conversation_id: UUID) -> Optional[Conversation]:
        """End a conversation."""
        conversation = await self.get_conversation(conversation_id)
        if conversation:
            conversation.ended_at = datetime.utcnow()
            conversation.status = "completed"
            await self.commit()
            
        return conversation
    
    async def get_conversation_history(
        self,
        session_id: Optional[UUID] = None,
        limit: int = 50,
        offset: int = 0
    ) -> List[Conversation]:
        """Get conversation history."""
        query = self.session.query(Conversation)
        
        if session_id:
            query = query.filter(Conversation.session_id == session_id)
            
        query = query.order_by(desc(Conversation.started_at))
        query = query.limit(limit).offset(offset)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def update_conversation_metadata(
        self,
        conversation_id: UUID,
        metadata: Dict[str, Any]
    ) -> Optional[Conversation]:
        """Update conversation metadata."""
        conversation = await self.get_conversation(conversation_id)
        if conversation:
            conversation.metadata.update(metadata)
            await self.commit()
            
        return conversation


class MessageRepository(BaseRepository):
    """Repository for message management."""
    
    async def create_message(
        self,
        conversation_id: UUID,
        agent_type: AgentType,
        role: MessageRole,
        content: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Message:
        """Create a new message."""
        message = Message(
            conversation_id=conversation_id,
            agent_type=agent_type,
            role=role,
            content=content,
            metadata=metadata or {}
        )
        
        self.session.add(message)
        await self.commit()
        await self.refresh(message)
        
        self.logger.debug(f"Created message {message.id} from {agent_type} in conversation {conversation_id}")
        return message
    
    async def get_message(self, message_id: UUID) -> Optional[Message]:
        """Get message by ID."""
        return await self.session.get(Message, message_id)
    
    async def get_conversation_messages(
        self,
        conversation_id: UUID,
        limit: int = 100,
        offset: int = 0,
        agent_type: Optional[AgentType] = None
    ) -> List[Message]:
        """Get messages for a conversation."""
        query = self.session.query(Message).filter(Message.conversation_id == conversation_id)
        
        if agent_type:
            query = query.filter(Message.agent_type == agent_type)
            
        query = query.order_by(Message.created_at)
        query = query.limit(limit).offset(offset)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def get_recent_messages(
        self,
        session_id: Optional[UUID] = None,
        limit: int = 20,
        agent_type: Optional[AgentType] = None
    ) -> List[Message]:
        """Get recent messages across conversations."""
        query = self.session.query(Message)
        
        if session_id:
            query = query.join(Conversation).filter(Conversation.session_id == session_id)
            
        if agent_type:
            query = query.filter(Message.agent_type == agent_type)
            
        query = query.order_by(desc(Message.created_at))
        query = query.limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def update_message_metadata(
        self,
        message_id: UUID,
        metadata: Dict[str, Any]
    ) -> Optional[Message]:
        """Update message metadata."""
        message = await self.get_message(message_id)
        if message:
            message.metadata.update(metadata)
            await self.commit()
            
        return message
    
    async def search_messages(
        self,
        query: str,
        conversation_id: Optional[UUID] = None,
        limit: int = 50
    ) -> List[Message]:
        """Search messages by content."""
        search_query = self.session.query(Message).filter(
            Message.content.ilike(f"%{query}%")
        )
        
        if conversation_id:
            search_query = search_query.filter(Message.conversation_id == conversation_id)
            
        search_query = search_query.order_by(desc(Message.created_at))
        search_query = search_query.limit(limit)
        
        result = await self.session.execute(search_query)
        return result.scalars().all()


class TaskExecutionRepository(BaseRepository):
    """Repository for task execution management."""
    
    async def create_task_execution(
        self,
        conversation_id: UUID,
        task_type: str,
        input_data: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None
    ) -> TaskExecution:
        """Create a new task execution."""
        task_execution = TaskExecution(
            conversation_id=conversation_id,
            task_type=task_type,
            status=TaskStatus.PENDING,
            input_data=input_data,
            metadata=metadata or {}
        )
        
        self.session.add(task_execution)
        await self.commit()
        await self.refresh(task_execution)
        
        self.logger.info(f"Created task execution {task_execution.id} of type {task_type}")
        return task_execution
    
    async def get_task_execution(self, task_id: UUID) -> Optional[TaskExecution]:
        """Get task execution by ID."""
        return await self.session.get(TaskExecution, task_id)
    
    async def update_task_status(
        self,
        task_id: UUID,
        status: TaskStatus,
        output_data: Optional[Dict[str, Any]] = None
    ) -> Optional[TaskExecution]:
        """Update task execution status."""
        task = await self.get_task_execution(task_id)
        if task:
            task.status = status
            if output_data:
                task.output_data = output_data
                
            if status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                task.completed_at = datetime.utcnow()
                
            await self.commit()
            
        return task
    
    async def get_conversation_tasks(
        self,
        conversation_id: UUID,
        status: Optional[TaskStatus] = None
    ) -> List[TaskExecution]:
        """Get task executions for a conversation."""
        query = self.session.query(TaskExecution).filter(
            TaskExecution.conversation_id == conversation_id
        )
        
        if status:
            query = query.filter(TaskExecution.status == status)
            
        query = query.order_by(desc(TaskExecution.started_at))
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def get_active_tasks(self) -> List[TaskExecution]:
        """Get all active task executions."""
        result = await self.session.execute(
            self.session.query(TaskExecution).filter(
                TaskExecution.status.in_([TaskStatus.PENDING, TaskStatus.RUNNING])
            ).order_by(TaskExecution.started_at)
        )
        return result.scalars().all()
    
    async def update_task_metadata(
        self,
        task_id: UUID,
        metadata: Dict[str, Any]
    ) -> Optional[TaskExecution]:
        """Update task execution metadata."""
        task = await self.get_task_execution(task_id)
        if task:
            task.metadata.update(metadata)
            await self.commit()
            
        return task
    
    async def get_task_metrics(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """Get task execution metrics."""
        query = self.session.query(TaskExecution)
        
        if start_date:
            query = query.filter(TaskExecution.started_at >= start_date)
        if end_date:
            query = query.filter(TaskExecution.started_at <= end_date)
            
        # Get status counts
        status_counts = await self.session.execute(
            query.with_entities(
                TaskExecution.status,
                func.count(TaskExecution.id)
            ).group_by(TaskExecution.status)
        )
        
        # Get average execution time
        avg_time = await self.session.execute(
            query.filter(TaskExecution.completed_at.isnot(None))
            .with_entities(
                func.avg(
                    func.extract('epoch', TaskExecution.completed_at - TaskExecution.started_at)
                )
            )
        )
        
        return {
            "status_counts": dict(status_counts.fetchall()),
            "average_execution_time": avg_time.scalar() or 0,
            "total_tasks": await self.session.execute(
                query.with_entities(func.count(TaskExecution.id))
            ).scalar()
        }


class AgentProposalRepository(BaseRepository):
    """Repository for agent proposal management."""
    
    async def create_proposal(
        self,
        task_execution_id: UUID,
        agent_type: AgentType,
        proposal_content: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> AgentProposal:
        """Create a new agent proposal."""
        proposal = AgentProposal(
            task_execution_id=task_execution_id,
            agent_type=agent_type,
            proposal_content=proposal_content,
            metadata=metadata or {}
        )
        
        self.session.add(proposal)
        await self.commit()
        await self.refresh(proposal)
        
        self.logger.info(f"Created proposal {proposal.id} from {agent_type}")
        return proposal
    
    async def get_proposal(self, proposal_id: UUID) -> Optional[AgentProposal]:
        """Get proposal by ID."""
        return await self.session.get(AgentProposal, proposal_id)
    
    async def get_task_proposals(self, task_execution_id: UUID) -> List[AgentProposal]:
        """Get all proposals for a task."""
        result = await self.session.execute(
            self.session.query(AgentProposal)
            .filter(AgentProposal.task_execution_id == task_execution_id)
            .order_by(AgentProposal.created_at)
        )
        return result.scalars().all()
    
    async def add_critique(
        self,
        proposal_id: UUID,
        critique_data: Dict[str, Any]
    ) -> Optional[AgentProposal]:
        """Add critique data to a proposal."""
        proposal = await self.get_proposal(proposal_id)
        if proposal:
            if not proposal.critique_data:
                proposal.critique_data = {}
            proposal.critique_data.update(critique_data)
            await self.commit()
            
        return proposal
    
    async def increment_vote_count(self, proposal_id: UUID) -> Optional[AgentProposal]:
        """Increment vote count for a proposal."""
        proposal = await self.get_proposal(proposal_id)
        if proposal:
            proposal.vote_count += 1
            await self.commit()
            
        return proposal
    
    async def get_winning_proposal(self, task_execution_id: UUID) -> Optional[AgentProposal]:
        """Get the proposal with highest vote count."""
        result = await self.session.execute(
            self.session.query(AgentProposal)
            .filter(AgentProposal.task_execution_id == task_execution_id)
            .order_by(desc(AgentProposal.vote_count))
            .limit(1)
        )
        return result.scalar_one_or_none()
    
    async def update_proposal_metadata(
        self,
        proposal_id: UUID,
        metadata: Dict[str, Any]
    ) -> Optional[AgentProposal]:
        """Update proposal metadata."""
        proposal = await self.get_proposal(proposal_id)
        if proposal:
            proposal.metadata.update(metadata)
            await self.commit()
            
        return proposal


class RepositoryManager:
    """Central manager for all repositories."""
    
    def __init__(self, session: AsyncSession):
        self.session = session
        self.conversations = ConversationRepository(session)
        self.messages = MessageRepository(session)
        self.tasks = TaskExecutionRepository(session)
        self.proposals = AgentProposalRepository(session)
        self.logger = logging.getLogger(__name__)
    
    async def cleanup_old_data(self, days_to_keep: int = 30) -> Dict[str, int]:
        """Clean up old data beyond retention period."""
        cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
        
        # Get counts before deletion
        old_conversations = await self.session.execute(
            self.session.query(func.count(Conversation.id))
            .filter(Conversation.started_at < cutoff_date)
        )
        old_messages = await self.session.execute(
            self.session.query(func.count(Message.id))
            .join(Conversation)
            .filter(Conversation.started_at < cutoff_date)
        )
        
        # Delete old data (cascading deletes will handle related records)
        await self.session.execute(
            self.session.query(Conversation)
            .filter(Conversation.started_at < cutoff_date)
            .delete()
        )
        
        await self.session.commit()
        
        cleaned_counts = {
            "conversations": old_conversations.scalar(),
            "messages": old_messages.scalar()
        }
        
        self.logger.info(f"Cleaned up old data: {cleaned_counts}")
        return cleaned_counts
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """Get system-wide statistics."""
        # Count active conversations
        active_conversations = await self.session.execute(
            self.session.query(func.count(Conversation.id))
            .filter(Conversation.ended_at.is_(None))
        )
        
        # Count total messages
        total_messages = await self.session.execute(
            self.session.query(func.count(Message.id))
        )
        
        # Count active tasks
        active_tasks = await self.session.execute(
            self.session.query(func.count(TaskExecution.id))
            .filter(TaskExecution.status.in_([TaskStatus.PENDING, TaskStatus.RUNNING]))
        )
        
        # Get agent activity
        agent_activity = await self.session.execute(
            self.session.query(
                Message.agent_type,
                func.count(Message.id)
            )
            .filter(Message.created_at >= datetime.utcnow() - timedelta(days=1))
            .group_by(Message.agent_type)
        )
        
        return {
            "active_conversations": active_conversations.scalar(),
            "total_messages": total_messages.scalar(),
            "active_tasks": active_tasks.scalar(),
            "agent_activity_24h": dict(agent_activity.fetchall())
        }
</file>

<file path="src/ui/__init__.py">
"""Rich-based terminal user interface components."""

from .terminal import TerminalUI, UIManager
from .display import DisplayManager, DisplayTheme
from .streaming import StreamingUI, RealTimeStreamer, OutputBuffer, StreamEvent, StreamEventType
from .input_handler import InputHandler, CommandProcessor, InputMode, async_input_loop

__all__ = [
    "TerminalUI",
    "UIManager",
    "DisplayManager", 
    "DisplayTheme",
    "StreamingUI",
    "RealTimeStreamer",
    "OutputBuffer",
    "StreamEvent",
    "StreamEventType",
    "InputHandler",
    "CommandProcessor",
    "InputMode",
    "async_input_loop"
]
</file>

<file path="src/ui/collaboration_ui.py">
"""
Rich Terminal UI for AngelaMCP Collaboration.

This module provides a real-time visual interface showing multi-agent
collaboration in action. Users can see agents debating, voting, and
reaching consensus live in the terminal.
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field

from rich.console import Console
from rich.layout import Layout
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskID
from rich.text import Text
from rich.table import Table
from rich.live import Live
from rich.align import Align
from rich import box

from src.orchestrator.collaboration import CollaborationOrchestrator, CollaborationRequest, CollaborationResult
from src.utils.logger import get_logger

logger = get_logger("ui.collaboration")


@dataclass
class AgentStatus:
    """Status information for an agent in the UI."""
    name: str
    emoji: str
    status: str = "idle"
    current_task: str = ""
    last_update: float = field(default_factory=time.time)
    messages: List[str] = field(default_factory=list)
    is_active: bool = False


class CollaborationUI:
    """
    Rich terminal UI for real-time multi-agent collaboration display.
    
    Shows three agent panels with live updates as they debate, critique,
    and vote on solutions. Makes the collaboration process visually engaging.
    """
    
    def __init__(self, console: Optional[Console] = None):
        """Initialize the collaboration UI."""
        self.console = console or Console()
        self.layout = self._create_layout()
        
        # Agent status tracking
        self.agents = {
            "claude_code": AgentStatus("Claude Code", "üîß", status="ready"),
            "openai": AgentStatus("OpenAI", "üß†", status="ready"), 
            "gemini": AgentStatus("Gemini", "‚ú®", status="ready")
        }
        
        # UI state
        self.current_phase = "idle"
        self.progress_message = ""
        self.collaboration_id = ""
        self.start_time = 0.0
        
        # Progress tracking
        self.progress = Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=self.console,
            transient=True
        )
        
        self.logger = get_logger("ui")
    
    def _create_layout(self) -> Layout:
        """Create the main terminal layout."""
        layout = Layout()
        
        # Split into header, body, and footer
        layout.split_column(
            Layout(name="header", size=3),
            Layout(name="body"),
            Layout(name="footer", size=3)
        )
        
        # Split body into agent panels and status
        layout["body"].split_row(
            Layout(name="agents", ratio=3),
            Layout(name="status", ratio=2)
        )
        
        # Split agents into three columns
        layout["agents"].split_row(
            Layout(name="claude", ratio=1),
            Layout(name="openai", ratio=1),
            Layout(name="gemini", ratio=1)
        )
        
        # Split status into progress and results
        layout["status"].split_column(
            Layout(name="progress", ratio=1),
            Layout(name="results", ratio=2)
        )
        
        return layout
    
    def update_header(self) -> None:
        """Update the header panel."""
        if self.collaboration_id:
            elapsed = time.time() - self.start_time if self.start_time else 0
            title = f"üé≠ AngelaMCP Collaboration [{self.collaboration_id[:8]}] - {elapsed:.1f}s"
        else:
            title = "üé≠ AngelaMCP - Multi-Agent Collaboration Platform"
        
        header_panel = Panel(
            Align.center(title),
            style="bold blue",
            box=box.ROUNDED
        )
        self.layout["header"].update(header_panel)
    
    def update_agent_panel(self, agent_key: str) -> None:
        """Update an individual agent panel."""
        agent = self.agents[agent_key]
        
        # Create status indicator
        status_color = "green" if agent.status == "ready" else "yellow" if agent.is_active else "red"
        status_text = f"[{status_color}]‚óè[/{status_color}] {agent.status.title()}"
        
        # Create content
        content_lines = [
            f"{agent.emoji} [bold]{agent.name}[/bold]",
            f"Status: {status_text}",
            ""
        ]
        
        if agent.current_task:
            content_lines.append(f"[italic]{agent.current_task}[/italic]")
            content_lines.append("")
        
        # Add recent messages (last 5)
        if agent.messages:
            content_lines.append("[dim]Recent activity:[/dim]")
            for msg in agent.messages[-5:]:
                content_lines.append(f"[dim]‚Ä¢ {msg}[/dim]")
        
        content = "\n".join(content_lines)
        
        # Style based on activity
        border_style = "green" if agent.is_active else "blue" if agent.status == "ready" else "yellow"
        
        panel = Panel(
            content,
            title=f" {agent.name} ",
            border_style=border_style,
            box=box.ROUNDED
        )
        
        self.layout[agent_key.replace("_", "")].update(panel)
    
    def update_progress_panel(self) -> None:
        """Update the progress panel."""
        content_lines = [
            f"[bold]Current Phase:[/bold] {self.current_phase.replace('_', ' ').title()}",
            ""
        ]
        
        if self.progress_message:
            content_lines.append(f"[italic]{self.progress_message}[/italic]")
        
        content_lines.extend([
            "",
            "[dim]Process Flow:[/dim]",
            "[dim]1. üí° Agents propose solutions[/dim]",
            "[dim]2. üîç Agents critique each other[/dim]", 
            "[dim]3. ‚ú® Agents refine proposals[/dim]",
            "[dim]4. üó≥Ô∏è Weighted voting[/dim]",
            "[dim]5. üèÜ Final decision[/dim]"
        ])
        
        panel = Panel(
            "\n".join(content_lines),
            title=" Progress ",
            border_style="yellow",
            box=box.ROUNDED
        )
        
        self.layout["progress"].update(panel)
    
    def update_results_panel(self, result: Optional[CollaborationResult] = None) -> None:
        """Update the results panel."""
        if result is None:
            content = "[dim]Collaboration results will appear here...[/dim]"
        else:
            content_lines = []
            
            if result.success:
                content_lines.extend([
                    f"[green]‚úÖ Success![/green]",
                    f"[bold]Winner:[/bold] {result.chosen_agent}",
                    f"[bold]Duration:[/bold] {result.total_duration:.1f}s",
                    f"[bold]Consensus:[/bold] {'Yes' if result.consensus_reached else 'No'}",
                    ""
                ])
                
                if result.voting_result:
                    content_lines.append("[bold]Vote Breakdown:[/bold]")
                    for score in result.voting_result.proposal_scores:
                        status_emoji = "üèÜ" if score.proposal.agent_name == result.chosen_agent else "üö´" if score.claude_vetoed else ""
                        content_lines.append(
                            f"‚Ä¢ {score.proposal.agent_name}: {score.weighted_score:.1f} "
                            f"(‚úÖ{score.approval_count} ‚ùå{score.rejection_count}) {status_emoji}"
                        )
            else:
                content_lines.extend([
                    "[red]‚ùå Collaboration Failed[/red]",
                    f"[bold]Error:[/bold] {result.error_message}",
                    f"[bold]Duration:[/bold] {result.total_duration:.1f}s"
                ])
            
            content = "\n".join(content_lines)
        
        panel = Panel(
            content,
            title=" Results ",
            border_style="green" if result and result.success else "red" if result else "blue",
            box=box.ROUNDED
        )
        
        self.layout["results"].update(panel)
    
    def update_footer(self) -> None:
        """Update the footer panel."""
        footer_text = "Press Ctrl+C to stop | üîß Claude Code (Senior Dev) | üß† OpenAI (Reviewer) | ‚ú® Gemini (Researcher)"
        
        footer_panel = Panel(
            Align.center(footer_text),
            style="dim",
            box=box.ROUNDED
        )
        self.layout["footer"].update(footer_panel)
    
    def refresh_display(self) -> None:
        """Refresh the entire display."""
        self.update_header()
        
        for agent_key in self.agents.keys():
            self.update_agent_panel(agent_key)
        
        self.update_progress_panel()
        self.update_results_panel()
        self.update_footer()
    
    def add_agent_message(self, agent_name: str, message: str) -> None:
        """Add a message to an agent's activity log."""
        agent_key = agent_name.lower().replace(" ", "_")
        if agent_key in self.agents:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.agents[agent_key].messages.append(f"[{timestamp}] {message}")
            self.agents[agent_key].last_update = time.time()
    
    def set_agent_status(self, agent_name: str, status: str, task: str = "") -> None:
        """Set an agent's status and current task."""
        agent_key = agent_name.lower().replace(" ", "_")
        if agent_key in self.agents:
            self.agents[agent_key].status = status
            self.agents[agent_key].current_task = task
            self.agents[agent_key].is_active = status in ["working", "thinking", "voting", "critiquing"]
            self.agents[agent_key].last_update = time.time()
    
    def set_phase(self, phase: str, message: str = "") -> None:
        """Set the current collaboration phase."""
        self.current_phase = phase
        self.progress_message = message
    
    async def run_collaboration_with_ui(
        self,
        orchestrator: CollaborationOrchestrator,
        request: CollaborationRequest
    ) -> CollaborationResult:
        """
        Run a collaboration with live UI updates.
        
        Args:
            orchestrator: The collaboration orchestrator
            request: The collaboration request
            
        Returns:
            CollaborationResult
        """
        self.collaboration_id = ""
        self.start_time = time.time()
        
        # Set up status callback for real-time updates
        def status_callback(message: str) -> None:
            # Parse status messages to update UI appropriately
            if "Starting collaboration" in message:
                self.collaboration_id = message.split()[2] if len(message.split()) > 2 else "unknown"
                self.set_phase("initializing", "Setting up agents...")
            elif "debate mode" in message:
                self.set_phase("debate_setup", "Preparing for debate...")
            elif "Getting proposal from" in message:
                agent_name = message.split("from")[1].strip() if "from" in message else "unknown"
                self.set_agent_status(agent_name, "thinking", "Creating proposal...")
                self.add_agent_message(agent_name, "Generating solution proposal")
            elif "critique" in message.lower():
                if "reviewing" in message:
                    parts = message.split()
                    if len(parts) >= 2:
                        critic = parts[1]
                        self.set_agent_status(critic, "critiquing", "Reviewing proposals...")
                        self.add_agent_message(critic, "Analyzing and critiquing solutions")
            elif "voting" in message.lower():
                if "voting on" in message:
                    agent_name = message.split()[1] if len(message.split()) > 1 else "unknown"
                    self.set_agent_status(agent_name, "voting", "Evaluating proposals...")
                    self.add_agent_message(agent_name, "Casting weighted vote")
                elif "Starting voting" in message:
                    self.set_phase("voting", "Agents voting on proposals...")
            elif "completed" in message.lower():
                winner = message.split(":")[-1].strip() if ":" in message else "unknown"
                self.set_phase("completed", f"Winner: {winner}")
                # Reset all agents to ready
                for agent_name in ["Claude Code", "OpenAI", "Gemini"]:
                    self.set_agent_status(agent_name, "ready", "")
        
        # Set the callback
        orchestrator.status_callback = status_callback
        
        # Initialize display
        self.refresh_display()
        
        # Run collaboration with live display
        with Live(self.layout, console=self.console, refresh_per_second=4) as live:
            try:
                # Start collaboration
                result = await orchestrator.collaborate(request)
                
                # Update final display
                self.update_results_panel(result)
                
                # Show completion message
                if result.success:
                    self.add_agent_message(result.chosen_agent, f"üèÜ Solution selected by consensus!")
                else:
                    self.set_phase("failed", "Collaboration failed")
                
                self.refresh_display()
                
                return result
                
            except KeyboardInterrupt:
                self.set_phase("cancelled", "Collaboration cancelled by user")
                self.refresh_display()
                raise
            except Exception as e:
                self.set_phase("error", f"Error: {str(e)}")
                self.refresh_display()
                raise
    
    def create_summary_table(self, result: CollaborationResult) -> Table:
        """Create a summary table for the collaboration result."""
        table = Table(title="üé≠ Collaboration Summary", box=box.ROUNDED)
        
        table.add_column("Metric", style="cyan", no_wrap=True)
        table.add_column("Value", style="green")
        
        table.add_row("Task", result.request.task_description[:50] + "..." if len(result.request.task_description) > 50 else result.request.task_description)
        table.add_row("Winner", result.chosen_agent or "None")
        table.add_row("Duration", f"{result.total_duration:.1f}s")
        table.add_row("Consensus", "‚úÖ Yes" if result.consensus_reached else "‚ùå No")
        table.add_row("Mode", result.request.mode.value.replace("_", " ").title())
        
        if result.voting_result:
            table.add_row("Total Votes", str(sum(len(score.votes) for score in result.voting_result.proposal_scores)))
            if result.voting_result.claude_used_veto:
                table.add_row("Claude Veto", "üö´ Used")
        
        return table
    
    async def demo_collaboration(self, task: str = "Create a Python function to calculate Fibonacci numbers") -> None:
        """Run a demo collaboration for testing the UI."""
        # Create a mock orchestrator for demo
        from src.orchestrator.collaboration import CollaborationOrchestrator
        
        orchestrator = CollaborationOrchestrator()
        request = CollaborationRequest(
            task_description=task,
            timeout_minutes=5
        )
        
        try:
            result = await self.run_collaboration_with_ui(orchestrator, request)
            
            # Show final summary
            self.console.print("\n")
            summary_table = self.create_summary_table(result)
            self.console.print(summary_table)
            
            if result.success and result.final_solution:
                self.console.print(f"\n[bold green]üìã Final Solution:[/bold green]")
                self.console.print(Panel(result.final_solution[:500] + "..." if len(result.final_solution) > 500 else result.final_solution))
        
        except KeyboardInterrupt:
            self.console.print("\n[yellow]‚ö†Ô∏è Demo cancelled by user[/yellow]")
        except Exception as e:
            self.console.print(f"\n[red]‚ùå Demo failed: {e}[/red]")


# Utility functions for quick UI operations
def quick_collaboration(task: str, timeout_minutes: int = 5) -> None:
    """Quick function to run a collaboration with UI."""
    async def _run():
        ui = CollaborationUI()
        await ui.demo_collaboration(task)
    
    try:
        asyncio.run(_run())
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    # Demo the UI
    quick_collaboration("Create a REST API for managing todo items")
</file>

<file path="src/ui/display.py">
"""
Display components for AngelaMCP Rich terminal UI.

This module provides specialized display components for visualizing different
aspects of multi-agent collaboration including debates, voting, and performance.
"""

import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass

from rich.console import Console, Group
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from rich.tree import Tree
from rich.columns import Columns
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn
from rich.syntax import Syntax
from rich.markdown import Markdown
from rich.align import Align

from src.orchestration.debate import DebateResult, DebateArgument, DebateRole, ArgumentType
from src.orchestration.voting import VoteResult, Vote, VoteType
from src.agents.base import BaseAgent, AgentResponse
from src.utils.logger import get_logger

logger = get_logger("ui.display")


@dataclass
class DisplayTheme:
    """Theme configuration for display components."""
    primary: str = "bright_blue"
    secondary: str = "bright_cyan"
    success: str = "bright_green"
    warning: str = "bright_yellow"
    error: str = "bright_red"
    info: str = "bright_white"
    muted: str = "dim white"
    
    # Agent-specific colors
    claude_color: str = "bright_magenta"
    openai_color: str = "bright_green"
    gemini_color: str = "bright_blue"


class DisplayManager:
    """
    Manager for creating Rich display components.
    
    I'm providing specialized display components for different aspects
    of the multi-agent system with consistent styling and formatting.
    """
    
    def __init__(self, console: Optional[Console] = None, theme: Optional[DisplayTheme] = None):
        self.console = console or Console()
        self.theme = theme or DisplayTheme()
        self.logger = get_logger("ui.display_manager")
    
    def get_agent_color(self, agent_type: str) -> str:
        """Get color for an agent type."""
        color_map = {
            "claude_code": self.theme.claude_color,
            "openai": self.theme.openai_color,
            "gemini": self.theme.gemini_color
        }
        return color_map.get(agent_type, self.theme.info)
    
    def format_timestamp(self, timestamp: float) -> str:
        """Format timestamp for display."""
        return datetime.fromtimestamp(timestamp).strftime("%H:%M:%S")
    
    def format_duration(self, duration_ms: float) -> str:
        """Format duration for display."""
        if duration_ms < 1000:
            return f"{duration_ms:.0f}ms"
        elif duration_ms < 60000:
            return f"{duration_ms/1000:.1f}s"
        else:
            return f"{duration_ms/60000:.1f}m"
    
    def create_agent_panel(self, agent: BaseAgent, detailed: bool = False) -> Panel:
        """Create a panel displaying agent information."""
        content = []
        
        # Basic info
        info_table = Table(show_header=False, box=None)
        info_table.add_column(style="bold")
        info_table.add_column()
        
        info_table.add_row("Name:", agent.name)
        info_table.add_row("Type:", agent.agent_type.value)
        
        # Performance metrics
        metrics = agent.performance_metrics
        info_table.add_row("Requests:", str(metrics["total_requests"]))
        info_table.add_row("Cost:", f"${metrics['total_cost_usd']:.4f}")
        info_table.add_row("Uptime:", f"{metrics['uptime_seconds']:.0f}s")
        
        content.append(info_table)
        
        # Capabilities if detailed
        if detailed and agent.capabilities:
            cap_tree = Tree("Capabilities", style=self.theme.secondary)
            for cap in agent.capabilities:
                cap_node = cap_tree.add(f"üìã {cap.name}")
                cap_node.add(f"Description: {cap.description}")
                if cap.supported_formats:
                    cap_node.add(f"Formats: {', '.join(cap.supported_formats)}")
                if cap.cost_per_request:
                    cap_node.add(f"Cost: ${cap.cost_per_request:.4f}")
            
            content.append(cap_tree)
        
        agent_color = self.get_agent_color(agent.agent_type.value)
        return Panel(
            Group(*content),
            title=f"Agent: {agent.name}",
            style=agent_color
        )
    
    def create_debate_panel(self, debate_result: DebateResult, detailed: bool = True) -> Panel:
        """Create a panel displaying debate results."""
        content = []
        
        # Debate summary
        summary_table = Table(title="Debate Summary", style=self.theme.primary)
        summary_table.add_column("Metric", style="bold")
        summary_table.add_column("Value", style=self.theme.success)
        
        summary_table.add_row("Debate ID", debate_result.debate_id[:8])
        summary_table.add_row("Success", "‚úÖ Yes" if debate_result.success else "‚ùå No")
        summary_table.add_row("Confidence", f"{debate_result.confidence_score:.2f}")
        summary_table.add_row("Rounds", str(len(debate_result.rounds)))
        summary_table.add_row("Duration", self.format_duration(debate_result.total_duration_ms))
        summary_table.add_row("Cost", f"${debate_result.total_cost_usd:.4f}")
        summary_table.add_row("Participants", ", ".join(debate_result.participating_agents))
        
        content.append(summary_table)
        
        if detailed:
            # Debate flow
            debate_tree = Tree("Debate Flow", style=self.theme.secondary)
            
            for round_data in debate_result.rounds:
                round_node = debate_tree.add(
                    f"Round {round_data.round_number} "
                    f"(consensus: {round_data.consensus_score:.2f})"
                )
                
                for arg in round_data.arguments:
                    agent_color = self.get_agent_color(arg.agent_type)
                    arg_text = f"{arg.agent_name} ({arg.role.value}): {arg.argument_type.value}"
                    arg_node = round_node.add(arg_text, style=agent_color)
                    
                    # Add truncated content
                    preview = arg.content[:100] + "..." if len(arg.content) > 100 else arg.content
                    arg_node.add(Text(preview, style="dim"))
                    
                    if arg.confidence_score:
                        arg_node.add(f"Confidence: {arg.confidence_score:.2f}")
            
            content.append(debate_tree)
            
            # Final consensus
            if debate_result.final_consensus:
                consensus_text = debate_result.final_consensus
                if len(consensus_text) > 300:
                    consensus_text = consensus_text[:300] + "..."
                
                content.append(
                    Panel(
                        Text(consensus_text, style=self.theme.info),
                        title="Final Consensus",
                        style=self.theme.success if debate_result.success else self.theme.warning
                    )
                )
        
        return Panel(
            Group(*content),
            title="Debate Results",
            style=self.theme.primary
        )
    
    def create_voting_panel(self, vote_result: VoteResult, detailed: bool = True) -> Panel:
        """Create a panel displaying voting results."""
        content = []
        
        # Voting summary
        summary_table = Table(title="Vote Summary", style=self.theme.primary)
        summary_table.add_column("Metric", style="bold")
        summary_table.add_column("Value", style=self.theme.success)
        
        summary_table.add_row("Vote ID", vote_result.vote_id[:8])
        summary_table.add_row("Decision", vote_result.final_decision.value.title())
        summary_table.add_row("Confidence", f"{vote_result.confidence_score:.2f}")
        summary_table.add_row("Method", vote_result.voting_method.value.replace("_", " ").title())
        summary_table.add_row("Duration", self.format_duration(vote_result.duration_ms))
        
        if vote_result.has_veto:
            summary_table.add_row("Veto", "‚ö†Ô∏è Yes", style=self.theme.warning)
        
        content.append(summary_table)
        
        # Vote breakdown
        breakdown_table = Table(title="Vote Breakdown", style=self.theme.secondary)
        breakdown_table.add_column("Vote Type", style="bold")
        breakdown_table.add_column("Weight", style=self.theme.success)
        breakdown_table.add_column("Percentage", style=self.theme.info)
        
        total_weight = vote_result.total_weight or 1.0
        
        breakdown_table.add_row(
            "Approve", 
            f"{vote_result.approve_weight:.2f}",
            f"{(vote_result.approve_weight/total_weight)*100:.1f}%"
        )
        breakdown_table.add_row(
            "Reject",
            f"{vote_result.reject_weight:.2f}", 
            f"{(vote_result.reject_weight/total_weight)*100:.1f}%"
        )
        breakdown_table.add_row(
            "Abstain",
            f"{vote_result.abstain_weight:.2f}",
            f"{(vote_result.abstain_weight/total_weight)*100:.1f}%"
        )
        
        content.append(breakdown_table)
        
        if detailed and vote_result.votes:
            # Individual votes
            votes_tree = Tree("Individual Votes", style=self.theme.secondary)
            
            for vote in vote_result.votes:
                agent_color = self.get_agent_color(vote.agent_type)
                vote_icon = {
                    VoteType.APPROVE: "‚úÖ",
                    VoteType.REJECT: "‚ùå", 
                    VoteType.ABSTAIN: "ü§∑",
                    VoteType.VETO: "‚õî"
                }.get(vote.vote_type, "‚ùì")
                
                vote_text = f"{vote_icon} {vote.agent_name} - {vote.vote_type.value.title()}"
                vote_node = votes_tree.add(vote_text, style=agent_color)
                
                vote_node.add(f"Weight: {vote.weight:.2f}")
                vote_node.add(f"Confidence: {vote.confidence:.2f}")
                
                if vote.reasoning:
                    reasoning_preview = vote.reasoning[:150] + "..." if len(vote.reasoning) > 150 else vote.reasoning
                    vote_node.add(Text(reasoning_preview, style="dim"))
            
            content.append(votes_tree)
            
            # Veto reasoning if present
            if vote_result.has_veto and vote_result.veto_reason:
                veto_text = vote_result.veto_reason
                if len(veto_text) > 200:
                    veto_text = veto_text[:200] + "..."
                
                content.append(
                    Panel(
                        Text(veto_text, style=self.theme.warning),
                        title="‚õî Veto Reasoning",
                        style=self.theme.error
                    )
                )
        
        return Panel(
            Group(*content),
            title="Voting Results", 
            style=self.theme.primary
        )
    
    def create_task_progress_panel(self, task_id: str, progress_info: Dict[str, Any]) -> Panel:
        """Create a panel showing task progress."""
        content = []
        
        # Task info
        info_table = Table(show_header=False, box=None)
        info_table.add_column(style="bold")
        info_table.add_column()
        
        info_table.add_row("Task ID:", task_id[:8])
        info_table.add_row("Type:", progress_info.get("type", "Unknown"))
        info_table.add_row("Strategy:", progress_info.get("strategy", "Unknown"))
        info_table.add_row("Status:", progress_info.get("status", "Unknown"))
        
        content.append(info_table)
        
        # Progress bar if available
        if "progress" in progress_info:
            progress = Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn()
            )
            
            task = progress.add_task(
                "Processing...", 
                total=100, 
                completed=progress_info["progress"]
            )
            
            content.append(progress)
        
        # Active agents
        if "agents" in progress_info:
            agents_text = Text("Active Agents: ", style="bold")
            for i, agent in enumerate(progress_info["agents"]):
                if i > 0:
                    agents_text.append(", ")
                agents_text.append(agent, style=self.get_agent_color(agent))
            
            content.append(agents_text)
        
        return Panel(
            Group(*content),
            title="Task Progress",
            style=self.theme.info
        )
    
    def create_performance_panel(self, metrics: Dict[str, Any]) -> Panel:
        """Create a panel displaying performance metrics."""
        content = []
        
        # Key metrics table
        metrics_table = Table(title="Performance Metrics", style=self.theme.primary)
        metrics_table.add_column("Metric", style="bold")
        metrics_table.add_column("Value", style=self.theme.success)
        metrics_table.add_column("Trend", style=self.theme.info)
        
        metrics_table.add_row(
            "Total Tasks",
            str(metrics.get("total_tasks", 0)),
            "üìà"  # Could show actual trend
        )
        metrics_table.add_row(
            "Success Rate", 
            f"{metrics.get('success_rate', 0):.1%}",
            "üìä"
        )
        metrics_table.add_row(
            "Avg Cost/Task",
            f"${metrics.get('average_cost_per_task', 0):.4f}",
            "üí∞"
        )
        metrics_table.add_row(
            "Tasks/Min",
            f"{metrics.get('tasks_per_minute', 0):.1f}",
            "‚ö°"
        )
        
        content.append(metrics_table)
        
        # Additional metrics if available
        if "agent_performance" in metrics:
            agent_perf = metrics["agent_performance"]
            agent_table = Table(title="Agent Performance", style=self.theme.secondary)
            agent_table.add_column("Agent", style="bold")
            agent_table.add_column("Requests", style=self.theme.info)
            agent_table.add_column("Success Rate", style=self.theme.success)
            agent_table.add_column("Avg Cost", style=self.theme.warning)
            
            for agent_name, perf in agent_perf.items():
                agent_table.add_row(
                    agent_name,
                    str(perf.get("requests", 0)),
                    f"{perf.get('success_rate', 0):.1%}",
                    f"${perf.get('avg_cost', 0):.4f}"
                )
            
            content.append(agent_table)
        
        return Panel(
            Group(*content),
            title="Performance Dashboard",
            style=self.theme.primary
        )
    
    def create_error_panel(self, error_message: str, details: Optional[Dict[str, Any]] = None) -> Panel:
        """Create a panel displaying error information."""
        content = [Text(error_message, style=self.theme.error)]
        
        if details:
            details_table = Table(show_header=False, box=None)
            details_table.add_column(style="bold")
            details_table.add_column(style=self.theme.muted)
            
            for key, value in details.items():
                details_table.add_row(f"{key}:", str(value))
            
            content.append(details_table)
        
        return Panel(
            Group(*content),
            title="‚ö†Ô∏è Error",
            style=self.theme.error
        )
</file>

<file path="src/ui/input_handler.py">
"""
Input handling for AngelaMCP terminal UI.

This module provides keyboard input handling, command processing, and
interactive features for the Rich-based terminal interface.
"""

import asyncio
import sys
import termios
import tty
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass
from enum import Enum

from rich.console import Console
from rich.prompt import Prompt, Confirm, IntPrompt, FloatPrompt
from rich.text import Text
from rich.panel import Panel

from src.orchestration.orchestrator import OrchestrationTask, OrchestrationStrategy, TaskType, TaskPriority
from src.agents.base import agent_registry
from src.utils.logger import get_logger

logger = get_logger("ui.input_handler")


class InputMode(str, Enum):
    """Input handling modes."""
    NORMAL = "normal"           # Normal navigation mode
    COMMAND = "command"         # Command entry mode
    TASK_ENTRY = "task_entry"   # Task creation mode
    SEARCH = "search"           # Search mode


@dataclass
class KeyBinding:
    """Represents a key binding."""
    key: str
    description: str
    action: Callable
    mode: InputMode = InputMode.NORMAL


class CommandProcessor:
    """
    Command processor for handling user commands.
    
    I'm implementing a command system that allows users to interact
    with the multi-agent system through text commands.
    """
    
    def __init__(self, orchestration_engine):
        self.engine = orchestration_engine
        self.logger = get_logger("ui.command_processor")
        
        # Command registry
        self.commands = {
            "help": self._cmd_help,
            "agents": self._cmd_agents,
            "tasks": self._cmd_tasks,
            "new": self._cmd_new_task,
            "debate": self._cmd_start_debate,
            "vote": self._cmd_start_vote,
            "status": self._cmd_status,
            "clear": self._cmd_clear,
            "exit": self._cmd_exit,
            "quit": self._cmd_exit,
        }
    
    async def process_command(self, command_line: str) -> Dict[str, Any]:
        """Process a command line and return result."""
        if not command_line.strip():
            return {"success": True, "message": ""}
        
        parts = command_line.strip().split()
        command = parts[0].lower()
        args = parts[1:] if len(parts) > 1 else []
        
        if command not in self.commands:
            return {
                "success": False,
                "message": f"Unknown command: {command}. Type 'help' for available commands."
            }
        
        try:
            result = await self.commands[command](args)
            return {"success": True, "result": result}
        except Exception as e:
            self.logger.error(f"Command '{command}' failed: {e}")
            return {"success": False, "message": f"Command failed: {e}"}
    
    async def _cmd_help(self, args: List[str]) -> str:
        """Show help information."""
        help_text = """
Available Commands:

ü§ñ Agent Commands:
  agents              - List all agents and their status
  status              - Show system status and metrics

üìã Task Commands:
  tasks               - List recent tasks
  new <description>   - Create a new task
  debate <topic>      - Start a debate on a topic
  vote <proposal>     - Start a voting session

üéõÔ∏è  System Commands:
  clear               - Clear the display
  help                - Show this help message
  exit/quit           - Exit the application

Examples:
  new "Write a Python function to sort a list"
  debate "Should we use microservices architecture?"
  vote "Implement the new feature using React"
"""
        return help_text
    
    async def _cmd_agents(self, args: List[str]) -> str:
        """List agents and their status."""
        agents = agent_registry.get_all_agents()
        
        if not agents:
            return "No agents are currently registered."
        
        result = "Registered Agents:\n\n"
        for agent in agents:
            metrics = agent.performance_metrics
            result += f"ü§ñ {agent.name} ({agent.agent_type.value})\n"
            result += f"   Requests: {metrics['total_requests']}\n"
            result += f"   Cost: ${metrics['total_cost_usd']:.4f}\n"
            result += f"   Capabilities: {len(agent.capabilities)}\n\n"
        
        return result
    
    async def _cmd_tasks(self, args: List[str]) -> str:
        """List recent tasks."""
        active_tasks = self.engine.orchestrator.get_active_tasks()
        metrics = self.engine.orchestrator.get_performance_metrics()
        
        result = f"Task Summary:\n\n"
        result += f"Total Tasks: {metrics['total_tasks']}\n"
        result += f"Success Rate: {metrics['success_rate']:.1%}\n"
        result += f"Active Tasks: {len(active_tasks)}\n\n"
        
        if active_tasks:
            result += "Active Tasks:\n"
            for task in active_tasks:
                result += f"  üìã {task.task_id[:8]} - {task.task_type.value} ({task.strategy.value})\n"
        else:
            result += "No active tasks.\n"
        
        return result
    
    async def _cmd_new_task(self, args: List[str]) -> str:
        """Create a new task."""
        if not args:
            return "Usage: new <task description>"
        
        description = " ".join(args)
        
        # Create task with intelligent routing
        result = await self.engine.analyze_and_route(description)
        
        if result.success:
            return f"‚úÖ Task completed successfully!\n\nResult:\n{result.content}"
        else:
            return f"‚ùå Task failed: {result.error_message}"
    
    async def _cmd_start_debate(self, args: List[str]) -> str:
        """Start a debate session."""
        if not args:
            return "Usage: debate <topic>"
        
        topic = " ".join(args)
        
        # Create debate task
        result = await self.engine.process_request(
            topic,
            task_type=TaskType.ANALYSIS,
            strategy=OrchestrationStrategy.DEBATE
        )
        
        if result.success:
            return f"üó£Ô∏è Debate completed!\n\nConsensus:\n{result.content}"
        else:
            return f"‚ùå Debate failed: {result.error_message}"
    
    async def _cmd_start_vote(self, args: List[str]) -> str:
        """Start a voting session."""
        if not args:
            return "Usage: vote <proposal>"
        
        proposal = " ".join(args)
        
        # This would need integration with the voting system
        return f"üó≥Ô∏è Voting on: {proposal}\n(Voting system integration needed)"
    
    async def _cmd_status(self, args: List[str]) -> str:
        """Show system status."""
        status = self.engine.get_status()
        
        result = "System Status:\n\n"
        result += f"Available Agents: {status['available_agents']}\n"
        
        metrics = status['orchestrator_metrics']
        result += f"Total Tasks: {metrics['total_tasks']}\n"
        result += f"Success Rate: {metrics['success_rate']:.1%}\n"
        result += f"Total Cost: ${metrics['total_cost_usd']:.4f}\n"
        result += f"Uptime: {metrics['uptime_seconds']:.0f}s\n"
        
        return result
    
    async def _cmd_clear(self, args: List[str]) -> str:
        """Clear the display."""
        return "CLEAR_DISPLAY"  # Special marker for UI to clear
    
    async def _cmd_exit(self, args: List[str]) -> str:
        """Exit the application."""
        return "EXIT_APPLICATION"  # Special marker for UI to exit


class InputHandler:
    """
    Advanced input handler for the terminal UI.
    
    I'm implementing a comprehensive input system that handles keyboard
    shortcuts, command processing, and interactive task creation.
    """
    
    def __init__(self, orchestration_engine, console: Optional[Console] = None):
        self.engine = orchestration_engine
        self.console = console or Console()
        self.command_processor = CommandProcessor(orchestration_engine)
        self.logger = get_logger("ui.input_handler")
        
        # Input state
        self.mode = InputMode.NORMAL
        self.command_buffer = ""
        self.history: List[str] = []
        self.history_index = -1
        
        # Key bindings
        self.key_bindings = self._setup_key_bindings()
        
        # Callbacks
        self.mode_change_callback: Optional[Callable] = None
        self.refresh_callback: Optional[Callable] = None
        self.exit_callback: Optional[Callable] = None
    
    def _setup_key_bindings(self) -> Dict[str, KeyBinding]:
        """Set up keyboard shortcuts."""
        bindings = {}
        
        # Navigation keys
        bindings['r'] = KeyBinding('r', "Refresh display", self._key_refresh)
        bindings['m'] = KeyBinding('m', "Switch mode", self._key_mode_switch)
        bindings['h'] = KeyBinding('h', "Show help", self._key_help)
        bindings['q'] = KeyBinding('q', "Quit application", self._key_quit)
        bindings['c'] = KeyBinding('c', "Clear display", self._key_clear)
        
        # Task management keys
        bindings['n'] = KeyBinding('n', "New task", self._key_new_task)
        bindings['d'] = KeyBinding('d', "Start debate", self._key_debate)
        bindings['v'] = KeyBinding('v', "Start vote", self._key_vote)
        bindings['s'] = KeyBinding('s', "Show status", self._key_status)
        
        # Command mode
        bindings[':'] = KeyBinding(':', "Enter command mode", self._key_command_mode)
        
        return bindings
    
    def set_callbacks(self, mode_change: Optional[Callable] = None,
                     refresh: Optional[Callable] = None,
                     exit_app: Optional[Callable] = None):
        """Set callback functions for UI events."""
        self.mode_change_callback = mode_change
        self.refresh_callback = refresh
        self.exit_callback = exit_app
    
    async def handle_key(self, key: str) -> bool:
        """Handle a single key press. Returns True if app should continue."""
        try:
            if self.mode == InputMode.COMMAND:
                return await self._handle_command_mode_key(key)
            else:
                return await self._handle_normal_mode_key(key)
        except Exception as e:
            self.logger.error(f"Error handling key '{key}': {e}")
            return True
    
    async def _handle_normal_mode_key(self, key: str) -> bool:
        """Handle key in normal mode."""
        if key in self.key_bindings:
            binding = self.key_bindings[key]
            return await binding.action()
        else:
            # Unknown key, ignore
            return True
    
    async def _handle_command_mode_key(self, key: str) -> bool:
        """Handle key in command mode."""
        if key == '\r' or key == '\n':  # Enter
            await self._execute_command()
            return True
        elif key == '\x1b':  # Escape
            self._exit_command_mode()
            return True
        elif key == '\x7f':  # Backspace
            if self.command_buffer:
                self.command_buffer = self.command_buffer[:-1]
            return True
        elif key == '\x03':  # Ctrl+C
            self._exit_command_mode()
            return True
        elif len(key) == 1 and ord(key) >= 32:  # Printable character
            self.command_buffer += key
            return True
        else:
            # Other special keys, ignore in command mode
            return True
    
    async def _execute_command(self):
        """Execute the current command buffer."""
        if self.command_buffer.strip():
            # Add to history
            self.history.append(self.command_buffer)
            
            # Process command
            result = await self.command_processor.process_command(self.command_buffer)
            
            # Handle special results
            if result.get("success") and "result" in result:
                command_result = result["result"]
                if command_result == "CLEAR_DISPLAY":
                    if self.refresh_callback:
                        self.refresh_callback()
                elif command_result == "EXIT_APPLICATION":
                    if self.exit_callback:
                        self.exit_callback()
                else:
                    # Display result
                    self.console.print(Panel(command_result, title="Command Result"))
            elif not result.get("success"):
                self.console.print(Panel(result.get("message", "Unknown error"), 
                                       title="Error", style="red"))
        
        self._exit_command_mode()
    
    def _exit_command_mode(self):
        """Exit command mode."""
        self.mode = InputMode.NORMAL
        self.command_buffer = ""
        if self.mode_change_callback:
            self.mode_change_callback(self.mode)
    
    # Key binding actions
    async def _key_refresh(self) -> bool:
        """Refresh display."""
        if self.refresh_callback:
            self.refresh_callback()
        return True
    
    async def _key_mode_switch(self) -> bool:
        """Switch UI mode."""
        # This would cycle through UI modes
        if self.mode_change_callback:
            self.mode_change_callback("next")
        return True
    
    async def _key_help(self) -> bool:
        """Show help."""
        help_result = await self.command_processor._cmd_help([])
        self.console.print(Panel(help_result, title="Help"))
        return True
    
    async def _key_quit(self) -> bool:
        """Quit application."""
        if Confirm.ask("Are you sure you want to quit?"):
            if self.exit_callback:
                self.exit_callback()
            return False
        return True
    
    async def _key_clear(self) -> bool:
        """Clear display."""
        if self.refresh_callback:
            self.refresh_callback()
        return True
    
    async def _key_new_task(self) -> bool:
        """Create new task interactively."""
        description = Prompt.ask("Enter task description")
        if description:
            result = await self.command_processor._cmd_new_task(description.split())
            self.console.print(Panel(result, title="Task Result"))
        return True
    
    async def _key_debate(self) -> bool:
        """Start debate interactively."""
        topic = Prompt.ask("Enter debate topic")
        if topic:
            result = await self.command_processor._cmd_start_debate(topic.split())
            self.console.print(Panel(result, title="Debate Result"))
        return True
    
    async def _key_vote(self) -> bool:
        """Start vote interactively."""
        proposal = Prompt.ask("Enter proposal to vote on")
        if proposal:
            result = await self.command_processor._cmd_start_vote(proposal.split())
            self.console.print(Panel(result, title="Vote Result"))
        return True
    
    async def _key_status(self) -> bool:
        """Show status."""
        result = await self.command_processor._cmd_status([])
        self.console.print(Panel(result, title="System Status"))
        return True
    
    async def _key_command_mode(self) -> bool:
        """Enter command mode."""
        self.mode = InputMode.COMMAND
        self.command_buffer = ""
        if self.mode_change_callback:
            self.mode_change_callback(self.mode)
        return True
    
    def get_command_prompt(self) -> str:
        """Get command prompt display."""
        if self.mode == InputMode.COMMAND:
            return f":{self.command_buffer}"
        return ""
    
    def get_status_line(self) -> str:
        """Get status line for display."""
        mode_indicator = {
            InputMode.NORMAL: "NORMAL",
            InputMode.COMMAND: "COMMAND",
            InputMode.TASK_ENTRY: "TASK",
            InputMode.SEARCH: "SEARCH"
        }.get(self.mode, "UNKNOWN")
        
        return f"Mode: {mode_indicator} | Press 'h' for help, 'q' to quit"


# Utility functions for cross-platform keyboard input
def get_key():
    """Get a single key press (cross-platform)."""
    try:
        if sys.platform == 'win32':
            import msvcrt
            return msvcrt.getch().decode('utf-8')
        else:
            fd = sys.stdin.fileno()
            old_settings = termios.tcgetattr(fd)
            try:
                tty.setraw(sys.stdin.fileno())
                key = sys.stdin.read(1)
                return key
            finally:
                termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
    except Exception:
        return None


async def async_input_loop(input_handler: InputHandler) -> None:
    """Async loop for handling keyboard input."""
    loop = asyncio.get_event_loop()
    
    def get_input():
        return get_key()
    
    while True:
        try:
            # Get key in a non-blocking way
            key = await loop.run_in_executor(None, get_input)
            if key:
                should_continue = await input_handler.handle_key(key)
                if not should_continue:
                    break
            await asyncio.sleep(0.01)  # Small delay to prevent busy waiting
        except KeyboardInterrupt:
            break
        except Exception as e:
            logger.error(f"Error in input loop: {e}")
            await asyncio.sleep(0.1)
</file>

<file path="src/ui/streaming.py">
"""
Real-time streaming UI components for AngelaMCP.

This module provides real-time streaming capabilities for displaying agent
output, task progress, and system events as they happen.
"""

import asyncio
import time
import queue
import threading
from collections import deque
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable, AsyncIterator
from dataclasses import dataclass
from enum import Enum

from rich.console import Console, Group
from rich.panel import Panel
from rich.text import Text
from rich.live import Live
from rich.status import Status
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.tree import Tree

from src.agents.base import AgentResponse, BaseAgent
from src.orchestration.orchestrator import TaskResult
from src.utils.logger import get_logger

logger = get_logger("ui.streaming")


class StreamEventType(str, Enum):
    """Types of streaming events."""
    TASK_STARTED = "task_started"
    TASK_COMPLETED = "task_completed"
    AGENT_RESPONSE = "agent_response"
    DEBATE_ROUND = "debate_round"
    VOTE_CAST = "vote_cast"
    ERROR = "error"
    LOG_MESSAGE = "log_message"
    PROGRESS_UPDATE = "progress_update"


@dataclass
class StreamEvent:
    """Event for the streaming system."""
    event_type: StreamEventType
    timestamp: float
    source: str
    data: Dict[str, Any]
    correlation_id: Optional[str] = None


class OutputBuffer:
    """
    Circular buffer for managing streaming output.
    
    I'm implementing a thread-safe buffer that can store streaming events
    and provide them for real-time display.
    """
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.buffer = deque(maxlen=max_size)
        self.lock = threading.Lock()
        self.event_queue = asyncio.Queue()
        
    def add_event(self, event: StreamEvent):
        """Add an event to the buffer."""
        with self.lock:
            self.buffer.append(event)
        
        # Add to async queue for real-time processing
        try:
            self.event_queue.put_nowait(event)
        except asyncio.QueueFull:
            # If queue is full, remove oldest item
            try:
                self.event_queue.get_nowait()
                self.event_queue.put_nowait(event)
            except asyncio.QueueEmpty:
                pass
    
    def get_recent_events(self, count: int = 50, event_type: Optional[StreamEventType] = None) -> List[StreamEvent]:
        """Get recent events from the buffer."""
        with self.lock:
            events = list(self.buffer)
        
        if event_type:
            events = [e for e in events if e.event_type == event_type]
        
        return events[-count:]
    
    def get_events_since(self, timestamp: float) -> List[StreamEvent]:
        """Get events since a specific timestamp."""
        with self.lock:
            return [e for e in self.buffer if e.timestamp >= timestamp]
    
    async def stream_events(self) -> AsyncIterator[StreamEvent]:
        """Async generator for streaming events."""
        while True:
            try:
                event = await asyncio.wait_for(self.event_queue.get(), timeout=1.0)
                yield event
            except asyncio.TimeoutError:
                continue
    
    def clear(self):
        """Clear the buffer."""
        with self.lock:
            self.buffer.clear()
        
        # Clear async queue
        while not self.event_queue.empty():
            try:
                self.event_queue.get_nowait()
            except asyncio.QueueEmpty:
                break


class RealTimeStreamer:
    """
    Real-time streaming display manager.
    
    I'm implementing a system that can display live updates from the
    multi-agent system including task progress, agent responses, and events.
    """
    
    def __init__(self, console: Console, buffer: OutputBuffer):
        self.console = console
        self.buffer = buffer
        self.logger = get_logger("ui.streaming_manager")
        
        # Display state
        self.active_tasks: Dict[str, Dict[str, Any]] = {}
        self.agent_statuses: Dict[str, str] = {}
        self.last_update = time.time()
        
        # Streaming components
        self.progress_displays: Dict[str, Progress] = {}
        self.status_displays: Dict[str, Status] = {}
        
    def create_live_panel(self) -> Panel:
        """Create the main live display panel."""
        content = []
        
        # Active tasks section
        if self.active_tasks:
            tasks_table = Table(title="üöÄ Active Tasks", style="bright_yellow")
            tasks_table.add_column("Task ID", style="dim")
            tasks_table.add_column("Type", style="bright_cyan")
            tasks_table.add_column("Progress", style="bright_green")
            tasks_table.add_column("Agent(s)", style="bright_magenta")
            tasks_table.add_column("Duration", style="bright_blue")
            
            for task_id, task_info in self.active_tasks.items():
                duration = time.time() - task_info.get("start_time", time.time())
                agents = ", ".join(task_info.get("agents", ["Unknown"]))
                progress = task_info.get("progress", 0)
                
                tasks_table.add_row(
                    task_id[:8],
                    task_info.get("type", "Unknown"),
                    f"{progress:.0f}%",
                    agents,
                    f"{duration:.1f}s"
                )
            
            content.append(tasks_table)
        
        # Agent status section
        if self.agent_statuses:
            agents_table = Table(title="ü§ñ Agent Status", style="bright_green")
            agents_table.add_column("Agent", style="bold")
            agents_table.add_column("Status", style="bright_cyan")
            agents_table.add_column("Last Activity", style="dim")
            
            for agent_name, status in self.agent_statuses.items():
                agents_table.add_row(
                    agent_name,
                    status,
                    "Just now"  # Would track actual last activity
                )
            
            content.append(agents_table)
        
        # Recent events
        recent_events = self.buffer.get_recent_events(10)
        if recent_events:
            events_tree = Tree("üìã Recent Events", style="bright_cyan")
            
            for event in recent_events[-5:]:  # Show last 5 events
                event_time = datetime.fromtimestamp(event.timestamp).strftime("%H:%M:%S")
                event_text = f"[{event_time}] {event.event_type.value}"
                
                event_node = events_tree.add(event_text)
                if event.source:
                    event_node.add(f"Source: {event.source}")
                
                # Add key data
                if "message" in event.data:
                    msg = event.data["message"]
                    if len(msg) > 100:
                        msg = msg[:100] + "..."
                    event_node.add(Text(msg, style="dim"))
            
            content.append(events_tree)
        
        # System status
        status_text = Text()
        status_text.append("üü¢ System Active ", style="bright_green")
        status_text.append(f"| Last Update: {datetime.fromtimestamp(self.last_update).strftime('%H:%M:%S')} ", style="dim")
        status_text.append(f"| Events: {len(self.buffer.buffer)}", style="bright_blue")
        
        content.append(status_text)
        
        if not content:
            content.append(Text("üåü Waiting for activity...", style="dim"))
        
        return Panel(
            Group(*content),
            title="üî¥ Live Feed",
            style="bright_red"
        )
    
    async def start_streaming(self, update_callback: Optional[Callable] = None):
        """Start the streaming display."""
        self.logger.info("Starting real-time streaming")
        
        async for event in self.buffer.stream_events():
            await self.process_event(event)
            
            if update_callback:
                update_callback()
    
    async def process_event(self, event: StreamEvent):
        """Process a streaming event and update displays."""
        self.last_update = time.time()
        
        try:
            if event.event_type == StreamEventType.TASK_STARTED:
                await self._handle_task_started(event)
            elif event.event_type == StreamEventType.TASK_COMPLETED:
                await self._handle_task_completed(event)
            elif event.event_type == StreamEventType.AGENT_RESPONSE:
                await self._handle_agent_response(event)
            elif event.event_type == StreamEventType.PROGRESS_UPDATE:
                await self._handle_progress_update(event)
            elif event.event_type == StreamEventType.ERROR:
                await self._handle_error(event)
            
        except Exception as e:
            self.logger.error(f"Error processing stream event: {e}")
    
    async def _handle_task_started(self, event: StreamEvent):
        """Handle task started event."""
        task_id = event.data.get("task_id")
        if task_id:
            self.active_tasks[task_id] = {
                "type": event.data.get("task_type", "Unknown"),
                "start_time": event.timestamp,
                "progress": 0,
                "agents": event.data.get("agents", []),
                "status": "Starting"
            }
            
            self.logger.debug(f"Task started: {task_id}")
    
    async def _handle_task_completed(self, event: StreamEvent):
        """Handle task completed event."""
        task_id = event.data.get("task_id")
        if task_id and task_id in self.active_tasks:
            # Remove from active tasks
            del self.active_tasks[task_id]
            self.logger.debug(f"Task completed: {task_id}")
    
    async def _handle_agent_response(self, event: StreamEvent):
        """Handle agent response event."""
        agent_name = event.data.get("agent_name")
        if agent_name:
            success = event.data.get("success", False)
            status = "‚úÖ Active" if success else "‚ö†Ô∏è Error"
            self.agent_statuses[agent_name] = status
    
    async def _handle_progress_update(self, event: StreamEvent):
        """Handle progress update event."""
        task_id = event.data.get("task_id")
        progress = event.data.get("progress", 0)
        
        if task_id and task_id in self.active_tasks:
            self.active_tasks[task_id]["progress"] = progress
            
            if "status" in event.data:
                self.active_tasks[task_id]["status"] = event.data["status"]
    
    async def _handle_error(self, event: StreamEvent):
        """Handle error event."""
        error_msg = event.data.get("error", "Unknown error")
        source = event.source or "System"
        
        self.logger.error(f"Stream error from {source}: {error_msg}")


class StreamingUI:
    """
    Main streaming UI coordinator.
    
    I'm providing a high-level interface for managing real-time streaming
    displays with automatic event capture and display updates.
    """
    
    def __init__(self, console: Optional[Console] = None):
        self.console = console or Console()
        self.buffer = OutputBuffer(max_size=2000)
        self.streamer = RealTimeStreamer(self.console, self.buffer)
        self.logger = get_logger("ui.streaming_ui")
        
        # Event subscribers
        self.subscribers: List[Callable[[StreamEvent], None]] = []
        
        # Auto-capture settings
        self.auto_capture = True
        self.capture_task = None
        
    def subscribe(self, callback: Callable[[StreamEvent], None]):
        """Subscribe to streaming events."""
        self.subscribers.append(callback)
    
    def emit_event(self, event_type: StreamEventType, source: str, data: Dict[str, Any], 
                   correlation_id: Optional[str] = None):
        """Emit a streaming event."""
        event = StreamEvent(
            event_type=event_type,
            timestamp=time.time(),
            source=source,
            data=data,
            correlation_id=correlation_id
        )
        
        self.buffer.add_event(event)
        
        # Notify subscribers
        for subscriber in self.subscribers:
            try:
                subscriber(event)
            except Exception as e:
                self.logger.error(f"Error in event subscriber: {e}")
    
    def emit_task_started(self, task_id: str, task_type: str, agents: List[str]):
        """Emit task started event."""
        self.emit_event(
            StreamEventType.TASK_STARTED,
            "orchestrator",
            {
                "task_id": task_id,
                "task_type": task_type,
                "agents": agents
            }
        )
    
    def emit_task_completed(self, task_id: str, success: bool, result: Optional[TaskResult] = None):
        """Emit task completed event."""
        data = {
            "task_id": task_id,
            "success": success
        }
        
        if result:
            data.update({
                "execution_time_ms": result.execution_time_ms,
                "cost_usd": result.total_cost_usd,
                "strategy": result.strategy_used.value if result.strategy_used else None
            })
        
        self.emit_event(StreamEventType.TASK_COMPLETED, "orchestrator", data)
    
    def emit_agent_response(self, agent_name: str, response: AgentResponse):
        """Emit agent response event."""
        self.emit_event(
            StreamEventType.AGENT_RESPONSE,
            agent_name,
            {
                "agent_name": agent_name,
                "success": response.success,
                "execution_time_ms": response.execution_time_ms,
                "cost_usd": response.cost_usd,
                "tokens_used": response.tokens_used,
                "error": response.error_message
            }
        )
    
    def emit_progress_update(self, task_id: str, progress: float, status: str):
        """Emit progress update event."""
        self.emit_event(
            StreamEventType.PROGRESS_UPDATE,
            "orchestrator",
            {
                "task_id": task_id,
                "progress": progress,
                "status": status
            }
        )
    
    def emit_error(self, source: str, error_message: str, details: Optional[Dict[str, Any]] = None):
        """Emit error event."""
        data = {"error": error_message}
        if details:
            data["details"] = details
        
        self.emit_event(StreamEventType.ERROR, source, data)
    
    def emit_log_message(self, source: str, level: str, message: str):
        """Emit log message event."""
        self.emit_event(
            StreamEventType.LOG_MESSAGE,
            source,
            {
                "level": level,
                "message": message
            }
        )
    
    def create_live_display(self) -> Panel:
        """Create the live display panel."""
        return self.streamer.create_live_panel()
    
    async def start_streaming(self, update_callback: Optional[Callable] = None):
        """Start streaming mode."""
        self.logger.info("Starting streaming UI")
        await self.streamer.start_streaming(update_callback)
    
    def get_recent_events(self, count: int = 50, event_type: Optional[StreamEventType] = None) -> List[StreamEvent]:
        """Get recent events."""
        return self.buffer.get_recent_events(count, event_type)
    
    def clear_events(self):
        """Clear all events."""
        self.buffer.clear()
        self.logger.info("Cleared streaming events")
</file>

<file path="src/ui/terminal.py">
"""
Rich-based terminal UI for AngelaMCP.

This module implements a sophisticated terminal interface using Rich that provides
real-time visualization of multi-agent collaboration, including agent status,
task progress, debate visualization, and performance metrics.
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum

from rich.console import Console, Group
from rich.layout import Layout
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.text import Text
from rich.live import Live
from rich.align import Align
from rich.columns import Columns
from rich.tree import Tree
from rich.status import Status
from rich.prompt import Prompt, Confirm
from rich.syntax import Syntax
from rich.markdown import Markdown

from src.agents.base import BaseAgent, AgentResponse, agent_registry
from src.orchestration.orchestrator import OrchestrationTask, TaskResult, OrchestrationStrategy
from src.orchestration.debate import DebateResult, DebateArgument
from src.orchestration.voting import VoteResult, Vote
from src.utils.logger import get_logger

logger = get_logger("ui.terminal")


class UIMode(str, Enum):
    """UI display modes."""
    OVERVIEW = "overview"
    AGENTS = "agents"
    TASKS = "tasks"
    DEBATE = "debate"
    VOTING = "voting"
    LOGS = "logs"
    PERFORMANCE = "performance"


@dataclass
class UIState:
    """Current state of the UI."""
    mode: UIMode = UIMode.OVERVIEW
    active_task_id: Optional[str] = None
    active_debate_id: Optional[str] = None
    active_vote_id: Optional[str] = None
    show_details: bool = False
    auto_refresh: bool = True
    refresh_rate: float = 1.0
    
    # Data caches
    agent_statuses: Dict[str, Dict[str, Any]] = None
    task_results: Dict[str, TaskResult] = None
    debate_results: Dict[str, DebateResult] = None
    vote_results: Dict[str, VoteResult] = None
    
    def __post_init__(self):
        if self.agent_statuses is None:
            self.agent_statuses = {}
        if self.task_results is None:
            self.task_results = {}
        if self.debate_results is None:
            self.debate_results = {}
        if self.vote_results is None:
            self.vote_results = {}


class TerminalUI:
    """
    Rich-based terminal user interface for AngelaMCP.
    
    I'm implementing a comprehensive TUI that provides real-time monitoring
    and interaction with the multi-agent collaboration system.
    """
    
    def __init__(self, orchestration_engine):
        self.engine = orchestration_engine
        self.console = Console(record=True)
        self.state = UIState()
        self.logger = get_logger("ui.terminal")
        
        # UI components
        self.layout = self._create_layout()
        self.live = None
        self._running = False
        self._update_task = None
        
        # Color scheme
        self.colors = {
            "primary": "bright_blue",
            "secondary": "bright_cyan", 
            "success": "bright_green",
            "warning": "bright_yellow",
            "error": "bright_red",
            "info": "bright_white",
            "muted": "dim white"
        }
        
        # Status tracking
        self._last_update = time.time()
        self._update_count = 0
        
        self.logger.info("Terminal UI initialized")
    
    def _create_layout(self) -> Layout:
        """Create the main UI layout."""
        layout = Layout()
        
        # Split into header, body, and footer
        layout.split_column(
            Layout(name="header", size=3),
            Layout(name="body"),
            Layout(name="footer", size=3)
        )
        
        # Split body into sidebar and main content
        layout["body"].split_row(
            Layout(name="sidebar", size=30),
            Layout(name="main")
        )
        
        # Split main content into primary and secondary panels
        layout["main"].split_column(
            Layout(name="primary"),
            Layout(name="secondary", size=15)
        )
        
        return layout
    
    def _create_header(self) -> Panel:
        """Create the header panel."""
        title = Text("AngelaMCP - Multi-Agent Collaboration Platform", style="bold bright_blue")
        subtitle = Text(f"Mode: {self.state.mode.value.title()}", style="dim")
        
        # Status indicators
        agent_count = len(agent_registry.get_all_agents())
        active_tasks = len(self.engine.orchestrator.get_active_tasks())
        
        status_text = Text()
        status_text.append(f"Agents: {agent_count} ", style="bright_green")
        status_text.append(f"Active Tasks: {active_tasks} ", style="bright_yellow")
        status_text.append(f"Updated: {datetime.now().strftime('%H:%M:%S')}", style="dim")
        
        header_content = Group(
            Align.center(title),
            Columns([subtitle, Align.right(status_text)], expand=True)
        )
        
        return Panel(header_content, style="bright_blue")
    
    def _create_sidebar(self) -> Panel:
        """Create the sidebar with navigation and agent status."""
        sidebar_content = []
        
        # Navigation menu
        nav_tree = Tree("üéØ Navigation", style="bold bright_cyan")
        modes = [
            ("üìä Overview", UIMode.OVERVIEW),
            ("ü§ñ Agents", UIMode.AGENTS),
            ("üìã Tasks", UIMode.TASKS),
            ("üí¨ Debate", UIMode.DEBATE),
            ("üó≥Ô∏è  Voting", UIMode.VOTING),
            ("üìà Performance", UIMode.PERFORMANCE),
            ("üìù Logs", UIMode.LOGS)
        ]
        
        for label, mode in modes:
            style = "bold bright_green" if mode == self.state.mode else "dim"
            nav_tree.add(label, style=style)
        
        sidebar_content.append(nav_tree)
        
        # Agent status summary
        agents_tree = Tree("ü§ñ Agent Status", style="bold bright_cyan")
        
        for agent in agent_registry.get_all_agents():
            metrics = agent.performance_metrics
            status_icon = "üü¢" if metrics["total_requests"] > 0 else "üü°"
            status_text = f"{status_icon} {agent.name}"
            
            agent_node = agents_tree.add(status_text)
            agent_node.add(f"Requests: {metrics['total_requests']}")
            agent_node.add(f"Cost: ${metrics['total_cost_usd']:.3f}")
            
        sidebar_content.append(agents_tree)
        
        # Quick actions
        actions_tree = Tree("‚ö° Quick Actions", style="bold bright_cyan")
        actions_tree.add("üîÑ Refresh (r)")
        actions_tree.add("üéØ Switch Mode (m)")
        actions_tree.add("üìã New Task (n)")
        actions_tree.add("üö™ Exit (q)")
        
        sidebar_content.append(actions_tree)
        
        return Panel(Group(*sidebar_content), title="Navigation", style="bright_cyan")
    
    def _create_overview_panel(self) -> Panel:
        """Create the main overview panel."""
        content = []
        
        # System overview table
        overview_table = Table(title="System Overview", style="bright_blue")
        overview_table.add_column("Metric", style="bold")
        overview_table.add_column("Value", style="bright_green")
        overview_table.add_column("Details", style="dim")
        
        # Get orchestrator metrics
        metrics = self.engine.orchestrator.get_performance_metrics()
        
        overview_table.add_row(
            "Total Tasks", 
            str(metrics["total_tasks"]),
            f"Success Rate: {metrics['success_rate']:.1%}"
        )
        overview_table.add_row(
            "Active Tasks",
            str(metrics["active_tasks_count"]),
            f"{metrics['tasks_per_minute']:.1f}/min"
        )
        overview_table.add_row(
            "Total Cost",
            f"${metrics['total_cost_usd']:.4f}",
            f"${metrics['average_cost_per_task']:.4f}/task"
        )
        overview_table.add_row(
            "Uptime",
            f"{metrics['uptime_seconds']:.0f}s",
            f"{metrics['uptime_seconds']/3600:.1f} hours"
        )
        
        content.append(overview_table)
        
        # Recent tasks
        if self.state.task_results:
            recent_table = Table(title="Recent Tasks", style="bright_yellow")
            recent_table.add_column("Task ID", style="dim")
            recent_table.add_column("Strategy", style="bright_cyan")
            recent_table.add_column("Status", style="bold")
            recent_table.add_column("Cost", style="bright_green")
            recent_table.add_column("Time", style="bright_magenta")
            
            # Show last 5 tasks
            recent_tasks = list(self.state.task_results.values())[-5:]
            for task in recent_tasks:
                status_icon = "‚úÖ" if task.success else "‚ùå"
                recent_table.add_row(
                    task.task_id[:8],
                    task.strategy_used.value if task.strategy_used else "unknown",
                    f"{status_icon} {'Success' if task.success else 'Failed'}",
                    f"${task.total_cost_usd:.4f}",
                    f"{task.execution_time_ms:.0f}ms"
                )
            
            content.append(recent_table)
        
        return Panel(Group(*content), title="Overview", style="bright_blue")
    
    def _create_agents_panel(self) -> Panel:
        """Create the agents detail panel."""
        agents_table = Table(title="Agent Details", style="bright_green")
        agents_table.add_column("Agent", style="bold")
        agents_table.add_column("Type", style="bright_cyan")
        agents_table.add_column("Status", style="bold")
        agents_table.add_column("Requests", style="bright_yellow")
        agents_table.add_column("Success Rate", style="bright_green")
        agents_table.add_column("Cost", style="bright_magenta")
        agents_table.add_column("Avg Time", style="bright_blue")
        
        for agent in agent_registry.get_all_agents():
            metrics = agent.performance_metrics
            
            # Calculate success rate
            success_rate = 1.0
            if metrics["total_requests"] > 0:
                failed_requests = metrics.get("failed_requests", 0)
                success_rate = (metrics["total_requests"] - failed_requests) / metrics["total_requests"]
            
            # Status based on recent activity
            if metrics["total_requests"] == 0:
                status = "üü° Idle"
            elif success_rate > 0.9:
                status = "üü¢ Excellent"
            elif success_rate > 0.7:
                status = "üü† Good"
            else:
                status = "üî¥ Issues"
            
            # Average response time
            avg_time = "N/A"
            if metrics["total_requests"] > 0:
                # This would need to be tracked in agent metrics
                avg_time = "~1.5s"  # Placeholder
            
            agents_table.add_row(
                agent.name,
                agent.agent_type.value,
                status,
                str(metrics["total_requests"]),
                f"{success_rate:.1%}",
                f"${metrics['total_cost_usd']:.4f}",
                avg_time
            )
        
        # Agent capabilities
        capabilities_content = []
        for agent in agent_registry.get_all_agents():
            cap_tree = Tree(f"{agent.name} Capabilities", style="bright_cyan")
            for capability in agent.capabilities:
                cap_node = cap_tree.add(f"üìã {capability.name}")
                cap_node.add(f"Description: {capability.description}")
                cap_node.add(f"Formats: {', '.join(capability.supported_formats)}")
                if capability.cost_per_request:
                    cap_node.add(f"Cost: ${capability.cost_per_request:.4f}")
            
            capabilities_content.append(cap_tree)
        
        content = [agents_table]
        if capabilities_content:
            content.extend(capabilities_content)
        
        return Panel(Group(*content), title="Agents", style="bright_green")
    
    def _create_tasks_panel(self) -> Panel:
        """Create the tasks detail panel."""
        content = []
        
        # Active tasks
        active_tasks = self.engine.orchestrator.get_active_tasks()
        if active_tasks:
            active_table = Table(title="Active Tasks", style="bright_yellow")
            active_table.add_column("Task ID", style="dim")
            active_table.add_column("Type", style="bright_cyan")
            active_table.add_column("Strategy", style="bright_magenta")
            active_table.add_column("Priority", style="bold")
            active_table.add_column("Started", style="bright_blue")
            
            for task in active_tasks:
                active_table.add_row(
                    task.task_id[:8],
                    task.task_type.value,
                    task.strategy.value,
                    task.priority.value,
                    "Just now"  # Would need start time tracking
                )
            
            content.append(active_table)
        else:
            content.append(Text("No active tasks", style="dim"))
        
        # Task history
        if self.state.task_results:
            history_table = Table(title="Task History", style="bright_blue")
            history_table.add_column("Task ID", style="dim")
            history_table.add_column("Strategy", style="bright_cyan")
            history_table.add_column("Status", style="bold")
            history_table.add_column("Execution Time", style="bright_magenta")
            history_table.add_column("Cost", style="bright_green")
            history_table.add_column("Agents Used", style="bright_yellow")
            
            for task_id, result in list(self.state.task_results.items())[-10:]:
                status_icon = "‚úÖ" if result.success else "‚ùå"
                agents_used = "N/A"
                if result.agent_responses:
                    agents_used = ", ".join(set(resp.agent_type for resp in result.agent_responses))
                
                history_table.add_row(
                    task_id[:8],
                    result.strategy_used.value if result.strategy_used else "unknown",
                    f"{status_icon} {'Success' if result.success else 'Failed'}",
                    f"{result.execution_time_ms:.0f}ms",
                    f"${result.total_cost_usd:.4f}",
                    agents_used
                )
            
            content.append(history_table)
        
        return Panel(Group(*content), title="Tasks", style="bright_yellow")
    
    def _create_footer(self) -> Panel:
        """Create the footer panel."""
        help_text = Text()
        help_text.append("Controls: ", style="bold")
        help_text.append("(r) Refresh ", style="bright_green")
        help_text.append("(m) Mode ", style="bright_cyan")
        help_text.append("(n) New Task ", style="bright_yellow")
        help_text.append("(q) Quit ", style="bright_red")
        help_text.append("(h) Help", style="bright_magenta")
        
        return Panel(Align.center(help_text), style="dim")
    
    async def _update_data(self):
        """Update UI data from the system."""
        try:
            # Update agent statuses
            for agent in agent_registry.get_all_agents():
                self.state.agent_statuses[agent.name] = {
                    "type": agent.agent_type.value,
                    "metrics": agent.performance_metrics,
                    "capabilities": len(agent.capabilities)
                }
            
            # Update orchestrator metrics
            self.state.orchestrator_metrics = self.engine.orchestrator.get_performance_metrics()
            
            self._last_update = time.time()
            self._update_count += 1
            
        except Exception as e:
            self.logger.error(f"Error updating UI data: {e}")
    
    def _render_layout(self):
        """Render the current layout based on UI state."""
        # Update header
        self.layout["header"].update(self._create_header())
        
        # Update sidebar
        self.layout["sidebar"].update(self._create_sidebar())
        
        # Update main content based on mode
        if self.state.mode == UIMode.OVERVIEW:
            self.layout["primary"].update(self._create_overview_panel())
        elif self.state.mode == UIMode.AGENTS:
            self.layout["primary"].update(self._create_agents_panel())
        elif self.state.mode == UIMode.TASKS:
            self.layout["primary"].update(self._create_tasks_panel())
        else:
            # Placeholder for other modes
            self.layout["primary"].update(
                Panel(f"Mode: {self.state.mode.value.title()}", title="Coming Soon")
            )
        
        # Update secondary panel (status/logs)
        status_content = []
        status_content.append(f"Updates: {self._update_count}")
        status_content.append(f"Last: {datetime.fromtimestamp(self._last_update).strftime('%H:%M:%S')}")
        status_content.append(f"Rate: {self.state.refresh_rate:.1f}s")
        
        self.layout["secondary"].update(
            Panel("\n".join(status_content), title="Status", style="dim")
        )
        
        # Update footer
        self.layout["footer"].update(self._create_footer())
    
    async def _auto_refresh_loop(self):
        """Auto-refresh loop for real-time updates."""
        while self._running and self.state.auto_refresh:
            try:
                await self._update_data()
                self._render_layout()
                await asyncio.sleep(self.state.refresh_rate)
            except Exception as e:
                self.logger.error(f"Error in auto-refresh loop: {e}")
                await asyncio.sleep(1.0)
    
    async def start(self):
        """Start the terminal UI."""
        self._running = True
        
        try:
            with Live(self.layout, console=self.console, refresh_per_second=2) as live:
                self.live = live
                
                # Initial data update and render
                await self._update_data()
                self._render_layout()
                
                # Start auto-refresh task
                if self.state.auto_refresh:
                    self._update_task = asyncio.create_task(self._auto_refresh_loop())
                
                # Keep running until stopped
                while self._running:
                    await asyncio.sleep(0.1)
                
        except KeyboardInterrupt:
            self.logger.info("UI interrupted by user")
        finally:
            await self.stop()
    
    async def stop(self):
        """Stop the terminal UI."""
        self._running = False
        
        if self._update_task:
            self._update_task.cancel()
            try:
                await self._update_task
            except asyncio.CancelledError:
                pass
        
        self.logger.info("Terminal UI stopped")
    
    def switch_mode(self, mode: UIMode):
        """Switch UI mode."""
        self.state.mode = mode
        self._render_layout()
        self.logger.info(f"Switched to {mode.value} mode")
    
    def toggle_auto_refresh(self):
        """Toggle auto-refresh."""
        self.state.auto_refresh = not self.state.auto_refresh
        self.logger.info(f"Auto-refresh {'enabled' if self.state.auto_refresh else 'disabled'}")
    
    def set_refresh_rate(self, rate: float):
        """Set refresh rate in seconds."""
        self.state.refresh_rate = max(0.1, rate)
        self.logger.info(f"Refresh rate set to {self.state.refresh_rate}s")


class UIManager:
    """
    High-level UI manager that coordinates terminal interface with user input.
    
    I'm providing a simplified interface for running the terminal UI with
    keyboard handling and command processing.
    """
    
    def __init__(self, orchestration_engine):
        self.engine = orchestration_engine
        self.ui = TerminalUI(orchestration_engine)
        self.logger = get_logger("ui.manager")
        self._input_task = None
    
    async def _handle_input(self):
        """Handle keyboard input in a separate task."""
        # This would need to be implemented with a proper async input handler
        # For now, we'll use a placeholder
        pass
    
    async def run(self):
        """Run the UI manager."""
        self.logger.info("Starting UI manager")
        
        try:
            # Start UI and input handling
            ui_task = asyncio.create_task(self.ui.start())
            
            await ui_task
            
        except Exception as e:
            self.logger.error(f"UI manager error: {e}")
            raise
        finally:
            await self.ui.stop()
    
    async def process_command(self, command: str) -> bool:
        """Process a user command. Returns True if should continue."""
        command = command.strip().lower()
        
        if command == 'q' or command == 'quit':
            return False
        elif command == 'r' or command == 'refresh':
            await self.ui._update_data()
            self.ui._render_layout()
        elif command == 'm' or command == 'mode':
            # Cycle through modes
            modes = list(UIMode)
            current_index = modes.index(self.ui.state.mode)
            next_index = (current_index + 1) % len(modes)
            self.ui.switch_mode(modes[next_index])
        elif command == 'a' or command == 'auto':
            self.ui.toggle_auto_refresh()
        elif command.startswith('rate '):
            try:
                rate = float(command.split()[1])
                self.ui.set_refresh_rate(rate)
            except (IndexError, ValueError):
                self.logger.warning("Invalid rate format. Use: rate <seconds>")
        else:
            self.logger.warning(f"Unknown command: {command}")
        
        return True
</file>

<file path="src/utils/__init__.py">
"""Utility functions and helpers."""

from .logger import setup_logging, get_logger
from .exceptions import AngelaMCPError, AgentError, OrchestratorError
from .helpers import parse_json_response, format_timestamp

__all__ = [
    "setup_logging", "get_logger",
    "AngelaMCPError", "AgentError", "OrchestratorError",
    "parse_json_response", "format_timestamp"
]
</file>

<file path="src/utils/exceptions.py">
"""
Exception classes for AngelaMCP.

Custom exceptions for the multi-agent collaboration platform.
"""


class AngelaMCPError(Exception):
    """Base exception for AngelaMCP platform."""
    pass


class AgentError(AngelaMCPError):
    """Exception for agent-related errors."""
    pass


class OrchestrationError(AngelaMCPError):
    """Exception for orchestration-related errors."""
    pass


class DatabaseError(AngelaMCPError):
    """Exception for database-related errors."""
    pass


class ConfigurationError(AngelaMCPError):
    """Exception for configuration-related errors."""
    pass


class ValidationError(AngelaMCPError):
    """Exception for validation-related errors."""
    pass


class OrchestratorError(AngelaMCPError):
    """Exception for orchestrator-related errors."""
    pass
</file>

<file path="src/utils/helpers.py">
"""
Helper utilities for AngelaMCP.

Common utility functions used across the platform.
"""

import json
import asyncio
from typing import Any, Dict, Optional


def parse_json_response(response: str) -> Optional[Dict[str, Any]]:
    """Parse a JSON response string safely."""
    try:
        return json.loads(response)
    except (json.JSONDecodeError, TypeError):
        return None


def format_cost(cost_usd: float) -> str:
    """Format cost in USD for display."""
    return f"${cost_usd:.4f}"


def format_tokens(tokens: int) -> str:
    """Format token count for display."""
    if tokens < 1000:
        return str(tokens)
    elif tokens < 1000000:
        return f"{tokens/1000:.1f}K"
    else:
        return f"{tokens/1000000:.1f}M"


async def safe_async_call(coro, default_value=None):
    """Safely call an async function with error handling."""
    try:
        return await coro
    except Exception:
        return default_value


def truncate_text(text: str, max_length: int = 100) -> str:
    """Truncate text to a maximum length."""
    if len(text) <= max_length:
        return text
    return text[:max_length-3] + "..."


def format_timestamp(timestamp) -> str:
    """Format timestamp for display."""
    if hasattr(timestamp, 'strftime'):
        return timestamp.strftime('%Y-%m-%d %H:%M:%S')
    return str(timestamp)
</file>

<file path="src/utils/logger.py">
"""
Logging configuration for AngelaMCP.

I'm setting up structured logging with proper formatting and file rotation.
"""

import logging
import logging.handlers
import sys
from pathlib import Path
from typing import Optional
import asyncio
from datetime import datetime

from config.settings import settings


def setup_logging() -> None:
    """Setup application-wide logging configuration."""
    
    # Create logs directory if it doesn't exist
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # Root logger configuration
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, settings.log_level.upper()))
    
    # Clear any existing handlers
    root_logger.handlers.clear()
    
    # Create formatters
    detailed_formatter = logging.Formatter(
        fmt='%(asctime)s | %(levelname)-8s | %(name)s:%(lineno)d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    simple_formatter = logging.Formatter(
        fmt='%(levelname)-8s | %(name)s | %(message)s'
    )
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(simple_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler with rotation
    file_handler = logging.handlers.RotatingFileHandler(
        filename=log_dir / "angelamcp.log",
        maxBytes=settings.log_max_size,
        backupCount=settings.log_backup_count,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(detailed_formatter)
    root_logger.addHandler(file_handler)
    
    # Error file handler
    error_handler = logging.handlers.RotatingFileHandler(
        filename=log_dir / "errors.log",
        maxBytes=settings.log_max_size,
        backupCount=3,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(detailed_formatter)
    root_logger.addHandler(error_handler)
    
    # Suppress noisy third-party loggers
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("asyncio").setLevel(logging.WARNING)
    
    # Log startup message
    logger = logging.getLogger("setup")
    logger.info(f"Logging initialized - Level: {settings.log_level}")


def get_logger(name: str) -> logging.Logger:
    """Get a logger with the specified name."""
    return logging.getLogger(name)


class AsyncPerformanceLogger:
    """Logger for tracking async operation performance."""
    
    def __init__(self, logger_name: str):
        self.logger = get_logger(logger_name)
        self.start_time: Optional[float] = None
    
    async def __aenter__(self):
        self.start_time = asyncio.get_event_loop().time()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.start_time:
            duration = asyncio.get_event_loop().time() - self.start_time
            self.logger.debug(f"Operation completed in {duration:.3f}s")


def log_context(operation: str):
    """Decorator to log function entry/exit with context."""
    def decorator(func):
        async def async_wrapper(*args, **kwargs):
            logger = get_logger(func.__module__)
            logger.debug(f"Starting {operation}")
            try:
                result = await func(*args, **kwargs)
                logger.debug(f"Completed {operation}")
                return result
            except Exception as e:
                logger.error(f"Failed {operation}: {e}")
                raise
        
        def sync_wrapper(*args, **kwargs):
            logger = get_logger(func.__module__)
            logger.debug(f"Starting {operation}")
            try:
                result = func(*args, **kwargs)
                logger.debug(f"Completed {operation}")
                return result
            except Exception as e:
                logger.error(f"Failed {operation}: {e}")
                raise
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    return decorator
</file>

<file path="src/utils/metrics.py">
"""
Comprehensive metrics and analytics system for AngelaMCP.
Tracks performance, costs, usage patterns, and system health.
"""

import asyncio
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
import threading
from contextlib import asynccontextmanager

import psutil
from config.settings import settings


@dataclass
class MetricPoint:
    """Single metric data point."""
    name: str
    value: Union[int, float]
    timestamp: datetime
    tags: Dict[str, str] = field(default_factory=dict)
    unit: str = ""


@dataclass
class AgentMetrics:
    """Metrics for an individual agent."""
    agent_name: str
    requests_total: int = 0
    requests_successful: int = 0
    requests_failed: int = 0
    total_tokens_input: int = 0
    total_tokens_output: int = 0
    total_cost: float = 0.0
    avg_response_time: float = 0.0
    last_request_time: Optional[datetime] = None
    
    @property
    def success_rate(self) -> float:
        """Calculate success rate percentage."""
        if self.requests_total == 0:
            return 0.0
        return (self.requests_successful / self.requests_total) * 100


@dataclass
class SystemMetrics:
    """System-wide performance metrics."""
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    disk_usage: float = 0.0
    active_connections: int = 0
    uptime_seconds: int = 0
    
    def update_system_stats(self):
        """Update system statistics."""
        self.cpu_usage = psutil.cpu_percent()
        self.memory_usage = psutil.virtual_memory().percent
        self.disk_usage = psutil.disk_usage('/').percent


class PerformanceTimer:
    """Context manager for timing operations."""
    
    def __init__(self, metrics_collector: 'MetricsCollector', operation_name: str, tags: Dict[str, str] = None):
        self.metrics = metrics_collector
        self.operation_name = operation_name
        self.tags = tags or {}
        self.start_time = None
        
    async def __aenter__(self):
        self.start_time = time.time()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time
        await self.metrics.record_timing(self.operation_name, duration, self.tags)
        
        if exc_type is not None:
            await self.metrics.record_error(self.operation_name, str(exc_val), self.tags)


class CostTracker:
    """Tracks API costs for different providers."""
    
    def __init__(self):
        self.daily_costs = defaultdict(float)
        self.monthly_costs = defaultdict(float)
        self.total_costs = defaultdict(float)
        self.last_reset_day = datetime.now().day
        self.last_reset_month = datetime.now().month
        
    def add_cost(self, provider: str, input_tokens: int, output_tokens: int) -> float:
        """Add cost for API usage."""
        cost = 0.0
        
        if provider == "openai":
            cost = (input_tokens * settings.openai_input_cost / 1000) + \
                   (output_tokens * settings.openai_output_cost / 1000)
        elif provider == "gemini":
            cost = (input_tokens * settings.gemini_input_cost / 1000) + \
                   (output_tokens * settings.gemini_output_cost / 1000)
                   
        # Reset daily/monthly counters if needed
        now = datetime.now()
        if now.day != self.last_reset_day:
            self.daily_costs.clear()
            self.last_reset_day = now.day
            
        if now.month != self.last_reset_month:
            self.monthly_costs.clear()
            self.last_reset_month = now.month
            
        self.daily_costs[provider] += cost
        self.monthly_costs[provider] += cost
        self.total_costs[provider] += cost
        
        return cost
    
    def get_daily_total(self) -> float:
        """Get total daily cost across all providers."""
        return sum(self.daily_costs.values())
    
    def get_monthly_total(self) -> float:
        """Get total monthly cost across all providers."""
        return sum(self.monthly_costs.values())
        
    def is_over_budget(self) -> Dict[str, bool]:
        """Check if over daily/monthly budget limits."""
        return {
            "daily": self.get_daily_total() > settings.daily_budget_limit,
            "monthly": self.get_monthly_total() > settings.monthly_budget_limit
        }
    
    def get_budget_status(self) -> Dict[str, Any]:
        """Get comprehensive budget status."""
        daily_total = self.get_daily_total()
        monthly_total = self.get_monthly_total()
        
        return {
            "daily": {
                "spent": daily_total,
                "limit": settings.daily_budget_limit,
                "remaining": max(0, settings.daily_budget_limit - daily_total),
                "percentage_used": (daily_total / settings.daily_budget_limit) * 100 if settings.daily_budget_limit > 0 else 0
            },
            "monthly": {
                "spent": monthly_total,
                "limit": settings.monthly_budget_limit,
                "remaining": max(0, settings.monthly_budget_limit - monthly_total),
                "percentage_used": (monthly_total / settings.monthly_budget_limit) * 100 if settings.monthly_budget_limit > 0 else 0
            }
        }


class MetricsCollector:
    """Main metrics collection and aggregation system."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.start_time = time.time()
        
        # Agent metrics
        self.agent_metrics: Dict[str, AgentMetrics] = {
            "claude_code": AgentMetrics("claude_code"),
            "openai": AgentMetrics("openai"),
            "gemini": AgentMetrics("gemini")
        }
        
        # System metrics
        self.system_metrics = SystemMetrics()
        
        # Cost tracking
        self.cost_tracker = CostTracker()
        
        # Performance metrics
        self.response_times = defaultdict(lambda: deque(maxlen=100))
        self.error_counts = defaultdict(int)
        self.request_counts = defaultdict(int)
        
        # Time series data (keep last 1000 points)
        self.time_series: deque = deque(maxlen=1000)
        
        # Background metrics collection
        self._metrics_task = None
        self._running = False
        
    async def start_collection(self):
        """Start background metrics collection."""
        if self._running:
            return
            
        self._running = True
        self._metrics_task = asyncio.create_task(self._collect_system_metrics())
        self.logger.info("Started metrics collection")
    
    async def stop_collection(self):
        """Stop background metrics collection."""
        self._running = False
        if self._metrics_task:
            self._metrics_task.cancel()
            try:
                await self._metrics_task
            except asyncio.CancelledError:
                pass
        self.logger.info("Stopped metrics collection")
    
    async def _collect_system_metrics(self):
        """Background task to collect system metrics."""
        while self._running:
            try:
                self.system_metrics.update_system_stats()
                
                # Record system metrics as time series
                now = datetime.now()
                self.time_series.append(MetricPoint(
                    "cpu_usage", self.system_metrics.cpu_usage, now, unit="%"
                ))
                self.time_series.append(MetricPoint(
                    "memory_usage", self.system_metrics.memory_usage, now, unit="%"
                ))
                
                await asyncio.sleep(30)  # Collect every 30 seconds
                
            except Exception as e:
                self.logger.error(f"Error collecting system metrics: {e}")
                await asyncio.sleep(60)  # Wait longer on error
    
    async def record_agent_request(
        self,
        agent_name: str,
        success: bool,
        input_tokens: int = 0,
        output_tokens: int = 0,
        response_time: float = 0.0
    ):
        """Record agent request metrics."""
        if agent_name not in self.agent_metrics:
            self.agent_metrics[agent_name] = AgentMetrics(agent_name)
            
        metrics = self.agent_metrics[agent_name]
        metrics.requests_total += 1
        metrics.last_request_time = datetime.now()
        
        if success:
            metrics.requests_successful += 1
        else:
            metrics.requests_failed += 1
            
        metrics.total_tokens_input += input_tokens
        metrics.total_tokens_output += output_tokens
        
        # Update average response time
        if response_time > 0:
            total_time = metrics.avg_response_time * (metrics.requests_total - 1) + response_time
            metrics.avg_response_time = total_time / metrics.requests_total
            
        # Track cost
        if agent_name in ["openai", "gemini"] and (input_tokens > 0 or output_tokens > 0):
            cost = self.cost_tracker.add_cost(agent_name, input_tokens, output_tokens)
            metrics.total_cost += cost
            
            # Check budget alerts
            budget_status = self.cost_tracker.is_over_budget()
            if budget_status["daily"] or budget_status["monthly"]:
                self.logger.warning(f"Budget limit exceeded: {budget_status}")
    
    async def record_timing(self, operation: str, duration: float, tags: Dict[str, str] = None):
        """Record operation timing."""
        self.response_times[operation].append(duration)
        
        # Record as time series
        self.time_series.append(MetricPoint(
            f"{operation}_duration",
            duration,
            datetime.now(),
            tags or {},
            "seconds"
        ))
    
    async def record_error(self, operation: str, error_message: str, tags: Dict[str, str] = None):
        """Record error occurrence."""
        self.error_counts[operation] += 1
        
        # Record as time series
        self.time_series.append(MetricPoint(
            f"{operation}_error",
            1,
            datetime.now(),
            {**(tags or {}), "error": error_message[:100]},
            "count"
        ))
    
    @asynccontextmanager
    async def time_operation(self, operation_name: str, tags: Dict[str, str] = None):
        """Context manager for timing operations."""
        timer = PerformanceTimer(self, operation_name, tags)
        async with timer:
            yield timer
    
    def get_agent_summary(self) -> Dict[str, Any]:
        """Get summary of all agent metrics."""
        summary = {}
        
        for agent_name, metrics in self.agent_metrics.items():
            summary[agent_name] = {
                "requests_total": metrics.requests_total,
                "success_rate": metrics.success_rate,
                "avg_response_time": metrics.avg_response_time,
                "total_tokens": metrics.total_tokens_input + metrics.total_tokens_output,
                "total_cost": metrics.total_cost,
                "last_request": metrics.last_request_time.isoformat() if metrics.last_request_time else None
            }
            
        return summary
    
    def get_system_summary(self) -> Dict[str, Any]:
        """Get system performance summary."""
        uptime = time.time() - self.start_time
        
        return {
            "uptime_seconds": int(uptime),
            "uptime_formatted": str(timedelta(seconds=int(uptime))),
            "cpu_usage": self.system_metrics.cpu_usage,
            "memory_usage": self.system_metrics.memory_usage,
            "disk_usage": self.system_metrics.disk_usage,
            "active_connections": self.system_metrics.active_connections
        }
    
    def get_cost_summary(self) -> Dict[str, Any]:
        """Get cost tracking summary."""
        return {
            "budget_status": self.cost_tracker.get_budget_status(),
            "daily_costs": dict(self.cost_tracker.daily_costs),
            "monthly_costs": dict(self.cost_tracker.monthly_costs),
            "total_costs": dict(self.cost_tracker.total_costs)
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance metrics summary."""
        summary = {}
        
        for operation, times in self.response_times.items():
            if times:
                summary[operation] = {
                    "avg_time": sum(times) / len(times),
                    "min_time": min(times),
                    "max_time": max(times),
                    "request_count": len(times)
                }
                
        return {
            "response_times": summary,
            "error_counts": dict(self.error_counts),
            "request_counts": dict(self.request_counts)
        }
    
    def get_comprehensive_metrics(self) -> Dict[str, Any]:
        """Get all metrics in a comprehensive report."""
        return {
            "timestamp": datetime.now().isoformat(),
            "agents": self.get_agent_summary(),
            "system": self.get_system_summary(),
            "costs": self.get_cost_summary(),
            "performance": self.get_performance_summary(),
            "alerts": self._get_alerts()
        }
    
    def _get_alerts(self) -> List[Dict[str, Any]]:
        """Generate system alerts based on metrics."""
        alerts = []
        
        # Budget alerts
        budget_status = self.cost_tracker.is_over_budget()
        if budget_status["daily"]:
            alerts.append({
                "type": "budget",
                "severity": "warning",
                "message": "Daily budget limit exceeded",
                "details": f"Spent: ${self.cost_tracker.get_daily_total():.2f}"
            })
            
        if budget_status["monthly"]:
            alerts.append({
                "type": "budget", 
                "severity": "critical",
                "message": "Monthly budget limit exceeded",
                "details": f"Spent: ${self.cost_tracker.get_monthly_total():.2f}"
            })
        
        # System performance alerts
        if self.system_metrics.cpu_usage > 90:
            alerts.append({
                "type": "performance",
                "severity": "warning", 
                "message": "High CPU usage detected",
                "details": f"CPU: {self.system_metrics.cpu_usage:.1f}%"
            })
            
        if self.system_metrics.memory_usage > 90:
            alerts.append({
                "type": "performance",
                "severity": "warning",
                "message": "High memory usage detected", 
                "details": f"Memory: {self.system_metrics.memory_usage:.1f}%"
            })
        
        # Agent failure rate alerts
        for agent_name, metrics in self.agent_metrics.items():
            if metrics.requests_total > 10 and metrics.success_rate < 80:
                alerts.append({
                    "type": "agent",
                    "severity": "warning",
                    "message": f"Low success rate for {agent_name}",
                    "details": f"Success rate: {metrics.success_rate:.1f}%"
                })
        
        return alerts
    
    async def flush(self):
        """Flush metrics to storage/logging."""
        try:
            metrics_summary = self.get_comprehensive_metrics()
            self.logger.info(f"Metrics summary: {metrics_summary}")
            
            # Here you could also send to external monitoring systems
            # like Prometheus, Grafana, DataDog, etc.
            
        except Exception as e:
            self.logger.error(f"Error flushing metrics: {e}")
    
    async def cleanup(self):
        """Cleanup metrics collector."""
        await self.stop_collection()
        await self.flush()


# Global metrics instance
_metrics_collector: Optional[MetricsCollector] = None


async def get_metrics_collector() -> MetricsCollector:
    """Get or create global metrics collector."""
    global _metrics_collector
    
    if _metrics_collector is None:
        _metrics_collector = MetricsCollector()
        await _metrics_collector.start_collection()
        
    return _metrics_collector


async def shutdown_metrics():
    """Shutdown global metrics collector."""
    global _metrics_collector
    
    if _metrics_collector:
        await _metrics_collector.cleanup()
        _metrics_collector = None
</file>

<file path="src/__init__.py">
"""AngelaMCP - Multi-AI Agent Collaboration Platform."""

__version__ = "1.0.0"
</file>

<file path="src/cli.py">
"""
CLI interface for AngelaMCP standalone mode.

This provides an interactive terminal interface when not running as MCP server.
I'm implementing a rich terminal experience with real-time collaboration display.
"""

import asyncio
import sys
from typing import Optional, Dict, Any
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.markdown import Markdown
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.prompt import Prompt
from rich.live import Live

from src.orchestrator.manager import TaskOrchestrator, CollaborationStrategy
from src.utils.logger import get_logger
from config.settings import settings


class CLI:
    """
    Command-line interface for AngelaMCP.
    
    Provides interactive terminal interface for multi-agent collaboration.
    """
    
    def __init__(self, orchestrator: TaskOrchestrator):
        self.orchestrator = orchestrator
        self.console = Console()
        self.logger = get_logger("cli")
        self.running = False
        
    async def run(self) -> None:
        """Run the CLI interface."""
        try:
            await self.show_welcome()
            self.running = True
            
            while self.running:
                try:
                    # Get user input
                    user_input = await self.get_user_input()
                    
                    if not user_input.strip():
                        continue
                    
                    # Handle commands
                    if user_input.startswith('/'):
                        await self.handle_command(user_input)
                    else:
                        # Regular collaboration request
                        await self.handle_collaboration_request(user_input)
                        
                except KeyboardInterrupt:
                    self.console.print("\n[yellow]Use /exit to quit properly[/yellow]")
                except EOFError:
                    break
                except Exception as e:
                    self.logger.error(f"CLI error: {e}", exc_info=True)
                    self.console.print(f"[red]Error: {e}[/red]")
            
        except Exception as e:
            self.logger.error(f"CLI run failed: {e}", exc_info=True)
            self.console.print(f"[red]CLI failed: {e}[/red]")
        finally:
            await self.cleanup()
    
    async def show_welcome(self) -> None:
        """Show welcome message and status."""
        welcome_panel = Panel.fit(
            f"""[bold cyan]AngelaMCP - Multi-AI Agent Collaboration Platform[/bold cyan]
[dim]Version {settings.app_version} | Environment: {settings.app_env}[/dim]

[green]‚úì[/green] Claude Code Agent ready
[green]‚úì[/green] OpenAI Agent ({settings.openai_model}) ready  
[green]‚úì[/green] Gemini Agent ({settings.gemini_model}) ready

Type your task to start collaboration, or use commands:
[cyan]/help[/cyan] - Show all commands
[cyan]/debate <topic>[/cyan] - Start structured debate
[cyan]/status[/cyan] - Show agent status
[cyan]/exit[/cyan] - Quit application""",
            title="Welcome",
            border_style="bright_blue"
        )
        
        self.console.print(welcome_panel)
        self.console.print()
    
    async def get_user_input(self) -> str:
        """Get user input with proper prompt."""
        return await asyncio.to_thread(
            Prompt.ask, 
            "[bold blue]MACP>[/bold blue]",
            console=self.console
        )
    
    async def handle_command(self, command: str) -> None:
        """Handle CLI commands."""
        cmd_parts = command.strip().split(' ', 1)
        cmd = cmd_parts[0].lower()
        args = cmd_parts[1] if len(cmd_parts) > 1 else ""
        
        if cmd == "/help":
            await self.show_help()
        elif cmd == "/exit" or cmd == "/quit":
            self.console.print("[yellow]Goodbye![/yellow]")
            self.running = False
        elif cmd == "/status":
            await self.show_status()
        elif cmd == "/debug":
            await self.show_debug_info()
        elif cmd == "/debate":
            if not args:
                self.console.print("[red]Usage: /debate <topic>[/red]")
                return
            await self.start_debate(args)
        elif cmd == "/analyze":
            if not args:
                self.console.print("[red]Usage: /analyze <task description>[/red]")
                return
            await self.analyze_task_complexity(args)
        elif cmd == "/config":
            await self.show_config()
        else:
            self.console.print(f"[red]Unknown command: {cmd}[/red]")
            self.console.print("Type [cyan]/help[/cyan] for available commands")
    
    async def handle_collaboration_request(self, task_description: str) -> None:
        """Handle a collaboration request."""
        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console,
                transient=True
            ) as progress:
                
                # Analyze task first
                progress.add_task("Analyzing task complexity...", total=None)
                analysis = await self.orchestrator.analyze_task_complexity(task_description)
                
                # Show analysis
                self.show_task_analysis(analysis)
                
                # Ask user for strategy preference
                strategy = self.get_strategy_preference(analysis["recommended_strategy"])
                
                # Start collaboration
                progress.add_task("Starting collaboration...", total=None)
                
                result = await self.orchestrator.collaborate_on_task(
                    task_description=task_description,
                    strategy=strategy,
                    max_rounds=3,
                    require_consensus=True
                )
                
                # Show results
                self.show_collaboration_result(result)
                
        except Exception as e:
            self.console.print(f"[red]Collaboration failed: {e}[/red]")
            self.logger.error(f"Collaboration error: {e}", exc_info=True)
    
    async def start_debate(self, topic: str) -> None:
        """Start a structured debate."""
        try:
            with self.console.status("[bold green]Starting debate...") as status:
                result = await self.orchestrator.start_debate(
                    topic=topic,
                    max_rounds=3,
                    timeout_seconds=300
                )
                
                self.show_debate_result(result)
                
        except Exception as e:
            self.console.print(f"[red]Debate failed: {e}[/red]")
    
    async def analyze_task_complexity(self, task_description: str) -> None:
        """Analyze and show task complexity."""
        try:
            analysis = await self.orchestrator.analyze_task_complexity(task_description)
            self.show_task_analysis(analysis)
        except Exception as e:
            self.console.print(f"[red]Analysis failed: {e}[/red]")
    
    def show_task_analysis(self, analysis: Dict[str, Any]) -> None:
        """Show task complexity analysis."""
        table = Table(title="Task Complexity Analysis")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")
        
        table.add_row("Complexity Score", f"{analysis.get('complexity_score', 0):.1f}/10")
        table.add_row("Complexity Level", analysis.get('complexity_level', 'Unknown'))
        table.add_row("Recommended Strategy", analysis.get('recommended_strategy', 'Unknown'))
        table.add_row("Estimated Time", analysis.get('estimated_time', 'Unknown'))
        table.add_row("Collaboration Benefit", analysis.get('collaboration_benefit', 'Unknown'))
        
        self.console.print(table)
        
        if analysis.get('reasoning'):
            reasoning_panel = Panel(
                analysis['reasoning'],
                title="Analysis Reasoning",
                border_style="dim"
            )
            self.console.print(reasoning_panel)
    
    def get_strategy_preference(self, recommended: str) -> str:
        """Get user's strategy preference."""
        strategies = {
            "1": "single_agent",
            "2": "parallel", 
            "3": "debate",
            "4": "consensus"
        }
        
        self.console.print(f"\n[bold]Collaboration Strategies:[/bold]")
        self.console.print(f"1. Single Agent (fast)")
        self.console.print(f"2. Parallel (balanced)")
        self.console.print(f"3. Debate (thorough)")
        self.console.print(f"4. Consensus (comprehensive)")
        self.console.print(f"\nRecommended: [green]{recommended}[/green]")
        
        choice = Prompt.ask(
            "Choose strategy (1-4) or press Enter for recommended",
            choices=["1", "2", "3", "4", ""],
            default="",
            console=self.console
        )
        
        return strategies.get(choice, recommended)
    
    def show_collaboration_result(self, result) -> None:
        """Show collaboration results."""
        # Status panel
        status_color = "green" if result.success else "red"
        status_text = "‚úÖ Success" if result.success else "‚ùå Failed"
        
        result_panel = Panel.fit(
            f"""[bold]{status_text}[/bold]
Strategy: {result.strategy_used.value if result.strategy_used else 'Unknown'}
Execution Time: {result.execution_time:.2f}s
Consensus Score: {result.consensus_score:.2f}
Agents: {len(result.agent_responses)}""",
            title="Collaboration Result",
            border_style=status_color
        )
        
        self.console.print(result_panel)
        
        # Final solution
        if result.final_solution:
            solution_panel = Panel(
                Markdown(result.final_solution),
                title="Final Solution",
                border_style="bright_blue"
            )
            self.console.print(solution_panel)
        
        # Agent responses
        if result.agent_responses:
            self.console.print("\n[bold]Agent Contributions:[/bold]")
            for i, response in enumerate(result.agent_responses, 1):
                agent_name = response.get('agent', 'Unknown')
                content = response.get('content', 'No response')[:200] + "..."
                confidence = response.get('confidence', 0)
                
                agent_panel = Panel(
                    f"[dim]{content}[/dim]\n\nConfidence: {confidence:.2f}",
                    title=f"{i}. {agent_name.title()} Agent",
                    border_style="dim"
                )
                self.console.print(agent_panel)
        
        # Debate summary
        if result.debate_summary:
            debate_panel = Panel(
                result.debate_summary,
                title="Debate Summary",
                border_style="yellow"
            )
            self.console.print(debate_panel)
    
    def show_debate_result(self, result: Dict[str, Any]) -> None:
        """Show debate results."""
        topic = result.get('topic', 'Unknown')
        rounds_completed = result.get('rounds_completed', 0)
        
        debate_panel = Panel.fit(
            f"""[bold]Debate Topic:[/bold] {topic}
[bold]Rounds Completed:[/bold] {rounds_completed}
[bold]Participants:[/bold] Claude, OpenAI, Gemini""",
            title="Debate Results",
            border_style="bright_magenta"
        )
        
        self.console.print(debate_panel)
        
        # Show rounds
        rounds = result.get('rounds', [])
        for round_data in rounds:
            round_num = round_data.get('round', 0)
            self.console.print(f"\n[bold]Round {round_num}:[/bold]")
            
            responses = round_data.get('responses', [])
            for response in responses:
                agent = response.get('agent', 'Unknown')
                content = response.get('content', 'No response')[:300] + "..."
                
                response_panel = Panel(
                    content,
                    title=f"{agent.title()} Position",
                    border_style="dim"
                )
                self.console.print(response_panel)
        
        # Final consensus
        consensus = result.get('consensus', {})
        if consensus.get('summary'):
            consensus_panel = Panel(
                f"{consensus['summary']}\n\nConsensus Score: {consensus.get('score', 0):.2f}",
                title="Final Consensus",
                border_style="green"
            )
            self.console.print(consensus_panel)
    
    async def show_status(self) -> None:
        """Show agent status."""
        try:
            # Get agent health
            claude_health = await self.orchestrator.claude_agent.health_check()
            openai_health = await self.orchestrator.openai_agent.health_check()
            gemini_health = await self.orchestrator.gemini_agent.health_check()
            
            # Create status table
            table = Table(title="Agent Status")
            table.add_column("Agent", style="cyan")
            table.add_column("Status", style="magenta")
            table.add_column("Response Time", style="green")
            table.add_column("Model", style="yellow")
            
            # Add rows
            claude_status = "‚úÖ Healthy" if claude_health.get("status") == "healthy" else "‚ùå Unhealthy"
            table.add_row(
                "Claude Code",
                claude_status,
                f"{claude_health.get('response_time', 0):.3f}s",
                claude_health.get('version', 'Unknown')
            )
            
            openai_status = "‚úÖ Healthy" if openai_health.get("status") == "healthy" else "‚ùå Unhealthy"
            table.add_row(
                "OpenAI",
                openai_status, 
                f"{openai_health.get('response_time', 0):.3f}s",
                openai_health.get('model', settings.openai_model)
            )
            
            gemini_status = "‚úÖ Healthy" if gemini_health.get("status") == "healthy" else "‚ùå Unhealthy"
            table.add_row(
                "Gemini",
                gemini_status,
                f"{gemini_health.get('response_time', 0):.3f}s",
                settings.gemini_model
            )
            
            self.console.print(table)
            
        except Exception as e:
            self.console.print(f"[red]Failed to get status: {e}[/red]")
    
    async def show_debug_info(self) -> None:
        """Show debug information."""
        try:
            # Get database health
            db_health = await self.orchestrator.db_manager.health_check()
            
            debug_table = Table(title="Debug Information")
            debug_table.add_column("Component", style="cyan")
            debug_table.add_column("Status", style="green")
            debug_table.add_column("Details", style="dim")
            
            # Database status
            pg_status = db_health.get('postgres', {}).get('status', 'unknown')
            redis_status = db_health.get('redis', {}).get('status', 'unknown')
            
            debug_table.add_row("PostgreSQL", pg_status, str(db_health.get('postgres', {})))
            debug_table.add_row("Redis", redis_status, str(db_health.get('redis', {})))
            debug_table.add_row("Environment", settings.app_env, f"Debug: {settings.debug}")
            
            self.console.print(debug_table)
            
        except Exception as e:
            self.console.print(f"[red]Debug info failed: {e}[/red]")
    
    async def show_config(self) -> None:
        """Show configuration information."""
        config_table = Table(title="Configuration")
        config_table.add_column("Setting", style="cyan")
        config_table.add_column("Value", style="green")
        
        config_table.add_row("Environment", settings.app_env)
        config_table.add_row("Debug Mode", str(settings.debug))
        config_table.add_row("Log Level", settings.log_level)
        config_table.add_row("OpenAI Model", settings.openai_model)
        config_table.add_row("Gemini Model", settings.gemini_model)
        config_table.add_row("Claude Path", str(settings.claude_code_path))
        
        self.console.print(config_table)
    
    async def show_help(self) -> None:
        """Show help information."""
        help_text = """[bold cyan]AngelaMCP Commands:[/bold cyan]

[bold]Basic Usage:[/bold]
‚Ä¢ Type any task description to start collaboration
‚Ä¢ Example: "Create a REST API with authentication"

[bold]Commands:[/bold]
‚Ä¢ [cyan]/help[/cyan] - Show this help message
‚Ä¢ [cyan]/status[/cyan] - Show agent status and health
‚Ä¢ [cyan]/debug[/cyan] - Show debug information
‚Ä¢ [cyan]/config[/cyan] - Show configuration
‚Ä¢ [cyan]/exit[/cyan] - Quit the application

[bold]Advanced Commands:[/bold]
‚Ä¢ [cyan]/debate <topic>[/cyan] - Start structured debate
‚Ä¢ [cyan]/analyze <task>[/cyan] - Analyze task complexity

[bold]Examples:[/bold]
‚Ä¢ [dim]create a calculator function[/dim]
‚Ä¢ [dim]/debate Should we use TypeScript or Python?[/dim]
‚Ä¢ [dim]/analyze building a microservices architecture[/dim]"""

        help_panel = Panel(
            help_text,
            title="Help",
            border_style="bright_blue"
        )
        
        self.console.print(help_panel)
    
    async def cleanup(self) -> None:
        """Cleanup CLI resources."""
        try:
            self.logger.info("CLI cleanup completed")
        except Exception as e:
            self.logger.error(f"CLI cleanup error: {e}")
    
    def __del__(self):
        """Destructor."""
        if hasattr(self, 'running') and self.running:
            asyncio.create_task(self.cleanup())
</file>

<file path="src/main.py">
#!/usr/bin/env python3
"""
Main application entry point for AngelaMCP.

This handles both standalone CLI mode and MCP server mode.
I'm implementing a production-ready system that can run either way.
"""

import asyncio
import logging
import signal
import sys
import os
from pathlib import Path
from typing import Optional

# Add project root to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import configuration first
from config.settings import settings
from src.utils.logger import setup_logging, get_logger

# Set up logging early
setup_logging()
logger = get_logger("main")


async def run_mcp_server():
    """Run as MCP server for Claude Code integration."""
    try:
        logger.info("Starting AngelaMCP as MCP Server...")
        from src.mcp_server import AngelaMCPServer
        
        server = AngelaMCPServer()
        await server.run()
        
    except Exception as e:
        logger.error(f"MCP server failed: {e}", exc_info=True)
        sys.exit(1)


async def run_standalone_cli():
    """Run as standalone CLI application."""
    try:
        logger.info("Starting AngelaMCP as standalone CLI...")
        from src.cli import CLI
        from src.orchestrator.manager import TaskOrchestrator
        from src.persistence.database import DatabaseManager
        from src.agents.claude_agent import ClaudeCodeAgent
        from src.agents.openai_agent import OpenAIAgent
        from src.agents.gemini_agent import GeminiAgent
        
        # Initialize database
        db_manager = DatabaseManager()
        await db_manager.initialize()
        
        # Initialize agents
        claude_agent = ClaudeCodeAgent()
        openai_agent = OpenAIAgent()
        gemini_agent = GeminiAgent()
        
        # Initialize orchestrator
        orchestrator = TaskOrchestrator(
            claude_agent=claude_agent,
            openai_agent=openai_agent,
            gemini_agent=gemini_agent,
            db_manager=db_manager
        )
        
        # Initialize CLI
        cli = CLI(orchestrator=orchestrator)
        
        # Run CLI
        await cli.run()
        
    except KeyboardInterrupt:
        logger.info("CLI shutdown requested")
    except Exception as e:
        logger.error(f"CLI failed: {e}", exc_info=True)
        sys.exit(1)


async def main():
    """Main application entry point."""
    try:
        # Check if running as MCP server
        if len(sys.argv) > 1 and sys.argv[1] == "mcp-server":
            await run_mcp_server()
        else:
            await run_standalone_cli()
            
    except Exception as e:
        logger.error(f"Application failed: {e}", exc_info=True)
        sys.exit(1)


def cli_main():
    """Entry point for CLI scripts (macp command)."""
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Application interrupted")
        sys.exit(0)
    except Exception as e:
        logger.error(f"CLI main failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    cli_main()
</file>

<file path="tests/integration/test_collaboration.py">
#!/usr/bin/env python3
"""
End-to-End Test for AngelaMCP Multi-Agent Collaboration.

This script demonstrates the full collaboration pipeline:
1. Three AI agents (Claude Code, OpenAI, Gemini) working together
2. Structured debate protocol
3. Weighted voting with Claude's veto power
4. Real-time Rich terminal UI

Run this to see AngelaMCP in action!
"""

import asyncio
import os
import sys
from pathlib import Path

# Add src to path so we can import our modules
sys.path.insert(0, str(Path(__file__).parent / "src"))

from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.text import Text

from src.orchestrator.collaboration import CollaborationOrchestrator, CollaborationRequest, CollaborationMode
from src.ui.collaboration_ui import CollaborationUI
from src.agents.claude_agent import ClaudeCodeAgent
from src.agents.openai_agent import OpenAIAgent  
from src.agents.gemini_agent import GeminiAgent
from config.settings import settings

console = Console()


def print_banner():
    """Print the AngelaMCP banner."""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                               ‚ïë
‚ïë     üé≠ AngelaMCP - Multi-Agent Collaboration Platform        ‚ïë
‚ïë                                                               ‚ïë
‚ïë  Claude Code (üîß) + OpenAI (üß†) + Gemini (‚ú®) = Better AI   ‚ïë
‚ïë                                                               ‚ïë
‚ïë  Watch AI agents debate, vote, and reach consensus!          ‚ïë
‚ïë                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
    console.print(banner, style="bold blue")


def check_environment():
    """Check if the environment is properly configured."""
    console.print("üîç Checking environment configuration...", style="yellow")
    
    issues = []
    
    # Check Claude Code path
    try:
        claude_path = Path(settings.claude_code_path)
        if not claude_path.exists():
            issues.append(f"‚ùå Claude Code not found at {claude_path}")
        else:
            console.print(f"‚úÖ Claude Code found at {claude_path}")
    except Exception as e:
        issues.append(f"‚ùå Claude Code path error: {e}")
    
    # Check API keys
    try:
        if not settings.openai_api_key.get_secret_value():
            issues.append("‚ùå OpenAI API key not configured")
        else:
            console.print("‚úÖ OpenAI API key configured")
    except Exception as e:
        issues.append(f"‚ùå OpenAI API key error: {e}")
    
    try:
        if not settings.google_api_key.get_secret_value():
            issues.append("‚ùå Google API key not configured")
        else:
            console.print("‚úÖ Google API key configured")
    except Exception as e:
        issues.append(f"‚ùå Google API key error: {e}")
    
    if issues:
        console.print("\n‚ö†Ô∏è Environment Issues Found:", style="bold red")
        for issue in issues:
            console.print(f"  {issue}")
        console.print(f"\nüí° Please check your .env file and ensure all API keys are set.")
        return False
    
    console.print("‚úÖ Environment looks good!\n", style="bold green")
    return True


async def test_individual_agents():
    """Test each agent individually."""
    console.print("üß™ Testing individual agents...", style="yellow")
    
    # Test Claude Code
    try:
        console.print("  Testing Claude Code agent...")
        claude = ClaudeCodeAgent()
        health = await claude.health_check()
        if health.get("status") == "healthy":
            console.print("  ‚úÖ Claude Code agent working")
        else:
            console.print(f"  ‚ùå Claude Code agent issue: {health.get('error', 'Unknown')}")
    except Exception as e:
        console.print(f"  ‚ùå Claude Code agent failed: {e}")
    
    # Test OpenAI
    try:
        console.print("  Testing OpenAI agent...")
        openai = OpenAIAgent()
        health = await openai.health_check()
        if health.get("status") == "healthy":
            console.print("  ‚úÖ OpenAI agent working")
        else:
            console.print(f"  ‚ùå OpenAI agent issue: {health.get('error', 'Unknown')}")
    except Exception as e:
        console.print(f"  ‚ùå OpenAI agent failed: {e}")
    
    # Test Gemini
    try:
        console.print("  Testing Gemini agent...")
        gemini = GeminiAgent()
        health = await gemini.health_check()
        if health.get("status") == "healthy":
            console.print("  ‚úÖ Gemini agent working")
        else:
            console.print(f"  ‚ùå Gemini agent issue: {health.get('error', 'Unknown')}")
    except Exception as e:
        console.print(f"  ‚ùå Gemini agent failed: {e}")
    
    console.print()


async def demo_simple_collaboration():
    """Run a simple collaboration demo without full UI."""
    console.print("ü§ù Running simple collaboration demo...", style="yellow")
    
    try:
        # Create orchestrator
        orchestrator = CollaborationOrchestrator()
        
        # Create a simple request
        request = CollaborationRequest(
            task_description="Create a Python function that calculates the factorial of a number",
            mode=CollaborationMode.CLAUDE_LEAD,  # Use Claude lead for faster demo
            timeout_minutes=3
        )
        
        console.print("üìã Task: Create a Python factorial function")
        console.print("üéØ Mode: Claude Lead (Claude implements, others review)")
        console.print("‚è±Ô∏è Timeout: 3 minutes\n")
        
        # Run collaboration
        result = await orchestrator.collaborate(request)
        
        # Display results
        if result.success:
            console.print("üéâ Collaboration successful!", style="bold green")
            console.print(f"Winner: {result.chosen_agent}")
            console.print(f"Duration: {result.total_duration:.1f}s")
            console.print(f"Consensus: {result.consensus_reached}")
            
            if result.final_solution:
                console.print("\nüìã Final Solution:")
                solution_preview = result.final_solution[:300] + "..." if len(result.final_solution) > 300 else result.final_solution
                console.print(Panel(solution_preview, title="Generated Code", border_style="green"))
        else:
            console.print("‚ùå Collaboration failed!", style="bold red")
            console.print(f"Error: {result.error_message}")
    
    except Exception as e:
        console.print(f"‚ùå Demo failed: {e}", style="bold red")
    
    console.print()


async def demo_full_ui_collaboration():
    """Run the full collaboration with Rich UI."""
    console.print("üé≠ Starting full UI collaboration demo...", style="bold yellow")
    console.print("This will show all agents working together in real-time!\n")
    
    # Get task from user or use default
    task_options = [
        "Create a Python function to calculate Fibonacci numbers",
        "Build a simple REST API for managing todo items",
        "Write a function to parse and validate email addresses",
        "Create a basic web scraper for extracting article titles",
        "Implement a binary search algorithm with proper error handling"
    ]
    
    console.print("üìù Choose a task for the agents to collaborate on:")
    for i, task in enumerate(task_options, 1):
        console.print(f"  {i}. {task}")
    
    try:
        choice = Prompt.ask("Enter choice (1-5) or press Enter for default", default="1")
        task_index = int(choice) - 1
        if 0 <= task_index < len(task_options):
            selected_task = task_options[task_index]
        else:
            selected_task = task_options[0]
    except (ValueError, KeyboardInterrupt):
        selected_task = task_options[0]
    
    console.print(f"\nüéØ Selected task: {selected_task}")
    console.print("Press Enter to continue...")
    input()
    
    try:
        # Create UI and orchestrator
        ui = CollaborationUI()
        orchestrator = CollaborationOrchestrator()
        
        # Create collaboration request
        request = CollaborationRequest(
            task_description=selected_task,
            mode=CollaborationMode.FULL_DEBATE,
            timeout_minutes=8
        )
        
        # Run with UI
        result = await ui.run_collaboration_with_ui(orchestrator, request)
        
        # Show final summary
        console.print("\n" + "="*60)
        console.print("üé≠ COLLABORATION COMPLETE", style="bold cyan", justify="center")
        console.print("="*60)
        
        summary_table = ui.create_summary_table(result)
        console.print(summary_table)
        
        if result.success and result.final_solution:
            console.print(f"\nüìã Final Solution Preview:")
            solution_preview = result.final_solution[:500] + "..." if len(result.final_solution) > 500 else result.final_solution
            console.print(Panel(solution_preview, title=f"Solution by {result.chosen_agent}", border_style="green"))
        
        # Show what makes this special
        console.print("\nüåü What Just Happened:")
        console.print("‚Ä¢ Three AI agents worked together on your task")
        console.print("‚Ä¢ Claude Code used its MCP tools for file system access")
        console.print("‚Ä¢ OpenAI provided technical review and critique")
        console.print("‚Ä¢ Gemini offered research and alternative approaches")
        console.print("‚Ä¢ They debated, voted, and reached consensus")
        console.print("‚Ä¢ You saw the whole process happen in real-time!")
        
    except KeyboardInterrupt:
        console.print("\n‚ö†Ô∏è Collaboration cancelled by user", style="yellow")
    except Exception as e:
        console.print(f"\n‚ùå Full demo failed: {e}", style="bold red")


async def main():
    """Main test runner."""
    print_banner()
    
    # Check environment
    if not check_environment():
        console.print("üõ†Ô∏è Please fix environment issues before running the demo.", style="bold red")
        return
    
    # Menu
    while True:
        console.print("üéÆ Choose an option:", style="bold cyan")
        console.print("1. Test individual agents")
        console.print("2. Simple collaboration demo (fast)")
        console.print("3. Full UI collaboration demo (recommended)")
        console.print("4. Exit")
        
        try:
            choice = Prompt.ask("Enter choice", choices=["1", "2", "3", "4"], default="3")
            
            if choice == "1":
                await test_individual_agents()
            elif choice == "2":
                await demo_simple_collaboration()
            elif choice == "3":
                await demo_full_ui_collaboration()
            elif choice == "4":
                console.print("üëã Thanks for trying AngelaMCP!", style="bold green")
                break
                
            if choice != "4":
                console.print("\nPress Enter to continue...")
                input()
                
        except KeyboardInterrupt:
            console.print("\nüëã Goodbye!", style="bold yellow")
            break
        except Exception as e:
            console.print(f"‚ùå Error: {e}", style="bold red")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\nüëã Goodbye!", style="bold yellow")
    except Exception as e:
        console.print(f"‚ùå Fatal error: {e}", style="bold red")
</file>

<file path="tests/unit/test_collaboration_orchestrator.py">
#!/usr/bin/env python3
"""
Unit tests for the CollaborationOrchestrator.

Tests the core collaboration orchestrator functionality without
requiring external API access.
"""

import pytest
import asyncio
from unittest.mock import AsyncMock, Mock, patch
from pathlib import Path
import sys

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from src.orchestrator.collaboration import (
    CollaborationOrchestrator, 
    CollaborationRequest, 
    CollaborationMode,
    CollaborationResult
)


class TestCollaborationOrchestrator:
    """Test the CollaborationOrchestrator class."""
    
    @pytest.fixture
    def orchestrator(self):
        """Create a collaboration orchestrator instance."""
        return CollaborationOrchestrator()
    
    @pytest.fixture
    def sample_request(self):
        """Create a sample collaboration request."""
        return CollaborationRequest(
            task_description="Create a Python function to calculate factorial",
            mode=CollaborationMode.FULL_DEBATE,
            timeout_minutes=5
        )
    
    def test_orchestrator_initialization(self, orchestrator):
        """Test orchestrator initialization."""
        assert orchestrator is not None
        assert hasattr(orchestrator, 'claude_agent')
        assert hasattr(orchestrator, 'openai_agent')
        assert hasattr(orchestrator, 'gemini_agent')
    
    def test_collaboration_request_creation(self, sample_request):
        """Test collaboration request creation."""
        assert sample_request.task_description == "Create a Python function to calculate factorial"
        assert sample_request.mode == CollaborationMode.FULL_DEBATE
        assert sample_request.timeout_minutes == 5
    
    @pytest.mark.asyncio
    async def test_get_agents_basic(self, orchestrator):
        """Test basic agent retrieval."""
        # This test just verifies the method exists and doesn't crash
        try:
            agents = await orchestrator._get_agents()
            # Should return a list, even if empty due to missing API keys
            assert isinstance(agents, list)
        except Exception:
            # If it fails due to missing keys, that's expected in test environment
            pass
    
    @pytest.mark.asyncio
    async def test_collaborate_mock(self, orchestrator, sample_request):
        """Test collaboration with mocked agents."""
        # Mock the internal agent methods
        with patch.object(orchestrator, '_get_agents') as mock_get_agents:
            mock_agent = Mock()
            mock_agent.name = "mock_claude"
            mock_agent.agent_type = "claude_code"
            mock_get_agents.return_value = [mock_agent]
            
            with patch.object(orchestrator, 'debate_protocol') as mock_debate:
                # Mock debate result
                mock_debate_result = Mock()
                mock_debate_result.success = True
                mock_debate_result.proposals = []
                mock_debate_result.consensus_reached = True
                mock_debate_result.final_proposal = None
                mock_debate_result.total_duration = 2.5
                
                mock_debate.conduct_debate = AsyncMock(return_value=mock_debate_result)
                
                with patch.object(orchestrator, 'voting_system') as mock_voting:
                    # Mock voting result
                    mock_voting_result = Mock()
                    mock_voting_result.success = True
                    mock_voting_result.winner = "mock_claude"
                    mock_voting_result.chosen_proposal = Mock()
                    mock_voting_result.chosen_proposal.solution = "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)"
                    
                    mock_voting.conduct_voting = AsyncMock(return_value=mock_voting_result)
                    
                    # Test collaboration
                    result = await orchestrator.collaborate(sample_request)
                    
                    # Verify result structure
                    assert isinstance(result, CollaborationResult)
                    assert result.success == True
                    assert result.chosen_agent == "mock_claude"
                    assert result.final_solution is not None
    
    def test_collaboration_modes(self):
        """Test different collaboration modes."""
        # Test FULL_DEBATE mode
        request1 = CollaborationRequest(
            task_description="Test task",
            mode=CollaborationMode.FULL_DEBATE
        )
        assert request1.mode == CollaborationMode.FULL_DEBATE
        
        # Test CLAUDE_LEAD mode
        request2 = CollaborationRequest(
            task_description="Test task",
            mode=CollaborationMode.CLAUDE_LEAD
        )
        assert request2.mode == CollaborationMode.CLAUDE_LEAD
        
        # Test QUICK_CONSENSUS mode
        request3 = CollaborationRequest(
            task_description="Test task",
            mode=CollaborationMode.QUICK_CONSENSUS
        )
        assert request3.mode == CollaborationMode.QUICK_CONSENSUS


class TestCollaborationRequest:
    """Test the CollaborationRequest data class."""
    
    def test_request_defaults(self):
        """Test default values for collaboration request."""
        request = CollaborationRequest(task_description="Test task")
        
        assert request.task_description == "Test task"
        assert request.mode == CollaborationMode.FULL_DEBATE
        assert request.timeout_minutes == 10
        assert request.context == {}
    
    def test_request_custom_values(self):
        """Test custom values for collaboration request."""
        context = {"key": "value"}
        request = CollaborationRequest(
            task_description="Custom task",
            mode=CollaborationMode.CLAUDE_LEAD,
            timeout_minutes=15,
            context=context
        )
        
        assert request.task_description == "Custom task"
        assert request.mode == CollaborationMode.CLAUDE_LEAD
        assert request.timeout_minutes == 15
        assert request.context == context


class TestCollaborationResult:
    """Test the CollaborationResult data class."""
    
    def test_result_success(self):
        """Test successful collaboration result."""
        result = CollaborationResult(
            success=True,
            chosen_agent="claude_code",
            final_solution="def test(): pass",
            total_duration=5.2,
            consensus_reached=True
        )
        
        assert result.success == True
        assert result.chosen_agent == "claude_code"
        assert result.final_solution == "def test(): pass"
        assert result.total_duration == 5.2
        assert result.consensus_reached == True
        assert result.error_message is None
    
    def test_result_failure(self):
        """Test failed collaboration result."""
        result = CollaborationResult(
            success=False,
            error_message="Collaboration timeout",
            total_duration=10.0
        )
        
        assert result.success == False
        assert result.error_message == "Collaboration timeout"
        assert result.chosen_agent is None
        assert result.final_solution is None
        assert result.consensus_reached == False
</file>

<file path="tests/unit/test_debate_system.py">
#!/usr/bin/env python3
"""
Unit tests for the Debate and Voting systems.

Tests the core debate protocol and voting system functionality.
"""

import pytest
from pathlib import Path
import sys

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from src.orchestrator.debate import DebateProtocol, AgentProposal, AgentCritique, DebateResult
from src.orchestrator.voting import VotingSystem, AgentVote, ProposalScore, VotingResult


class TestAgentProposal:
    """Test the AgentProposal data class."""
    
    def test_proposal_creation(self):
        """Test proposal creation."""
        proposal = AgentProposal(
            agent_name="claude_code",
            solution="def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
            reasoning="Recursive approach is clean and readable",
            confidence=0.9
        )
        
        assert proposal.agent_name == "claude_code"
        assert "factorial" in proposal.solution
        assert proposal.confidence == 0.9
        assert proposal.reasoning is not None
    
    def test_proposal_defaults(self):
        """Test proposal with default values."""
        proposal = AgentProposal(
            agent_name="test_agent",
            solution="test solution"
        )
        
        assert proposal.agent_name == "test_agent"
        assert proposal.solution == "test solution"
        assert proposal.reasoning == ""
        assert proposal.confidence == 0.5


class TestAgentCritique:
    """Test the AgentCritique data class."""
    
    def test_critique_creation(self):
        """Test critique creation."""
        critique = AgentCritique(
            agent_name="openai",
            target_agent="claude_code",
            points=["Missing error handling", "Could be optimized"],
            suggestions=["Add input validation", "Use iterative approach"],
            confidence=0.8
        )
        
        assert critique.agent_name == "openai"
        assert critique.target_agent == "claude_code"
        assert len(critique.points) == 2
        assert len(critique.suggestions) == 2
        assert critique.confidence == 0.8
    
    def test_critique_defaults(self):
        """Test critique with default values."""
        critique = AgentCritique(
            agent_name="test_agent",
            target_agent="target_agent"
        )
        
        assert critique.agent_name == "test_agent"
        assert critique.target_agent == "target_agent"
        assert critique.points == []
        assert critique.suggestions == []
        assert critique.confidence == 0.5


class TestAgentVote:
    """Test the AgentVote data class."""
    
    def test_vote_creation(self):
        """Test vote creation."""
        vote = AgentVote(
            agent_name="claude_code",
            choice="proposal_1",
            confidence=0.9,
            reasoning="Best implementation approach"
        )
        
        assert vote.agent_name == "claude_code"
        assert vote.choice == "proposal_1"
        assert vote.confidence == 0.9
        assert vote.reasoning == "Best implementation approach"
    
    def test_vote_defaults(self):
        """Test vote with default values."""
        vote = AgentVote(
            agent_name="test_agent",
            choice="test_choice"
        )
        
        assert vote.agent_name == "test_agent"
        assert vote.choice == "test_choice"
        assert vote.confidence == 0.5
        assert vote.reasoning == ""


class TestProposalScore:
    """Test the ProposalScore data class."""
    
    def test_score_creation(self):
        """Test proposal score creation."""
        score = ProposalScore(
            agent_name="claude_code",
            total_score=2.5,
            vote_count=3,
            avg_confidence=0.83
        )
        
        assert score.agent_name == "claude_code"
        assert score.total_score == 2.5
        assert score.vote_count == 3
        assert score.avg_confidence == 0.83
    
    def test_score_defaults(self):
        """Test proposal score with default values."""
        score = ProposalScore(agent_name="test_agent")
        
        assert score.agent_name == "test_agent"
        assert score.total_score == 0.0
        assert score.vote_count == 0
        assert score.avg_confidence == 0.0


class TestDebateResult:
    """Test the DebateResult data class."""
    
    def test_debate_result_success(self):
        """Test successful debate result."""
        proposals = [
            AgentProposal(agent_name="claude", solution="solution1"),
            AgentProposal(agent_name="openai", solution="solution2")
        ]
        
        result = DebateResult(
            success=True,
            proposals=proposals,
            consensus_reached=True,
            final_proposal=proposals[0],
            total_duration=5.2
        )
        
        assert result.success == True
        assert len(result.proposals) == 2
        assert result.consensus_reached == True
        assert result.final_proposal is not None
        assert result.total_duration == 5.2
        assert result.error_message is None
    
    def test_debate_result_failure(self):
        """Test failed debate result."""
        result = DebateResult(
            success=False,
            error_message="Timeout reached",
            total_duration=10.0
        )
        
        assert result.success == False
        assert result.error_message == "Timeout reached"
        assert result.proposals == []
        assert result.consensus_reached == False
        assert result.final_proposal is None


class TestVotingResult:
    """Test the VotingResult data class."""
    
    def test_voting_result_success(self):
        """Test successful voting result."""
        scores = [
            ProposalScore(agent_name="claude", total_score=3.0),
            ProposalScore(agent_name="openai", total_score=2.5)
        ]
        
        result = VotingResult(
            success=True,
            winner="claude",
            scores=scores,
            chosen_proposal=AgentProposal(agent_name="claude", solution="winning solution")
        )
        
        assert result.success == True
        assert result.winner == "claude"
        assert len(result.scores) == 2
        assert result.chosen_proposal is not None
        assert result.error_message is None
    
    def test_voting_result_failure(self):
        """Test failed voting result."""
        result = VotingResult(
            success=False,
            error_message="No valid votes received"
        )
        
        assert result.success == False
        assert result.error_message == "No valid votes received"
        assert result.winner is None
        assert result.scores == []
        assert result.chosen_proposal is None


class TestDebateProtocol:
    """Test the DebateProtocol class functionality."""
    
    @pytest.fixture
    def debate_protocol(self):
        """Create a debate protocol instance."""
        return DebateProtocol()
    
    def test_debate_protocol_initialization(self, debate_protocol):
        """Test debate protocol initialization."""
        assert debate_protocol is not None
        assert hasattr(debate_protocol, 'conduct_debate')
    
    def test_phase_transitions(self, debate_protocol):
        """Test debate phase management."""
        # This tests the basic structure, not async functionality
        assert hasattr(debate_protocol, 'conduct_debate')


class TestVotingSystem:
    """Test the VotingSystem class functionality."""
    
    @pytest.fixture
    def voting_system(self):
        """Create a voting system instance."""
        return VotingSystem()
    
    def test_voting_system_initialization(self, voting_system):
        """Test voting system initialization."""
        assert voting_system is not None
        assert hasattr(voting_system, 'conduct_voting')
    
    def test_claude_weights(self, voting_system):
        """Test Claude's special voting weights."""
        # Test that Claude has special treatment in voting
        # This is a structural test without async calls
        assert hasattr(voting_system, 'conduct_voting')
</file>

<file path="tests/__init__.py">
"""
AngelaMCP Test Suite

This package contains comprehensive tests for the AngelaMCP multi-agent
collaboration platform. The test suite is organized as follows:

Structure:
    unit/                   - Unit tests for individual components
        test_agents/        - Agent system tests
        test_orchestrator/  - Orchestration system tests  
        test_persistence/   - Database and persistence tests
        test_ui/           - User interface tests
    integration/           - Integration tests for full system
    fixtures/             - Shared test fixtures and data

Test Categories:
    - unit: Fast, isolated tests for individual components
    - integration: Tests that verify components work together
    - slow: Tests that take longer to run (marked separately)
    - ui: User interface specific tests
    - database: Tests requiring database setup

Running Tests:
    # All tests
    python scripts/run_tests.py --all
    
    # Unit tests only
    python scripts/run_tests.py --unit
    
    # Integration tests only  
    python scripts/run_tests.py --integration
    
    # Fast tests (excluding slow)
    python scripts/run_tests.py --fast
    
    # With coverage
    python scripts/run_tests.py --cov-html
    
    # Specific test
    python scripts/run_tests.py --test tests/unit/test_agents/test_base_agent.py

Coverage Target: >80%

The test suite uses pytest with async support and includes:
- Comprehensive fixtures for all components
- Mock agents and services for isolated testing
- Integration tests for end-to-end workflows
- Performance and stress tests
- UI component testing
- Database migration and persistence testing
"""

import pytest
import sys
from pathlib import Path

# Ensure src is in path for imports
src_path = Path(__file__).parent.parent / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

# Test configuration constants
TEST_TIMEOUT = 30  # Default test timeout in seconds
INTEGRATION_TIMEOUT = 60  # Integration test timeout
SLOW_TEST_TIMEOUT = 300  # Slow test timeout

# Export commonly used test utilities
from .conftest import *
</file>

<file path="tests/conftest.py">
"""Pytest configuration and fixtures."""

import pytest
import asyncio
import tempfile
import uuid
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
from typing import Dict, Any, Optional
import os
import sys

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Set test environment
os.environ["APP_ENV"] = "testing"
os.environ["DATABASE_URL"] = "sqlite:///test_angelamcp.db"
os.environ["LOG_LEVEL"] = "DEBUG"

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    yield loop
    loop.close()

@pytest.fixture
def temp_dir():
    """Create a temporary directory for tests."""
    with tempfile.TemporaryDirectory() as tmp_dir:
        yield Path(tmp_dir)

@pytest.fixture
def mock_settings(temp_dir):
    """Create mock settings for testing."""
    from config.settings import Settings
    from pydantic import SecretStr
    
    return Settings(
        # Core settings
        app_name="AngelaMCP-Test",
        app_version="1.0.0-test",
        debug=True,
        log_level="DEBUG",
        log_file=str(temp_dir / "test.log"),
        
        # Database
        database_url="sqlite:///test_angelamcp.db",
        database_pool_size=5,
        database_max_overflow=10,
        database_echo=False,
        
        # Agent paths and keys
        claude_code_path=str(temp_dir / "mock_claude"),
        openai_api_key=SecretStr("test-openai-key"),
        google_api_key=SecretStr("test-google-key"),
        
        # Claude Code settings
        claude_session_dir=str(temp_dir / "claude_sessions"),
        claude_session_persist=True,
        claude_code_max_turns=5,
        claude_code_output_format="json",
        
        # OpenAI settings
        openai_model="o3-mini",
        openai_max_tokens=4000,
        openai_temperature=0.7,
        
        # Gemini settings
        gemini_model="gemini-2.5-pro",
        gemini_max_output_tokens=4000,
        gemini_temperature=0.7,
        
        # Agent retry settings
        claude_code_max_retries=3,
        openai_max_retries=3,
        gemini_max_retries=3,
    )

@pytest.fixture
async def mock_database():
    """Create a mock database for testing."""
    from src.persistence.database import DatabaseManager
    
    # Use in-memory SQLite for tests
    db_manager = DatabaseManager("sqlite:///:memory:")
    await db_manager.initialize()
    
    yield db_manager
    
    await db_manager.close()

@pytest.fixture
def mock_console():
    """Create a mock Rich console for UI testing."""
    from rich.console import Console
    from io import StringIO
    
    string_io = StringIO()
    console = Console(file=string_io, force_terminal=True, width=120)
    return console

@pytest.fixture
def mock_agent_response():
    """Create a mock agent response."""
    from src.agents.base import AgentResponse
    
    return AgentResponse(
        success=True,
        content="Test response content",
        agent_type="test_agent",
        execution_time_ms=100.0,
        tokens_used=50,
        cost_usd=0.001,
        metadata={"test": "data"}
    )

@pytest.fixture
def mock_task_context():
    """Create a mock task context."""
    from src.agents.base import TaskContext, TaskType, AgentRole
    
    return TaskContext(
        task_id=str(uuid.uuid4()),
        task_type=TaskType.CUSTOM,
        agent_role=AgentRole.PRIMARY,
        max_tokens=1000,
        timeout_seconds=30
    )

@pytest.fixture
def mock_claude_agent(mock_settings):
    """Create a mock Claude Code agent."""
    from src.agents.claude_agent import ClaudeCodeAgent
    
    # Create mock claude executable
    mock_claude_path = Path(mock_settings.claude_code_path)
    mock_claude_path.parent.mkdir(parents=True, exist_ok=True)
    mock_claude_path.write_text("#!/bin/bash\necho 'Mock Claude response'")
    mock_claude_path.chmod(0o755)
    
    with patch('src.agents.claude_agent.ClaudeCodeAgent._verify_claude_installation'):
        agent = ClaudeCodeAgent("test_claude", str(mock_claude_path))
        agent._execute_command = AsyncMock(return_value={
            "type": "result",
            "subtype": "success", 
            "result": "Mock Claude response",
            "cost_usd": 0.001,
            "num_turns": 1
        })
        yield agent

@pytest.fixture
def mock_openai_agent(mock_settings):
    """Create a mock OpenAI agent."""
    from src.agents.openai_agent import OpenAIAgent
    from unittest.mock import MagicMock
    
    agent = OpenAIAgent("test_openai")
    
    # Mock the client
    mock_completion = MagicMock()
    mock_completion.choices = [MagicMock()]
    mock_completion.choices[0].message.content = "Mock OpenAI response"
    mock_completion.choices[0].finish_reason = "stop"
    mock_completion.usage.completion_tokens = 25
    mock_completion.model = "o3-mini"
    mock_completion.id = "test-id"
    mock_completion.system_fingerprint = "test-fp"
    
    agent.client.chat.completions.create = AsyncMock(return_value=mock_completion)
    
    yield agent

@pytest.fixture
def mock_gemini_agent(mock_settings):
    """Create a mock Gemini agent."""
    from src.agents.gemini_agent import GeminiAgent
    from unittest.mock import MagicMock
    
    agent = GeminiAgent("test_gemini")
    
    # Mock the client
    mock_response = MagicMock()
    mock_response.text = "Mock Gemini response"
    mock_response.candidates = [MagicMock()]
    mock_response.candidates[0].content.parts = [MagicMock()]
    mock_response.candidates[0].content.parts[0].text = "Mock Gemini response"
    mock_response.candidates[0].finish_reason = "STOP"
    mock_response.candidates[0].safety_ratings = []
    
    agent.client.aio.models.generate_content = AsyncMock(return_value=mock_response)
    
    yield agent

@pytest.fixture
async def mock_orchestrator(mock_database, mock_claude_agent, mock_openai_agent, mock_gemini_agent):
    """Create a mock orchestrator with agents."""
    from src.orchestration.orchestrator import TaskOrchestrator, OrchestrationEngine
    from src.agents.base import agent_registry
    
    # Clear and register mock agents
    agent_registry._agents.clear()
    for agent_list in agent_registry._agent_types.values():
        agent_list.clear()
    
    agent_registry.register(mock_claude_agent)
    agent_registry.register(mock_openai_agent) 
    agent_registry.register(mock_gemini_agent)
    
    orchestrator = TaskOrchestrator(mock_database)
    engine = OrchestrationEngine(mock_database)
    
    yield engine
    
    # Cleanup
    agent_registry._agents.clear()
    for agent_list in agent_registry._agent_types.values():
        agent_list.clear()

@pytest.fixture
def mock_debate_result():
    """Create a mock debate result."""
    from src.orchestration.debate import DebateResult, DebateRound, DebateArgument, DebateRole, ArgumentType
    
    arguments = [
        DebateArgument(
            agent_name="test_claude",
            agent_type="claude_code",
            role=DebateRole.PROPOSER,
            argument_type=ArgumentType.INITIAL_PROPOSAL,
            content="Initial proposal content",
            confidence_score=0.8
        ),
        DebateArgument(
            agent_name="test_openai",
            agent_type="openai",
            role=DebateRole.CHALLENGER,
            argument_type=ArgumentType.COUNTER_ARGUMENT,
            content="Counter argument content",
            confidence_score=0.7
        )
    ]
    
    rounds = [
        DebateRound(
            round_number=1,
            arguments=arguments,
            round_summary="Test round",
            consensus_score=0.75,
            round_duration_ms=1000.0
        )
    ]
    
    return DebateResult(
        debate_id=str(uuid.uuid4()),
        task_id=str(uuid.uuid4()),
        success=True,
        final_consensus="Test consensus",
        confidence_score=0.8,
        rounds=rounds,
        participating_agents=["test_claude", "test_openai"],
        total_duration_ms=1500.0,
        total_cost_usd=0.002,
        total_tokens=100
    )

@pytest.fixture
def mock_vote_result():
    """Create a mock vote result."""
    from src.orchestration.voting import VoteResult, Vote, VoteType, VotingMethod
    
    votes = [
        Vote(
            agent_name="test_claude",
            agent_type="claude_code",
            vote_type=VoteType.APPROVE,
            confidence=0.9,
            reasoning="Good proposal",
            weight=1.5
        ),
        Vote(
            agent_name="test_openai", 
            agent_type="openai",
            vote_type=VoteType.REJECT,
            confidence=0.7,
            reasoning="Has issues",
            weight=1.0
        )
    ]
    
    return VoteResult(
        vote_id=str(uuid.uuid4()),
        task_id=str(uuid.uuid4()),
        success=False,
        final_decision=VoteType.REJECT,
        confidence_score=0.7,
        votes=votes,
        total_weight=2.5,
        approve_weight=1.5,
        reject_weight=1.0,
        abstain_weight=0.0,
        has_veto=False,
        voting_method=VotingMethod.CLAUDE_VETO,
        duration_ms=500.0
    )

@pytest.fixture
def mock_stream_events():
    """Create mock stream events for UI testing."""
    from src.ui.streaming import StreamEvent, StreamEventType
    import time
    
    return [
        StreamEvent(
            event_type=StreamEventType.TASK_STARTED,
            timestamp=time.time(),
            source="orchestrator",
            data={
                "task_id": str(uuid.uuid4()),
                "task_type": "test",
                "agents": ["test_claude"]
            }
        ),
        StreamEvent(
            event_type=StreamEventType.AGENT_RESPONSE,
            timestamp=time.time(),
            source="test_claude",
            data={
                "agent_name": "test_claude",
                "success": True,
                "execution_time_ms": 100.0
            }
        )
    ]

# Test markers
pytest.mark.unit = pytest.mark.unit
pytest.mark.integration = pytest.mark.integration
pytest.mark.slow = pytest.mark.slow
</file>

<file path="tests/README.md">
# AngelaMCP Tests

This directory contains tests for the AngelaMCP multi-agent collaboration platform.

## Structure

```
tests/
‚îú‚îÄ‚îÄ unit/                              # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_collaboration_orchestrator.py  # Core orchestrator tests
‚îÇ   ‚îî‚îÄ‚îÄ test_debate_system.py              # Debate & voting system tests
‚îú‚îÄ‚îÄ integration/                        # Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ test_collaboration.py           # End-to-end collaboration test
‚îú‚îÄ‚îÄ conftest.py                        # Pytest configuration
‚îî‚îÄ‚îÄ README.md                          # This file
```

## Test Categories

### Unit Tests (`tests/unit/`)
- Test individual components in isolation
- Fast execution, no external dependencies
- Mock external services and APIs

### Integration Tests (`tests/integration/`)
- Test complete workflows end-to-end
- May require API keys for full functionality
- Test real agent collaboration scenarios

## Running Tests

### Prerequisites
1. Install test dependencies:
   ```bash
   pip install -r tests/requirements-test.txt
   ```

2. Set up environment (for integration tests):
   ```bash
   export OPENAI_API_KEY="your-key"
   export GOOGLE_API_KEY="your-key"
   ```

### Run All Tests
```bash
# From project root
pytest tests/

# With verbose output
pytest tests/ -v

# Run specific test categories
pytest tests/unit/          # Unit tests only
pytest tests/integration/   # Integration tests only
```

### Run Specific Tests
```bash
# Test specific file
pytest tests/unit/test_collaboration_orchestrator.py

# Test specific test
pytest tests/unit/test_collaboration_orchestrator.py::TestCollaborationOrchestrator::test_orchestrator_initialization
```

## Test Scope

This test suite focuses on the **core collaboration functionality**:
- Multi-agent orchestration
- Debate protocol
- Weighted voting system
- Real-time UI integration

### What's Tested
‚úÖ Collaboration orchestrator  
‚úÖ Debate and voting systems  
‚úÖ Data model structures  
‚úÖ End-to-end collaboration flows  

### What's Not Tested
‚ùå Database persistence (not used in core system)  
‚ùå Complex caching (not part of core collaboration)  
‚ùå Legacy orchestration components  

## Writing New Tests

### Unit Test Example
```python
import pytest
from src.orchestrator.collaboration import CollaborationOrchestrator

class TestMyComponent:
    @pytest.fixture
    def component(self):
        return CollaborationOrchestrator()
    
    def test_basic_functionality(self, component):
        assert component is not None
```

### Integration Test Example
```python
import pytest
from src.orchestrator.collaboration import CollaborationRequest

@pytest.mark.asyncio
async def test_full_workflow():
    request = CollaborationRequest(task_description="Test task")
    # Test end-to-end flow
```

## Contributing

When adding new features:
1. Add unit tests for individual components
2. Add integration tests for end-to-end workflows
3. Keep tests focused on current architecture
4. Remove tests for deprecated components
</file>

<file path="tests/requirements-test.txt">
# Testing dependencies for AngelaMCP

# Core testing framework
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
pytest-mock>=3.11.0
pytest-timeout>=2.1.0

# Test utilities
pytest-xdist>=3.3.0  # Parallel test execution
pytest-html>=3.2.0   # HTML test reports
pytest-benchmark>=4.0.0  # Performance benchmarking

# Coverage and quality
coverage>=7.2.0
pytest-cov>=4.1.0

# Mocking and fixtures
responses>=0.23.0
factory-boy>=3.3.0
faker>=19.0.0

# Database testing
pytest-postgresql>=5.0.0  # For PostgreSQL testing
aiosqlite>=0.19.0  # For async SQLite testing

# UI testing
pytest-rich>=0.1.0

# Performance and memory testing
memory-profiler>=0.61.0
psutil>=5.9.0

# Test data and fixtures
freezegun>=1.2.0  # Time mocking
testfixtures>=7.1.0

# Development and debugging
pytest-sugar>=0.9.7  # Better test output
pytest-clarity>=1.0.1  # Better assertion output
pytest-picked>=0.4.6  # Run tests based on git changes
</file>

<file path=".env.example">
# MACP Environment Configuration
# Copy this file to .env and fill in your values

# ============================================
# Application Settings
# ============================================
APP_NAME=AngelaMCP
APP_ENV=development  # development, staging, production
APP_VERSION=1.0.0
DEBUG=false
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=logs/macp.log
LOG_MAX_SIZE=10485760  # 10MB
LOG_BACKUP_COUNT=5

# ============================================
# Claude Code Configuration
# ============================================
# Path to Claude Code executable
CLAUDE_CODE_PATH=/usr/local/bin/claude

# Claude Code execution settings
CLAUDE_CODE_TIMEOUT=300  # 5 minutes
CLAUDE_CODE_MAX_TURNS=10
CLAUDE_CODE_OUTPUT_FORMAT=json  # text, json, stream-json

# Claude Code session management
CLAUDE_SESSION_PERSIST=true
CLAUDE_SESSION_DIR=~/.macp/claude_sessions

# ============================================
# OpenAI Configuration
# ============================================
# Your OpenAI API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenAI model settings
OPENAI_MODEL=o3-mini
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7
OPENAI_TOP_P=0.9
OPENAI_FREQUENCY_PENALTY=0.0
OPENAI_PRESENCE_PENALTY=0.0
OPENAI_TIMEOUT=120  # 2 minutes

# OpenAI rate limiting
OPENAI_RATE_LIMIT=60  # requests per minute
OPENAI_MAX_RETRIES=3
OPENAI_RETRY_DELAY=1  # seconds

# ============================================
# Google Gemini Configuration
# ============================================
# Your Google API key from https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your-google-api-key-here

# Gemini model settings
GEMINI_MODEL=gemini-2.5-pro-preview-06-05
GEMINI_MAX_OUTPUT_TOKENS=4096
GEMINI_TEMPERATURE=0.7
GEMINI_TOP_P=0.9
GEMINI_TOP_K=40
GEMINI_TIMEOUT=120  # 2 minutes

# Gemini safety settings
GEMINI_HARM_BLOCK_THRESHOLD=BLOCK_MEDIUM_AND_ABOVE
GEMINI_SAFETY_CATEGORIES=HARM_CATEGORY_HATE_SPEECH,HARM_CATEGORY_DANGEROUS_CONTENT

# Gemini rate limiting
GEMINI_RATE_LIMIT=60  # requests per minute
GEMINI_MAX_RETRIES=3
GEMINI_RETRY_DELAY=1  # seconds

# ============================================
# Database Configuration
# ============================================
# PostgreSQL connection
DATABASE_URL=postgresql://yoshi:yoshi@localhost:5432/angeladb
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40
DATABASE_POOL_TIMEOUT=30
DATABASE_ECHO=false  # Set to true for SQL logging

# Redis connection
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=50
REDIS_DECODE_RESPONSES=true
REDIS_SOCKET_TIMEOUT=5
REDIS_CONNECTION_TIMEOUT=5

# ============================================
# Session Configuration
# ============================================
SESSION_TIMEOUT=3600  # 1 hour in seconds
SESSION_CLEANUP_INTERVAL=300  # 5 minutes
MAX_CONVERSATION_LENGTH=100  # Maximum messages per conversation
MAX_CONCURRENT_SESSIONS=10

# ============================================
# Task Execution Configuration
# ============================================
# Task queue settings
TASK_QUEUE_MAX_SIZE=100
TASK_EXECUTION_TIMEOUT=1800  # 30 minutes
PARALLEL_TASK_LIMIT=5

# Debate protocol settings
DEBATE_TIMEOUT=300  # 5 minutes per debate round
DEBATE_MAX_ROUNDS=3
DEBATE_MIN_PARTICIPANTS=2

# Voting settings
VOTING_TIMEOUT=60  # 1 minute
CLAUDE_VOTE_WEIGHT=2.0
OPENAI_VOTE_WEIGHT=1.0
GEMINI_VOTE_WEIGHT=1.0
CLAUDE_VETO_ENABLED=true

# ============================================
# Feature Flags
# ============================================
ENABLE_COST_TRACKING=true
ENABLE_PARALLEL_EXECUTION=true
ENABLE_DEBATE_MODE=true
ENABLE_AUTO_SAVE=true
ENABLE_MEMORY_PERSISTENCE=true
ENABLE_ANALYTICS=false
ENABLE_TELEMETRY=false

# ============================================
# Cost Tracking Configuration
# ============================================
# Cost per 1K tokens (in USD)
OPENAI_INPUT_COST=0.003
OPENAI_OUTPUT_COST=0.006
GEMINI_INPUT_COST=0.00025
GEMINI_OUTPUT_COST=0.0005

# Budget limits (in USD)
DAILY_BUDGET_LIMIT=10.00
MONTHLY_BUDGET_LIMIT=250.00
BUDGET_WARNING_THRESHOLD=0.8  # Warn at 80% of limit

# ============================================
# UI Configuration
# ============================================
UI_THEME=dark  # dark, light, auto
UI_REFRESH_RATE=100  # milliseconds
UI_MAX_OUTPUT_LINES=1000
UI_SHOW_TIMESTAMPS=true
UI_SHOW_AGENT_ICONS=true
UI_ENABLE_COLORS=true
UI_ENABLE_ANIMATIONS=true

# ============================================
# File System Configuration
# ============================================
WORKSPACE_DIR=~/.macp/workspace
MAX_FILE_SIZE=10485760  # 10MB
ALLOWED_FILE_EXTENSIONS=.py,.js,.ts,.java,.cpp,.c,.h,.md,.txt,.json,.yaml,.yml,.toml,.ini,.cfg,.conf,.sh,.bash
AUTO_SAVE_INTERVAL=60  # seconds

# ============================================
# Security Configuration
# ============================================
ENABLE_INPUT_VALIDATION=true
ENABLE_OUTPUT_SANITIZATION=true
MAX_INPUT_LENGTH=10000  # characters
BLOCKED_COMMANDS=rm -rf,sudo rm,format,del /f
SANDBOX_MODE=false

# ============================================
# Monitoring Configuration
# ============================================
METRICS_ENABLED=true
METRICS_EXPORT_INTERVAL=60  # seconds
HEALTH_CHECK_INTERVAL=30  # seconds
ALERT_EMAIL=admin@example.com
ALERT_WEBHOOK_URL=

# ============================================
# Development Settings
# ============================================
DEV_MODE=false
DEV_AUTO_RELOAD=true
DEV_SHOW_ERRORS=true
DEV_MOCK_APIS=false
DEV_SKIP_AUTH=false

# ============================================
# External Integrations (Optional)
# ============================================
# GitHub integration
GITHUB_TOKEN=
GITHUB_DEFAULT_BRANCH=main

# Slack notifications
SLACK_WEBHOOK_URL=
SLACK_CHANNEL=#macp-notifications

# Sentry error tracking
SENTRY_DSN=
SENTRY_ENVIRONMENT=development
SENTRY_TRACES_SAMPLE_RATE=0.1
</file>

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto

# Python
*.py text
*.pyi text

# Config files
*.json text
*.yaml text
*.yml text
*.toml text
*.ini text
*.cfg text

# Documentation
*.md text
*.rst text
*.txt text

# Scripts
*.sh text eol=lf
*.bash text eol=lf

# Docker
Dockerfile text
docker-compose*.yml text
</file>

<file path=".gitignore">
# AngelaMCP .gitignore

# ============================================
# Python
# ============================================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# poetry
poetry.lock

# pdm
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# ============================================
# Virtual Environments
# ============================================
.env
env/
venv/
ENV/
env.bak/
venv.bak/
.venv/

# ============================================
# IDEs and Editors
# ============================================
# VSCode
.vscode/
*.code-workspace

# PyCharm
.idea/
*.iml
*.iws
*.ipr

# Sublime Text
*.sublime-project
*.sublime-workspace

# Vim
*.swp
*.swo
*.swn
.vim/

# Emacs
*~
\#*\#
.\#*

# macOS
.DS_Store
.AppleDouble
.LSOverride

# Windows
Thumbs.db
ehthumbs.db
Desktop.ini

# ============================================
# Project Specific
# ============================================
# Environment files
.env
.env.local
.env.*.local
.env.development
.env.production
.env.staging

# Logs
logs/
*.log
*.log.*
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Session data
sessions/
.macp/
claude_sessions/

# Workspace files
workspace/
temp/
tmp/

# Database
*.db
*.sqlite
*.sqlite3
backups/
dumps/

# Credentials and secrets
*.pem
*.key
*.cert
*.crt
secrets/
credentials/

# ============================================
# Testing and Quality
# ============================================
# MyPy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# Ruff
.ruff_cache/

# ============================================
# Documentation
# ============================================
site/
_build/
.doctrees/

# ============================================
# CI/CD
# ============================================
.gitlab-ci-local/
.github/workflows/local/

# ============================================
# Docker
# ============================================
.dockerignore
docker-compose.override.yml

# ============================================
# Miscellaneous
# ============================================
# Backup files
*.bak
*.backup
*.old
*.orig

# Archive files
*.zip
*.tar
*.tar.gz
*.tgz
*.rar

# Cache directories
.cache/
cache/

# Output directories
output/
results/
reports/

# Monitoring and metrics
metrics/
.metrics/

# User-specific files
personal/
private/
local/

# Generated files
generated/
gen/

# Temporary files
*.tmp
*.temp
~$*

# Lock files (except package managers)
*.lock
!package-lock.json
!yarn.lock
!poetry.lock
!Pipfile.lock

# ============================================
# Security
# ============================================
# Never commit these
.netrc
.authinfo
.pgpass
.ssh/
id_rsa*
id_dsa*
*.gpg

# API keys and tokens
*_api_key
*_token
.credentials/

# ============================================
# Operating System
# ============================================
# Linux
*~
.fuse_hidden*
.directory
.Trash-*
.nfs*

# macOS
.DS_Store
.AppleDouble
.LSOverride
Icon
._*
.DocumentRevisions-V100
.fseventsd
.Spotlight-V100
.TemporaryItems
.Trashes
.VolumeIcon.icns
.com.apple.timemachine.donotpresent
.AppleDB
.AppleDesktop
Network Trash Folder
Temporary Items
.apdisk

# Windows
Thumbs.db
Thumbs.db:encryptable
ehthumbs.db
ehthumbs_vista.db
*.stackdump
[Dd]esktop.ini
$RECYCLE.BIN/
*.cab
*.msi
*.msix
*.msm
*.msp
*.lnk

/context/
context/
/context
context
</file>

<file path="ARCHITECTURE.md">
# AngelaMCP Architecture Documentation

## üèóÔ∏è System Overview

The Multi-AI Agent Collaboration Platform (AngelaMCP) is designed as a modular, scalable system that orchestrates collaboration between multiple AI agents with Claude Code as the primary agent.

## üîÑ Core Architecture Flow

```mermaid
graph TB
    User[User Input] --> CLI[CLI Interface]
    CLI --> Orchestrator[Task Orchestrator]
    
    Orchestrator --> TaskAnalyzer[Task Analyzer]
    TaskAnalyzer --> Queue[Async Task Queue]
    
    Queue --> ClaudeCode[Claude Code Agent]
    Queue --> OpenAI[OpenAI Agent]
    Queue --> Gemini[Gemini Agent]
    
    ClaudeCode --> Debate[Debate Protocol]
    OpenAI --> Debate
    Gemini --> Debate
    
    Debate --> Voting[Voting System]
    Voting --> Consensus[Consensus Builder]
    
    Consensus --> Execution[Task Execution]
    Execution --> Results[Results Aggregator]
    
    Results --> UI[Rich Terminal UI]
    UI --> User
    
    Orchestrator --> DB[(PostgreSQL)]
    Orchestrator --> Cache[(Redis)]
```

## üß© Component Architecture

### 1. **Agent Layer**

```python
# Base Agent Interface
class BaseAgent(ABC):
    """Abstract base class for all AI agents."""
    
    @abstractmethod
    async def generate(self, prompt: str, context: Context) -> AgentResponse:
        """Generate response from agent."""
        pass
    
    @abstractmethod
    async def critique(self, solution: Solution) -> Critique:
        """Critique another agent's solution."""
        pass
```

**Agent Hierarchy:**
- **Claude Code Agent** (Senior Developer)
  - Primary task execution
  - File system operations
  - Code execution capabilities
  - Final decision authority

- **OpenAI Agent** (Code Reviewer)
  - Code quality assessment
  - Security analysis
  - Performance optimization suggestions

- **Gemini Agent** (Research Specialist)
  - Documentation generation
  - Best practices research
  - Parallel task execution

### 2. **Orchestration Layer**

The orchestrator manages the entire collaboration workflow:

```python
class TaskOrchestrator:
    """Manages agent collaboration and task execution."""
    
    async def execute_task(self, task: Task) -> TaskResult:
        # 1. Analyze task complexity
        analysis = await self.analyze_task(task)
        
        # 2. Distribute to agents
        if analysis.requires_collaboration:
            return await self.collaborative_execution(task)
        else:
            return await self.single_agent_execution(task)
    
    async def collaborative_execution(self, task: Task) -> TaskResult:
        # 1. Parallel proposal generation
        proposals = await self.gather_proposals(task)
        
        # 2. Debate phase
        debate_result = await self.conduct_debate(proposals)
        
        # 3. Voting and consensus
        consensus = await self.reach_consensus(debate_result)
        
        # 4. Execute final solution
        return await self.execute_solution(consensus)
```

### 3. **Collaboration Protocol**

#### **Debate System**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Debate Protocol               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Initial Proposals                    ‚îÇ
‚îÇ    - Each agent submits solution        ‚îÇ
‚îÇ    - Time limit: 180 seconds            ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 2. Critique Round                       ‚îÇ
‚îÇ    - Agents review others' proposals    ‚îÇ
‚îÇ    - Identify strengths/weaknesses      ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 3. Rebuttal Phase                       ‚îÇ
‚îÇ    - Agents defend their approaches     ‚îÇ
‚îÇ    - Incorporate valid feedback         ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ 4. Final Proposals                      ‚îÇ
‚îÇ    - Submit refined solutions           ‚îÇ
‚îÇ    - Ready for voting                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### **Voting Mechanism**

```python
class VotingSystem:
    """Weighted voting system for agent consensus."""
    
    VOTE_WEIGHTS = {
        "claude_code": 2.0,  # Senior developer weight
        "openai": 1.0,
        "gemini": 1.0
    }
    
    def calculate_winner(self, votes: List[Vote]) -> Solution:
        # Weighted scoring with Claude Code having veto power
        scores = self.calculate_weighted_scores(votes)
        
        if self.claude_has_strong_objection(votes):
            return self.get_claude_preferred_solution()
        
        return self.get_highest_scored_solution(scores)
```

### 4. **Task Management**

#### **Task Classification**

```python
@dataclass
class TaskClassification:
    primary_agent: AgentType
    supporting_agents: List[AgentType]
    collaboration_required: bool
    parallel_tasks: List[SubTask]
    estimated_complexity: ComplexityLevel
```

#### **Async Task Queue**

```python
class AsyncTaskQueue:
    """Manages parallel task execution."""
    
    async def process_tasks(self, tasks: List[Task]):
        # Primary task for Claude Code
        primary_task = self.extract_primary_task(tasks)
        
        # Parallel tasks for supporting agents
        parallel_tasks = self.extract_parallel_tasks(tasks)
        
        # Execute concurrently
        results = await asyncio.gather(
            self.claude_code.execute(primary_task),
            *[self.execute_parallel(task) for task in parallel_tasks]
        )
        
        return self.merge_results(results)
```

### 5. **Persistence Layer**

#### **Database Schema**

```sql
-- Conversations table
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID NOT NULL,
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ended_at TIMESTAMP,
    status VARCHAR(50) NOT NULL,
    metadata JSONB
);

-- Messages table
CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID REFERENCES conversations(id),
    agent_type VARCHAR(50) NOT NULL,
    role VARCHAR(20) NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Task executions table
CREATE TABLE task_executions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID REFERENCES conversations(id),
    task_type VARCHAR(100),
    status VARCHAR(50),
    input_data JSONB,
    output_data JSONB,
    debate_data JSONB,
    voting_data JSONB,
    started_at TIMESTAMP,
    completed_at TIMESTAMP
);

-- Agent proposals table
CREATE TABLE agent_proposals (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    task_execution_id UUID REFERENCES task_executions(id),
    agent_type VARCHAR(50),
    proposal_content TEXT,
    critique_data JSONB,
    vote_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### **Caching Strategy**

```python
class CacheManager:
    """Redis-based caching for active sessions."""
    
    async def cache_conversation(self, session_id: str, data: dict):
        # Cache active conversation in Redis
        key = f"conversation:{session_id}"
        await self.redis.setex(
            key, 
            settings.SESSION_TIMEOUT, 
            json.dumps(data)
        )
    
    async def get_cached_conversation(self, session_id: str):
        # Retrieve from cache, fallback to DB
        cached = await self.redis.get(f"conversation:{session_id}")
        if cached:
            return json.loads(cached)
        
        # Load from PostgreSQL and re-cache
        return await self.load_from_db(session_id)
```

### 6. **UI Layer**

#### **Terminal Interface Architecture**

```python
class RichTerminalUI:
    """Rich terminal interface with real-time updates."""
    
    def __init__(self):
        self.console = Console()
        self.layout = self.create_layout()
        self.live = Live(self.layout, console=self.console)
    
    def create_layout(self) -> Layout:
        # Split terminal into sections
        return Layout()
            .split_row(
                Layout(name="agents", ratio=3),
                Layout(name="status", ratio=1)
            )
            .split_column(
                Layout(name="output", ratio=4),
                Layout(name="input", ratio=1)
            )
```

## üîê Security Architecture

### **API Key Management**
- Environment variables for API keys
- Never logged or exposed in outputs
- Encrypted storage in database

### **Input Validation**
- Pydantic models for all inputs
- SQL injection prevention via SQLAlchemy
- Rate limiting per API

### **Error Handling**
```python
class ErrorHandler:
    """Centralized error handling with fallbacks."""
    
    async def handle_agent_error(self, agent: str, error: Exception):
        # Log error with context
        logger.error(f"Agent {agent} error", exc_info=error)
        
        # Attempt fallback strategies
        if isinstance(error, RateLimitError):
            await self.handle_rate_limit(agent)
        elif isinstance(error, APIError):
            await self.attempt_retry(agent)
        
        # Graceful degradation
        return self.get_fallback_response(agent)
```

## üìä Performance Considerations

### **Concurrency Model**
- AsyncIO for I/O-bound operations
- Thread pool for CPU-bound tasks
- Connection pooling for databases

### **Optimization Strategies**
1. **Batch API Calls**: Group related requests
2. **Smart Caching**: Cache common queries
3. **Lazy Loading**: Load agent responses on-demand
4. **Stream Processing**: Process responses as they arrive

## üîÑ Data Flow Example

```
1. User Input: "Create a REST API with authentication"
   ‚Üì
2. Task Analysis:
   - Type: Development
   - Complexity: High
   - Requires: Collaboration
   ‚Üì
3. Task Distribution:
   - Claude Code: Main API implementation
   - Gemini: Research auth best practices
   - OpenAI: Prepare security review
   ‚Üì
4. Parallel Execution:
   - All agents work simultaneously
   - Results streamed to UI
   ‚Üì
5. Debate Phase:
   - JWT vs Session debate
   - Security considerations
   - Performance analysis
   ‚Üì
6. Voting:
   - Claude Code: JWT (weight: 2.0)
   - OpenAI: Sessions (weight: 1.0)
   - Gemini: JWT (weight: 1.0)
   - Result: JWT wins
   ‚Üì
7. Implementation:
   - Claude Code implements JWT auth
   - Creates files, writes code
   - Other agents review
   ‚Üì
8. Output:
   - Complete API with JWT auth
   - Documentation by Gemini
   - Security notes by OpenAI
```

## üöÄ Scalability Paths

1. **Horizontal Scaling**
   - Multiple orchestrator instances
   - Load balancing via Redis queues
   - Distributed task processing

2. **Agent Scaling**
   - Dynamic agent pools
   - Plugin architecture for new agents
   - Custom agent implementations

3. **Storage Scaling**
   - PostgreSQL read replicas
   - Redis clustering
   - S3 for file storage

---
</file>

<file path="LICENSE">
MIT License

Copyright ¬©2025 Certgames.com ¬ÆAngelaMos

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="Makefile">
# AngelaMCP - Multi-AI Agent Collaboration Platform
# Production-grade Makefile for complete setup and management

SHELL = /bin/bash

PYTHON = venv/bin/python

.PHONY: help setup install db-setup verify run run-mcp test clean lint format \
        docker-build docker-up docker-down docker-logs docker-clean docker-prune-all \
		mcp-register mcp-test status

PYTHON_VERSION := $(shell python3 -c 'import sys; print(f"{sys.version_info.major}.{sys.version_info.minor}")')


# Default target
help:
	@echo "üëª AngelaMCP - Multi-AI Agent Collaboration Platform"
	@echo "=================================================="
	@echo ""
	@echo "Setup Commands:"
	@echo "  make setup        - Complete setup (recommended for first time)"
	@echo "  make install      - Install Python dependencies only"
	@echo "  make deps         - Install system dependencies (PostgreSQL, Redis)"
	@echo "  make db-setup     - Setup database (create DB, tables, verify)"
	@echo "  make verify       - Verify entire setup is working"
	@echo ""
	@echo "Development Commands:"
	@echo "  make run          - Run AngelaMCP standalone CLI"
	@echo "  make run-mcp      - Run as MCP server"
	@echo "  make test         - Run test suite"
	@echo "  make lint         - Run code linting"
	@echo "  make format       - Format code with ruff"
	@echo ""
	@echo "MCP Commands:"
	@echo "  make mcp-register - Register with Claude Code"
	@echo "  make mcp-test     - Test MCP integration"
	@echo ""
	@echo "Docker Commands:"
	@echo "  make docker-build - Build Docker containers"
	@echo "  make docker-up    - Start with Docker Compose"
	@echo "  make docker-down  - Stop Docker containers"
	@echo ""
	@echo "Maintenance:"
	@echo "  make clean        - Clean temporary files"
	@echo "  make reset        - Reset database and logs"

# Complete setup for new installations
setup: deps install env-setup db-setup verify
	@echo "‚úÖ Complete setup finished!"
	@echo ""
	@echo "Next steps:"
	@echo "1. Edit .env with your API keys"
	@echo "2. Run: make verify"
	@echo "3. Run: make mcp-register"
	@echo "4. Test: make run"

# Install system dependencies
deps:
	@echo "üì¶ Installing system dependencies..."
	sudo apt update
	sudo apt install -y postgresql postgresql-contrib redis-server python$(PYTHON_VERSION)-dev libpq-dev
	sudo systemctl start postgresql redis-server
	sudo systemctl enable postgresql redis-server
	@echo "‚úÖ System dependencies installed"

# Install Python dependencies
install:
	@echo "üêç Installing Python dependencies..."
	@test -d venv || python3 -m venv venv 
	$(PYTHON) -m pip install --upgrade pip
	$(PYTHON) -m pip install -r requirements.t`
	@echo "‚úÖ Python dependencies installed"
        
# Setup environment file
env-setup:
	@if [ ! -f .env ]; then \
		echo "üìù Creating .env file..."; \
		cp .env.example .env; \
		echo "‚ö†Ô∏è  Please edit .env with your API keys"; \
	else \
		echo "‚úÖ .env file already exists"; \
	fi

# Complete database setup
db-setup: db-init db-verify
	@echo "‚úÖ Database setup complete"

# Initialize database
db-init:
	@echo "üóÑÔ∏è Initializing database..."
	python scripts/init_db.py

# Verify database setup
db-verify:
	@echo "üîç Verifying database..."
	python -c "import asyncio; from src.persistence.database import DatabaseManager; asyncio.run(DatabaseManager().initialize())"

# Verify complete setup
verify:
	@echo "üîç Verifying AngelaMCP setup..."
	python scripts/verify_setup.py

# Run AngelaMCP standalone
run:
	@echo "üïµÔ∏è‚Äç‚ôÄÔ∏è Starting AngelaMCP standalone..."
	python -m src.main

# Run as MCP server
run-mcp:
	@echo "üßô‚Äç‚ôÄÔ∏è Starting AngelaMCP MCP server..."
	python -m src.main mcp-server

# Run development mode with debug
run-dev:
	@echo "üë©‚Äçüé® Starting AngelaMCP in development mode..."
	DEBUG=true LOG_LEVEL=DEBUG python -m src.main

# Register MCP server with Claude Code
mcp-register:
	@echo "üíÉ Registering AngelaMCP with Claude Code..."
	@if command -v claude >/dev/null 2>&1; then \
		claude mcp add angelamcp -s project -- python -m src.mcp_server && \
		echo "‚úÖ MCP server registered successfully" && \
		echo "Test with: claude 'Use AngelaMCP to help with a task'"; \
	else \
		echo "‚ùå Claude Code not found. Install Claude Code first."; \
		exit 1; \
	fi

# Test MCP integration
mcp-test:
	@echo "üß™ Testing MCP integration..."
	@if command -v claude >/dev/null 2>&1; then \
		claude mcp list | grep angelamcp || echo "‚ùå AngelaMCP not registered"; \
		echo "Run 'make mcp-register' to register the server"; \
	else \
		echo "‚ùå Claude Code not found"; \
	fi

# Run tests
test:
	@echo "üß™ Running tests..."
	pytest tests/ -v

# Run tests with coverage
test-coverage:
	@echo "üß™ Running tests with coverage..."
	pytest tests/ --cov=src --cov-report=html --cov-report=term

# Lint code
lint:
	@echo "üîç Linting code..."
	ruff check src/ config/ tests/
	mypy src/ --ignore-missing-imports

# Format code
format:
	@echo "‚ú® Formatting code..."
	ruff format src/ config/ tests/
	ruff check --fix src/ config/ tests/

# Clean temporary files
clean:
	@echo "üßπ Cleaning temporary files..."
	find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	find . -type f -name "*.pyc" -delete 2>/dev/null || true
	find . -type f -name "*.pyo" -delete 2>/dev/null || true
	find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
	rm -rf .pytest_cache/ .coverage htmlcov/ .mypy_cache/ .ruff_cache/
	@echo "‚úÖ Cleanup complete"

# Reset database and logs
reset: clean
	@echo "üîÑ Resetting AngelaMCP..."
	rm -rf logs/*.log
	@echo "Reset database? [y/N]" && read ans && [ $${ans:-N} = y ] && \
		python -c "from sqlalchemy import create_engine; from config.settings import settings; from src.persistence.models import Base; engine = create_engine(str(settings.database_url)); Base.metadata.drop_all(engine); Base.metadata.create_all(engine); print('‚úÖ Database reset')" || \
		echo "Database reset skipped"


docker-build:
	@echo "üê≥ Building Docker containers..."
	@docker-compose --env-file .env -f docker/docker-compose.yml build

docker-up:
	@echo "üê≥ Starting Docker containers..."
	@docker-compose --env-file .env -f docker/docker-compose.yml up -d
	@echo "‚úÖ Containers started. Check status with: docker-compose ps"

docker-down:
	@echo "üê≥ Stopping Docker containers..."
	@docker-compose --env-file .env -f docker/docker-compose.yml down
	
docker-logs:
	@echo "üìã Showing Docker logs..."
	@docker-compose --env-file .env -f docker/docker-compose.yml logs -f

docker-clean:
	@echo "üßπ Cleaning up all containers, networks, and volumes for THIS project..."
	@docker-compose --env-file .env -f docker/docker-compose.yml down --volumes
	@echo "‚úÖ Project cleanup complete."

docker-prune-all:
	@echo "‚ò¢Ô∏è  WARNING: This will remove ALL unused Docker assets on your entire system."
	@echo "This affects ALL projects, not just this one. This cannot be undone."
	@read -p "Are you absolutely sure you want to continue? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		echo "Pruning system..."; \
		docker system prune -a --volumes -f; \
		echo "‚úÖ Docker system prune complete."; \
	else \
		echo "Prune cancelled."; \
	fi


# Health check
health:
	@echo "üè• AngelaMCP Health Check..."
	@echo "PostgreSQL:" && systemctl is-active postgresql && echo "‚úÖ" || echo "‚ùå"
	@echo "Redis:" && systemctl is-active redis-server && echo "‚úÖ" || echo "‚ùå"
	@echo "Claude Code:" && claude --version >/dev/null 2>&1 && echo "‚úÖ" || echo "‚ùå"
	@echo "Python deps:" && python -c "import openai, google.genai; print('‚úÖ')" 2>/dev/null || echo "‚ùå"

# Show status
status:
	@echo "üïµÔ∏è‚Äç‚ôÄÔ∏è AngelaMCP Status"
	@echo "=================="
	@echo "Project Root: $(PWD)"
	@echo "Python: $(shell python --version)"
	@echo "Environment: $(shell [ -f .env ] && echo '‚úÖ Configured' || echo '‚ùå Missing')"
	@echo "Database: $(shell python -c "from config.settings import settings; print(settings.database_url)" 2>/dev/null | sed 's/:.*/.../' || echo '‚ùå Not configured')"
	@echo "MCP Server: $(shell claude mcp list 2>/dev/null | grep -q angelamcp && echo '‚úÖ Registered' || echo '‚ùå Not registered')"

# Development shortcuts
dev: run-dev
prod: run
server: run-mcp

# All-in-one commands for different use cases
first-time: setup mcp-register
	@echo "üëª First-time setup complete!"
	@echo "Try: make run"

quick-start: install verify run

# Backup and restore (for production)
backup:
	@echo "üíæ Creating backup..."
	mkdir -p backups
	pg_dump $(shell python -c "from config.settings import settings; print(settings.database_url)") > backups/angelamcp_$(shell date +%Y%m%d_%H%M%S).sql
	@echo "‚úÖ Backup created in backups/"

# Show environment info
env-info:
	@echo "üîß Environment Information"
	@echo "========================="
	@python -c "from config.settings import settings; import json; print(json.dumps({k: '***' if 'key' in k.lower() or 'password' in k.lower() else str(v) for k, v in settings.__dict__.items() if not k.startswith('_')}, indent=2))"
</file>

<file path="pyproject.toml">
[tool.poetry]
name = "AngelaMCP"
version = "1.0.0"
description = "Multi-AI Agent Collaboration Platform - Orchestrating Claude Code, OpenAI, and Gemini"
authors = ["CarterPerez carterperez@certgames.com"]
readme = "README.md"
license = "MIT"
packages = [{include = "src"}, {include = "config"}]

[tool.poetry.scripts]
macp = "src.main:cli_main"
angelamcp = "src.main:cli_main"

[tool.poetry.dependencies]
python = "^3.10"
# Core dependencies
asyncio = "*"
asyncpg = "^0.29.0"
sqlalchemy = {extras = ["asyncio"], version = "^2.0.0"}
alembic = "^1.13.0"
redis = "^5.0.0"
pydantic = "^2.5.0"
pydantic-settings = "^2.1.0"

# AI/ML APIs
openai = "^1.51.0"
google-generativeai = "^0.8.0"

# CLI and UI
rich = "^13.7.0"
click = "^8.1.0"
typer = "^0.12.0"

# HTTP and networking
httpx = "^0.26.0"
aiohttp = "^3.9.0"

# Utilities
python-dotenv = "^1.0.0"
pyyaml = "^6.0.0"
psutil = "^5.9.0"
loguru = "^0.7.0"

# Development tools
black = "^23.12.0"
ruff = "^0.1.0"
mypy = "^1.8.0"
pre-commit = "^3.6.0"

[tool.poetry.group.dev.dependencies]
# Testing
pytest = "^7.4.0"
pytest-asyncio = "^0.23.0"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"
pytest-xdist = "^3.5.0"
pytest-benchmark = "^4.0.0"

# Development
ipython = "^8.18.0"
jupyter = "^1.0.0"
notebook = "^7.0.0"

# Linting and formatting
bandit = "^1.7.0"
safety = "^2.3.0"
vulture = "^2.10.0"

# Documentation
mkdocs = "^1.5.0"
mkdocs-material = "^9.5.0"
mkdocstrings = "^0.24.0"

# Profiling and debugging
py-spy = "^0.3.0"
memory-profiler = "^0.61.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

# Ruff configuration - Fast Python linter
[tool.ruff]
target-version = "py310"
line-length = 88
select = [
    "E",    # pycodestyle errors
    "W",    # pycodestyle warnings
    "F",    # pyflakes
    "I",    # isort
    "B",    # flake8-bugbear
    "C4",   # flake8-comprehensions
    "UP",   # pyupgrade
    "ARG",  # flake8-unused-arguments
    "SIM",  # flake8-simplify
    "TCH",  # flake8-type-checking
    "DTZ",  # flake8-datetimez
    "Q",    # flake8-quotes
    "PTH",  # flake8-use-pathlib
    "RSE",  # flake8-raise
    "RET",  # flake8-return
    "LOG",  # flake8-logging
    "RUF",  # Ruff-specific rules
    "G",    # flake8-logging-format
    "N",    # pep8-naming
    "ASYNC",# flake8-async
    "S",    # flake8-bandit (security)
    "BLE",  # flake8-blind-except
    "A",    # flake8-builtins
    "COM",  # flake8-commas
    "C90",  # mccabe complexity
    "DJ",   # flake8-django
    "EM",   # flake8-errmsg
    "EXE",  # flake8-executable
    "ISC",  # flake8-implicit-str-concat
    "ICN",  # flake8-import-conventions
    "T10",  # flake8-debugger
    "T20",  # flake8-print
    "PIE",  # flake8-pie
    "PT",   # flake8-pytest-style
    "PD",   # pandas-vet
    "PGH",  # pygrep-hooks
    "TRY",  # tryceratops
    "FLY",  # flynt
    "PERF", # perflint
    "ERA",  # flake8-eradicate
    "NPY",  # NumPy-specific rules
]
ignore = [
    "E501",   # line too long (handled by black)
    "B008",   # do not perform function calls in argument defaults
    "C901",   # too complex
    "S101",   # use of assert detected
    "S603",   # subprocess call - we need this for Claude Code
    "S607",   # start process with partial path - needed for CLI tools
    "DTZ003", # use of datetime.utcnow()
    "RET504", # unnecessary assignment
    "SIM108", # ternary operator
    "TRY003", # long messages in exceptions
    "EM101",  # raw string in exception
    "EM102",  # f-string in exception
    "G004",   # f-string in logging
    "PTH123", # open() should be replaced with Path.open()
]
fixable = ["ALL"]
unfixable = []
exclude = [
    ".git",
    ".venv",
    "venv",
    "__pycache__",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    "build",
    "dist",
    "docs",
    "scripts/migrations",
]

[tool.ruff.per-file-ignores]
"tests/*" = ["S101", "ARG", "PLR2004", "S106"]
"scripts/*" = ["T201", "T203"]
"__init__.py" = ["F401"]

[tool.ruff.isort]
known-first-party = ["src", "config"]
combine-as-imports = true
force-wrap-aliases = true

[tool.ruff.mccabe]
max-complexity = 10

[tool.ruff.pydocstyle]
convention = "google"

# Black configuration
[tool.black]
line-length = 88
target-version = ['py310']
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | _build
  | buck-out
  | build
  | dist
)/
'''

# MyPy configuration
[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = false
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
ignore_missing_imports = true
pretty = true
show_error_codes = true
show_error_context = true
show_column_numbers = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false

# Pytest configuration
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-ra",
    "--strict-markers",
    "--cov=src",
    "--cov-branch",
    "--cov-report=term-missing:skip-covered",
    "--cov-report=html",
    "--cov-report=xml",
    "--cov-fail-under=80",
    "--maxfail=1",
    "--tb=short",
    "--asyncio-mode=auto",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "requires_api: marks tests that require API keys",
]
filterwarnings = [
    "error",
    "ignore::UserWarning",
    "ignore::DeprecationWarning",
]

# Coverage configuration
[tool.coverage.run]
source = ["src"]
branch = true
omit = [
    "*/tests/*",
    "*/test_*.py",
    "*/__init__.py",
    "*/migrations/*",
    "*/venv/*",
    "*/.venv/*",
]

[tool.coverage.report]
precision = 2
show_missing = true
skip_covered = false
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "class .*\\(Protocol\\):",
    "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"

# isort configuration (if not using ruff's isort)
[tool.isort]
profile = "black"
line_length = 88
known_first_party = ["src", "config"]
skip_gitignore = true
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true

# Bandit security linter
[tool.bandit]
targets = ["src"]
exclude_dirs = ["tests", "venv", ".venv"]
skips = ["B101", "B601", "B602", "B603", "B607"]

# Commitizen for conventional commits
[tool.commitizen]
name = "cz_conventional_commits"
version = "1.0.0"
tag_format = "v$version"
version_files = [
    "pyproject.toml:version",
    "src/__init__.py:__version__"
]

# Semantic Release
[tool.semantic_release]
version_variable = "src/__init__.py:__version__"
version_toml = "pyproject.toml:tool.poetry.version"
upload_to_repository = false
upload_to_release = true
build_command = "poetry build"
</file>

<file path="pytest.ini">
[tool:pytest]
# Pytest configuration for AngelaMCP

# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, multiple components)
    slow: Slow tests (may take several seconds)
    ui: UI-related tests
    database: Tests requiring database
    agents: Agent-specific tests
    orchestration: Orchestration system tests
    debug: Tests for debugging purposes (not run by default)

# Output and logging
addopts = 
    --verbose
    --tb=short
    --strict-markers
    --disable-warnings

# Async support
asyncio_mode = auto

# Test filtering
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:asyncio

# Minimum version
minversion = 7.0

# Test discovery patterns
norecursedirs = 
    .git
    .tox
    dist
    build
    *.egg
    venv
    __pycache__

# Timeout for individual tests (in seconds)
timeout = 300

# Coverage configuration
[coverage:run]
source = src
omit = 
    */tests/*
    */venv/*
    */__pycache__/*
    */migrations/*
    */scripts/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

[coverage:html]
directory = htmlcov
</file>

<file path="requirements.txt">
# Core dependencies
asyncio-mqtt==0.16.2
asyncpg==0.30.0
sqlalchemy[asyncio]==2.0.41
alembic==1.16.1
redis==6.2.0
pydantic==2.11.5
pydantic-settings==2.9.1
psycopg2==2.9.10
tiktoken

# AI/ML APIs
openai==1.86.0
google-genai==1.19.0

# MCP Protocol Support
mcp==1.9.3
anthropic==0.54.0

# CLI and UI
rich==14.0.0
click==8.2.1
typer==0.16.0
prompt-toolkit==3.0.43

# HTTP and networking
httpx==0.28.1
aiohttp==3.12.12

# Utilities
python-dotenv==1.0.0
pyyaml==6.0.2
psutil==7.0.0
loguru==0.7.3

# Development and testing
pytest==8.4.0
pytest-asyncio==1.0.0
pytest-mock==3.14.1
ruff==0.11.3
mypy==1.16.0

# Performance monitoring
asyncio-throttle==1.0.2
memory-profiler==0.61.0

# Additional utilities
tenacity==9.1.2  # For retry logic
backoff==2.2.1   # For exponential backoff
uuid==1.30       # For unique identifiers
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: yoshi
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run tests
      run: |
        pytest tests/ --cov=src --cov-report=xml
    
    - name: Run linting
      run: |
        ruff check src/ tests/
        mypy src/
</file>

<file path="config/settings.py">
"""
AngelaMCP Configuration Settings

Centralized configuration management using Pydantic Settings.
All settings are loaded from environment variables with validation.
"""

import os
from pathlib import Path
from typing import Optional, List, Dict, Any
from enum import Enum
from typing import Optional
from pydantic import Field, validator, PostgresDsn, RedisDsn
from pydantic_settings import BaseSettings
from pydantic.types import SecretStr


class Environment(str, Enum):
    """Application environment types."""
    DEVELOPMENT = "development"
    TESTING = "testing"
    STAGING = "staging"
    PRODUCTION = "production"


class LogLevel(str, Enum):
    """Logging level options."""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class UITheme(str, Enum):
    """UI theme options."""
    DARK = "dark"
    LIGHT = "light"
    AUTO = "auto"


class Settings(BaseSettings):
    """Main application settings."""

    # Application Settings
    app_name: str = Field(default="ANGELAMCP", env="APP_NAME")
    app_env: Environment = Field(default=Environment.DEVELOPMENT, env="APP_ENV")
    app_version: str = Field(default="1.0.0", env="APP_VERSION")
    debug: bool = Field(default=False, env="DEBUG")
    log_level: LogLevel = Field(default=LogLevel.INFO, env="LOG_LEVEL")
    log_file: Path = Field(default=Path("logs/macp.log"), env="LOG_FILE")
    log_max_size: int = Field(default=10485760, env="LOG_MAX_SIZE")  # 10MB
    log_backup_count: int = Field(default=5, env="LOG_BACKUP_COUNT")

    # Claude Code Configuration
    claude_code_path: Path = Field(default=Path("/usr/local/bin/claude"), env="CLAUDE_CODE_PATH")
    claude_code_timeout: int = Field(default=300, env="CLAUDE_CODE_TIMEOUT")
    claude_code_max_turns: int = Field(default=10, env="CLAUDE_CODE_MAX_TURNS")
    claude_code_output_format: str = Field(default="json", env="CLAUDE_CODE_OUTPUT_FORMAT")
    claude_session_persist: bool = Field(default=True, env="CLAUDE_SESSION_PERSIST")
    claude_session_dir: Path = Field(default=Path("~/.macp/claude_sessions").expanduser(), env="CLAUDE_SESSION_DIR")

    # OpenAI Configuration
    openai_api_key: SecretStr = Field(..., env="OPENAI_API_KEY")
    openai_model: str = Field(default="o3-mini", env="OPENAI_MODEL")
    openai_max_tokens: int = Field(default=4096, env="OPENAI_MAX_TOKENS")
    openai_temperature: float = Field(default=0.7, env="OPENAI_TEMPERATURE")
    openai_top_p: float = Field(default=0.9, env="OPENAI_TOP_P")
    openai_frequency_penalty: float = Field(default=0.0, env="OPENAI_FREQUENCY_PENALTY")
    openai_presence_penalty: float = Field(default=0.0, env="OPENAI_PRESENCE_PENALTY")
    openai_timeout: int = Field(default=120, env="OPENAI_TIMEOUT")
    openai_rate_limit: int = Field(default=60, env="OPENAI_RATE_LIMIT")
    openai_max_retries: int = Field(default=3, env="OPENAI_MAX_RETRIES")
    openai_retry_delay: int = Field(default=1, env="OPENAI_RETRY_DELAY")

    # Google Gemini Configuration
    google_api_key: SecretStr = Field(..., env="GOOGLE_API_KEY")
    gemini_model: str = Field(default="gemini-2.5-pro-preview-06-05", env="GEMINI_MODEL")
    gemini_max_output_tokens: int = Field(default=4096, env="GEMINI_MAX_OUTPUT_TOKENS")
    gemini_temperature: float = Field(default=0.7, env="GEMINI_TEMPERATURE")
    gemini_top_p: float = Field(default=0.9, env="GEMINI_TOP_P")
    gemini_top_k: int = Field(default=40, env="GEMINI_TOP_K")
    gemini_timeout: int = Field(default=120, env="GEMINI_TIMEOUT")
    gemini_harm_block_threshold: str = Field(default="BLOCK_MEDIUM_AND_ABOVE", env="GEMINI_HARM_BLOCK_THRESHOLD")
    gemini_rate_limit: int = Field(default=60, env="GEMINI_RATE_LIMIT")
    gemini_max_retries: int = Field(default=3, env="GEMINI_MAX_RETRIES")
    gemini_retry_delay: int = Field(default=1, env="GEMINI_RETRY_DELAY")

    # Database Configuration
    database_url: str = Field(..., env="DATABASE_URL")
    database_pool_size: int = Field(default=20, env="DATABASE_POOL_SIZE")
    database_max_overflow: int = Field(default=40, env="DATABASE_MAX_OVERFLOW")
    database_pool_timeout: int = Field(default=30, env="DATABASE_POOL_TIMEOUT")
    database_echo: bool = Field(default=False, env="DATABASE_ECHO")

    # Redis Configuration
    redis_url: str = Field(..., env="REDIS_URL")
    redis_max_connections: int = Field(default=50, env="REDIS_MAX_CONNECTIONS")
    redis_decode_responses: bool = Field(default=True, env="REDIS_DECODE_RESPONSES")
    redis_socket_timeout: int = Field(default=5, env="REDIS_SOCKET_TIMEOUT")
    redis_connection_timeout: int = Field(default=5, env="REDIS_CONNECTION_TIMEOUT")

    # Session Configuration
    session_timeout: int = Field(default=3600, env="SESSION_TIMEOUT")
    session_cleanup_interval: int = Field(default=300, env="SESSION_CLEANUP_INTERVAL")
    max_conversation_length: int = Field(default=100, env="MAX_CONVERSATION_LENGTH")
    max_concurrent_sessions: int = Field(default=10, env="MAX_CONCURRENT_SESSIONS")

    # Task Execution Configuration
    task_queue_max_size: int = Field(default=100, env="TASK_QUEUE_MAX_SIZE")
    task_execution_timeout: int = Field(default=1800, env="TASK_EXECUTION_TIMEOUT")
    parallel_task_limit: int = Field(default=5, env="PARALLEL_TASK_LIMIT")

    # Debate Protocol Settings
    debate_timeout: int = Field(default=300, env="DEBATE_TIMEOUT")
    debate_max_rounds: int = Field(default=3, env="DEBATE_MAX_ROUNDS")
    debate_min_participants: int = Field(default=2, env="DEBATE_MIN_PARTICIPANTS")

    # Voting Settings
    voting_timeout: int = Field(default=60, env="VOTING_TIMEOUT")
    claude_vote_weight: float = Field(default=2.0, env="CLAUDE_VOTE_WEIGHT")
    openai_vote_weight: float = Field(default=1.0, env="OPENAI_VOTE_WEIGHT")
    gemini_vote_weight: float = Field(default=1.0, env="GEMINI_VOTE_WEIGHT")
    claude_veto_enabled: bool = Field(default=True, env="CLAUDE_VETO_ENABLED")

    # Feature Flags
    enable_cost_tracking: bool = Field(default=True, env="ENABLE_COST_TRACKING")
    enable_parallel_execution: bool = Field(default=True, env="ENABLE_PARALLEL_EXECUTION")
    enable_debate_mode: bool = Field(default=True, env="ENABLE_DEBATE_MODE")
    enable_auto_save: bool = Field(default=True, env="ENABLE_AUTO_SAVE")
    enable_memory_persistence: bool = Field(default=True, env="ENABLE_MEMORY_PERSISTENCE")
    enable_analytics: bool = Field(default=False, env="ENABLE_ANALYTICS")
    enable_telemetry: bool = Field(default=False, env="ENABLE_TELEMETRY")

    # Cost Tracking Configuration (USD per 1K tokens)
    openai_input_cost: float = Field(default=0.003, env="OPENAI_INPUT_COST")
    openai_output_cost: float = Field(default=0.006, env="OPENAI_OUTPUT_COST")
    gemini_input_cost: float = Field(default=0.00025, env="GEMINI_INPUT_COST")
    gemini_output_cost: float = Field(default=0.0005, env="GEMINI_OUTPUT_COST")
    daily_budget_limit: float = Field(default=10.00, env="DAILY_BUDGET_LIMIT")
    monthly_budget_limit: float = Field(default=250.00, env="MONTHLY_BUDGET_LIMIT")
    budget_warning_threshold: float = Field(default=0.8, env="BUDGET_WARNING_THRESHOLD")

    # UI Configuration
    ui_theme: UITheme = Field(default=UITheme.DARK, env="UI_THEME")
    ui_refresh_rate: int = Field(default=100, env="UI_REFRESH_RATE")
    ui_max_output_lines: int = Field(default=1000, env="UI_MAX_OUTPUT_LINES")
    ui_show_timestamps: bool = Field(default=True, env="UI_SHOW_TIMESTAMPS")
    ui_show_agent_icons: bool = Field(default=True, env="UI_SHOW_AGENT_ICONS")
    ui_enable_colors: bool = Field(default=True, env="UI_ENABLE_COLORS")
    ui_enable_animations: bool = Field(default=True, env="UI_ENABLE_ANIMATIONS")

    # File System Configuration
    workspace_dir: Path = Field(default=Path("~/.angelamcp/workspace").expanduser(), env="WORKSPACE_DIR")
    max_file_size: int = Field(default=10485760, env="MAX_FILE_SIZE")  # 10MB
    allowed_file_extensions: List[str] = Field(
        default=[".py", ".js", ".ts", ".java", ".cpp", ".c", ".h", ".md", ".txt", ".json", ".yaml", ".yml", ".toml", ".ini", ".cfg", ".conf", ".sh", ".bash"],
        env="ALLOWED_FILE_EXTENSIONS"
    )
    auto_save_interval: int = Field(default=60, env="AUTO_SAVE_INTERVAL")

    # Security Configuration
    enable_input_validation: bool = Field(default=True, env="ENABLE_INPUT_VALIDATION")
    enable_output_sanitization: bool = Field(default=True, env="ENABLE_OUTPUT_SANITIZATION")
    max_input_length: int = Field(default=10000, env="MAX_INPUT_LENGTH")
    blocked_commands: List[str] = Field(
        default=["rm -rf", "sudo rm", "format", "del /f"],
        env="BLOCKED_COMMANDS"
    )
    sandbox_mode: bool = Field(default=False, env="SANDBOX_MODE")

    @validator("claude_code_path")
    def validate_claude_path(cls, v: Path) -> Path:
        """Ensure Claude Code executable exists."""
        if not v.exists():
            # Try to find claude in PATH
            import shutil
            claude_path = shutil.which("claude")
            if claude_path:
                return Path(claude_path)
            raise ValueError(f"Claude Code not found at {v}. Please install Claude Code or update CLAUDE_CODE_PATH.")
        return v

    @validator("workspace_dir", "claude_session_dir", "log_file")
    def create_directories(cls, v: Path) -> Path:
        """Create directories if they don't exist."""
        if v.suffix:  # It's a file
            v.parent.mkdir(parents=True, exist_ok=True)
        else:  # It's a directory
            v.mkdir(parents=True, exist_ok=True)
        return v

    @validator("allowed_file_extensions")
    def parse_file_extensions(cls, v: Any) -> List[str]:
        """Parse file extensions from string or list."""
        if isinstance(v, str):
            return [ext.strip() for ext in v.split(",")]
        return v

    @validator("blocked_commands")
    def parse_blocked_commands(cls, v: Any) -> List[str]:
        """Parse blocked commands from string or list."""
        if isinstance(v, str):
            return [cmd.strip() for cmd in v.split(",")]
        return v

    @property
    def is_production(self) -> bool:
        """Check if running in production mode."""
        return self.app_env == Environment.PRODUCTION

    @property
    def is_development(self) -> bool:
        """Check if running in development mode."""
        return self.app_env == Environment.DEVELOPMENT

    def get_agent_config(self, agent_type: str) -> Dict[str, Any]:
        """Get configuration for a specific agent type."""
        if agent_type == "claude_code":
            return {
                "path": str(self.claude_code_path),
                "timeout": self.claude_code_timeout,
                "max_turns": self.claude_code_max_turns,
                "output_format": self.claude_code_output_format,
            }
        elif agent_type == "openai":
            return {
                "api_key": self.openai_api_key.get_secret_value(),
                "model": self.openai_model,
                "max_tokens": self.openai_max_tokens,
                "temperature": self.openai_temperature,
                "timeout": self.openai_timeout,
            }
        elif agent_type == "gemini":
            return {
                "api_key": self.google_api_key.get_secret_value(),
                "model": self.gemini_model,
                "max_output_tokens": self.gemini_max_output_tokens,
                "temperature": self.gemini_temperature,
                "timeout": self.gemini_timeout,
            }
        else:
            raise ValueError(f"Unknown agent type: {agent_type}")

    class Config:
        """Pydantic configuration."""
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

        # Allow extra fields for forward compatibility
        extra = "allow"


# Create global settings instance
settings = Settings()
</file>

<file path="src/agents/claude_agent.py">
"""
Claude Code Agent implementation for AngelaMCP.

This agent wraps Claude Code functionality for file operations and code execution.
I'm implementing this as the "senior developer" agent with file system access.
"""

import asyncio
import json
import subprocess
import tempfile
import time
from pathlib import Path
from typing import Dict, List, Optional, Any

from src.agents.base import BaseAgent, AgentType, AgentResponse, TaskContext, TaskType
from src.utils.logger import get_logger
from src.utils.exceptions import AgentError
from config.settings import settings


class ClaudeCodeAgent(BaseAgent):
    """
    Claude Code agent with file system and execution capabilities.
    
    This is the primary agent that can:
    - Create and modify files
    - Execute code
    - Manage project structure
    - Act as senior developer with final authority
    """
    
    def __init__(self):
        super().__init__(
            agent_type=AgentType.CLAUDE,
            name="Claude Code Senior Developer",
            capabilities=[
                "file_operations",
                "code_execution", 
                "project_management",
                "senior_review",
                "final_authority",
                "debugging",
                "testing"
            ]
        )
        
        self.claude_path = settings.claude_code_path
        self.timeout = settings.claude_code_timeout
        self.max_turns = settings.claude_code_max_turns
        
        # Verify Claude Code is available
        self._verify_claude_installation()
        
        logger.info(f"Initialized Claude Code agent at: {self.claude_path}")
    
    def _verify_claude_installation(self) -> None:
        """Verify Claude Code is installed and accessible."""
        try:
            result = subprocess.run(
                [str(self.claude_path), "--version"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode != 0:
                raise AgentError(f"Claude Code not working: {result.stderr}")
                
            self.logger.info(f"Claude Code verified: {result.stdout.strip()}")
            
        except FileNotFoundError:
            raise AgentError(f"Claude Code not found at: {self.claude_path}")
        except subprocess.TimeoutExpired:
            raise AgentError("Claude Code verification timed out")
    
    async def generate(self, prompt: str, context: TaskContext) -> AgentResponse:
        """Generate response using Claude Code."""
        start_time = time.time()
        
        try:
            # Build Claude Code command
            cmd = self._build_claude_command(prompt, context)
            
            # Execute Claude Code
            result = await self._execute_claude_code(cmd)
            
            # Parse response
            response_content = self._parse_claude_response(result)
            
            execution_time = time.time() - start_time
            
            return AgentResponse(
                agent_type=self.agent_type,
                content=response_content,
                confidence=0.9,  # High confidence for Claude Code
                execution_time_ms=execution_time * 1000,
                metadata={
                    "claude_version": self._get_claude_version(),
                    "command_used": " ".join(cmd),
                    "context": context.task_type.value
                }
            )
            
        except Exception as e:
            self.logger.error(f"Claude Code generation failed: {e}", exc_info=True)
            raise AgentError(f"Claude Code error: {e}")
    
    async def critique(self, content: str, original_task: str, context: TaskContext) -> AgentResponse:
        """Provide senior developer critique using Claude Code."""
        critique_prompt = f"""As a senior developer, please provide a comprehensive code review of the following solution for: "{original_task}"

Solution to review:
{content}

Please provide:

**Technical Review:**
- Code quality assessment
- Architecture and design patterns
- Performance considerations
- Security implications

**Implementation Quality:**
- Best practices adherence
- Error handling
- Testing considerations
- Documentation quality

**Production Readiness:**
- Scalability concerns
- Maintenance considerations
- Deployment implications
- Monitoring and observability

**Specific Improvements:**
- Concrete code improvements
- Refactoring suggestions
- Alternative implementations
- Optimization opportunities

**Senior Developer Decision:**
- Overall assessment (Approve/Request Changes/Reject)
- Priority level of recommended changes
- Risk assessment for production deployment

Provide actionable, specific feedback that helps improve the solution."""

        critique_context = context.model_copy()
        critique_context.task_type = TaskType.CODE_REVIEW
        critique_context.agent_role = "senior_reviewer"
        
        return await self.generate(critique_prompt, critique_context)
    
    async def propose_solution(self, task_description: str, constraints: List[str], context: TaskContext) -> AgentResponse:
        """Propose solution as senior developer using Claude Code."""
        constraints_text = "\n".join(f"- {constraint}" for constraint in constraints) if constraints else "None specified"
        
        solution_prompt = f"""As a senior developer, design and implement a solution for:

**Task:** {task_description}

**Constraints:**
{constraints_text}

Please provide a complete solution including:

**1. Architecture Design:**
- High-level system design
- Component breakdown
- Technology stack recommendations
- Design patterns to use

**2. Implementation Plan:**
- File structure and organization
- Key classes and functions
- Data models and interfaces
- Integration points

**3. Code Implementation:**
- Create actual working code files
- Include proper error handling
- Add comprehensive documentation
- Implement testing framework

**4. Deployment Considerations:**
- Environment setup
- Dependencies and requirements
- Configuration management
- Monitoring and logging

**5. Testing Strategy:**
- Unit tests
- Integration tests
- Performance tests
- Security testing

**6. Documentation:**
- README with setup instructions
- API documentation
- Developer guidelines
- User documentation

Please create actual files and provide a working implementation, not just pseudocode."""

        solution_context = context.model_copy()
        solution_context.task_type = TaskType.CODE_GENERATION
        solution_context.agent_role = "senior_developer"
        
        return await self.generate(solution_prompt, solution_context)
    
    async def execute_code(self, code: str, language: str, context: TaskContext) -> AgentResponse:
        """Execute code using Claude Code capabilities."""
        execution_prompt = f"""Please execute the following {language} code and provide the results:

```{language}
{code}
```

If there are any errors, please:
1. Identify the issue
2. Provide a corrected version
3. Explain what was wrong
4. Execute the corrected version

Please show both the execution output and any debugging information."""

        execution_context = context.model_copy()
        execution_context.task_type = TaskType.CODE_GENERATION
        execution_context.metadata["execution_request"] = True
        
        return await self.generate(execution_prompt, execution_context)
    
    async def create_project(self, project_description: str, context: TaskContext) -> AgentResponse:
        """Create a complete project using Claude Code."""
        project_prompt = f"""Create a complete, production-ready project for:

{project_description}

Please:
1. Create proper project structure with folders
2. Implement all necessary files
3. Include comprehensive documentation
4. Add testing framework
5. Create deployment configuration
6. Include CI/CD setup if applicable

Make this a complete, working project that someone could clone and run immediately."""

        project_context = context.model_copy()
        project_context.task_type = TaskType.CODE_GENERATION
        project_context.metadata["project_creation"] = True
        
        return await self.generate(project_prompt, project_context)
    
    async def debug_issue(self, code: str, error: str, context: TaskContext) -> AgentResponse:
        """Debug issues using Claude Code expertise."""
        debug_prompt = f"""Please help debug the following issue:

**Code:**
```
{code}
```

**Error:**
{error}

As a senior developer, please:
1. Identify the root cause
2. Explain why this error occurred
3. Provide a corrected version
4. Suggest how to prevent similar issues
5. Add appropriate error handling
6. Include relevant tests to catch this type of error

Focus on both fixing the immediate issue and improving overall code robustness."""

        debug_context = context.model_copy()
        debug_context.task_type = TaskType.CODE_REVIEW
        debug_context.metadata["debugging"] = True
        
        return await self.generate(debug_prompt, debug_context)
    
    def _build_claude_command(self, prompt: str, context: TaskContext) -> List[str]:
        """Build Claude Code command with appropriate options."""
        cmd = [str(self.claude_path)]
        
        # Add context-specific options
        if context.task_type == TaskType.CODE_GENERATION:
            # For code generation, we want file operations
            pass
        elif context.task_type == TaskType.CODE_REVIEW:
            # For reviews, focus on analysis
            pass
        
        # Add the prompt as the final argument
        cmd.append(prompt)
        
        return cmd
    
    async def _execute_claude_code(self, cmd: List[str]) -> subprocess.CompletedProcess:
        """Execute Claude Code command asynchronously."""
        try:
            # Create a temporary file for complex prompts if needed
            if len(cmd[-1]) > 8000:  # Command line length limit
                with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                    f.write(cmd[-1])
                    temp_file = f.name
                
                # Replace prompt with file input
                cmd = cmd[:-1] + ['--file', temp_file]
            
            # Execute with timeout
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=Path.cwd()
            )
            
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(), 
                    timeout=self.timeout
                )
            except asyncio.TimeoutError:
                process.kill()
                await process.wait()
                raise AgentError(f"Claude Code timed out after {self.timeout}s")
            
            return subprocess.CompletedProcess(
                args=cmd,
                returncode=process.returncode,
                stdout=stdout.decode('utf-8'),
                stderr=stderr.decode('utf-8')
            )
            
        except Exception as e:
            raise AgentError(f"Failed to execute Claude Code: {e}")
    
    def _parse_claude_response(self, result: subprocess.CompletedProcess) -> str:
        """Parse Claude Code response and extract content."""
        if result.returncode != 0:
            error_msg = result.stderr or "Unknown error"
            raise AgentError(f"Claude Code failed: {error_msg}")
        
        response = result.stdout.strip()
        
        if not response:
            raise AgentError("Claude Code returned empty response")
        
        return response
    
    def _get_claude_version(self) -> str:
        """Get Claude Code version information."""
        try:
            result = subprocess.run(
                [str(self.claude_path), "--version"],
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.stdout.strip() if result.returncode == 0 else "unknown"
        except Exception:
            return "unknown"
    
    async def health_check(self) -> Dict[str, Any]:
        """Check Claude Code health and connectivity."""
        try:
            start_time = time.time()
            
            # Test basic functionality
            test_prompt = "Please respond with 'Claude Code is working' and nothing else."
            cmd = [str(self.claude_path), test_prompt]
            
            result = await self._execute_claude_code(cmd)
            response_time = time.time() - start_time
            
            # Check if response is reasonable
            if result.returncode == 0 and "working" in result.stdout.lower():
                status = "healthy"
            else:
                status = "degraded"
            
            return {
                "status": status,
                "claude_path": str(self.claude_path),
                "response_time": response_time,
                "version": self._get_claude_version(),
                "capabilities": self.capabilities,
                "last_check": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"Claude Code health check failed: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "claude_path": str(self.claude_path),
                "last_check": time.time()
            }
</file>

<file path="src/orchestrator/debate.py">
"""
Debate Protocol for AngelaMCP.

This module implements structured debates between AI agents where they can
propose solutions, critique each other's work, and reach consensus through
voting. I'm implementing a simple but effective debate flow focusing on
the core collaborative experience.
"""

import asyncio
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Set
from datetime import datetime

from src.agents.base import BaseAgent, AgentResponse, TaskContext, TaskType, AgentType
from src.utils.logger import get_logger, AsyncPerformanceLogger

logger = get_logger("orchestrator.debate")


class DebatePhase(str, Enum):
    """Phases of the debate process."""
    INITIALIZATION = "initialization"
    PROPOSALS = "proposals"
    CRITIQUE = "critique"
    REBUTTAL = "rebuttal"
    FINAL_PROPOSALS = "final_proposals"
    VOTING = "voting"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class AgentProposal:
    """A proposal from an agent."""
    agent_type: str
    agent_name: str
    content: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    confidence_score: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentCritique:
    """A critique of another agent's proposal."""
    critic_agent: str
    target_proposal: str  # agent_type of the proposal being critiqued
    content: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    severity: str = "moderate"  # low, moderate, high
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DebateRound:
    """Information about a single debate round."""
    round_number: int
    phase: DebatePhase
    started_at: datetime
    completed_at: Optional[datetime] = None
    proposals: List[AgentProposal] = field(default_factory=list)
    critiques: List[AgentCritique] = field(default_factory=list)
    phase_duration: Optional[float] = None


@dataclass
class DebateResult:
    """Final result of a debate."""
    debate_id: str
    topic: str
    success: bool
    winner: Optional[str] = None
    winning_proposal: Optional[AgentProposal] = None
    total_duration: float = 0.0
    rounds: List[DebateRound] = field(default_factory=list)
    participating_agents: List[str] = field(default_factory=list)
    consensus_reached: bool = False
    vote_breakdown: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class DebateProtocol:
    """
    Manages structured debates between AI agents.
    
    I'm implementing a collaborative debate system where agents propose solutions,
    critique each other's work, provide rebuttals, and reach consensus through voting.
    The focus is on creating engaging, productive multi-agent interactions.
    """
    
    def __init__(self, timeout_per_phase: int = 120, max_rounds: int = 3):
        """
        Initialize the debate protocol.
        
        Args:
            timeout_per_phase: Maximum time per debate phase in seconds
            max_rounds: Maximum number of debate rounds
        """
        self.timeout_per_phase = timeout_per_phase
        self.max_rounds = max_rounds
        self.logger = get_logger("debate")
        
        # Track active debates
        self._active_debates: Dict[str, Dict[str, Any]] = {}
    
    async def conduct_debate(
        self,
        topic: str,
        agents: List[BaseAgent],
        context: TaskContext,
        require_all_agents: bool = False
    ) -> DebateResult:
        """
        Conduct a structured debate between agents.
        
        Args:
            topic: The topic/task for agents to debate
            agents: List of participating agents
            context: Task context for the debate
            require_all_agents: Whether all agents must participate successfully
            
        Returns:
            DebateResult with complete debate transcript and outcome
        """
        debate_id = str(uuid.uuid4())
        start_time = time.time()
        
        # Initialize debate tracking
        debate_info = {
            "debate_id": debate_id,
            "topic": topic,
            "agents": {agent.name: agent for agent in agents},
            "start_time": start_time,
            "current_phase": DebatePhase.INITIALIZATION
        }
        self._active_debates[debate_id] = debate_info
        
        self.logger.info(f"üé™ Starting debate {debate_id[:8]} on: {topic[:60]}...")
        
        try:
            async with AsyncPerformanceLogger(self.logger, "debate_full", task_id=debate_id):
                # Phase 1: Initial Proposals
                self.logger.info(f"[{debate_id[:8]}] üí° Phase 1: Getting proposals from all agents")
                proposals = await self._phase_initial_proposals(topic, agents, context, debate_id)
                
                if not proposals and require_all_agents:
                    raise DebateError("Failed to get initial proposals from all agents")
                
                if not proposals:
                    self.logger.warning(f"[{debate_id[:8]}] No proposals received - ending debate")
                    return DebateResult(
                        debate_id=debate_id,
                        topic=topic,
                        success=False,
                        total_duration=time.time() - start_time,
                        participating_agents=[agent.name for agent in agents],
                        error_message="No proposals received from any agent"
                    )
                
                # Phase 2: Critique Round
                self.logger.info(f"[{debate_id[:8]}] üîç Phase 2: Agents critiquing each other's work")
                critiques = await self._phase_critique_round(proposals, agents, context, debate_id)
                
                # Phase 3: Final Proposals (agents refine based on feedback)
                self.logger.info(f"[{debate_id[:8]}] ‚ú® Phase 3: Refined proposals based on feedback")
                final_proposals = await self._phase_final_proposals(
                    topic, proposals, critiques, agents, context, debate_id
                )
                
                # Calculate total duration
                total_duration = time.time() - start_time
                
                # Create debate result with all the data needed for voting
                result = DebateResult(
                    debate_id=debate_id,
                    topic=topic,
                    success=len(final_proposals) > 0,
                    total_duration=total_duration,
                    participating_agents=[agent.name for agent in agents],
                    metadata={
                        "initial_proposals": len(proposals),
                        "critiques_generated": len(critiques),
                        "final_proposals": len(final_proposals),
                        "context": context.model_dump() if hasattr(context, 'model_dump') else {}
                    }
                )
                
                # Store proposals for voting
                result.rounds = [
                    DebateRound(
                        round_number=1,
                        phase=DebatePhase.PROPOSALS,
                        started_at=datetime.utcnow(),
                        proposals=[
                            AgentProposal(
                                agent_type=agent.agent_type.value,
                                agent_name=agent.name,
                                content=prop_content,
                                confidence_score=getattr(prop_response, 'confidence_score', None),
                                metadata=prop_response.metadata if hasattr(prop_response, 'metadata') else {}
                            )
                            for agent, prop_response, prop_content in final_proposals
                        ]
                    )
                ]
                
                self.logger.info(f"üéâ Debate {debate_id[:8]} completed in {total_duration:.1f}s with {len(final_proposals)} proposals")
                return result
                
        except Exception as e:
            total_duration = time.time() - start_time
            self.logger.error(f"‚ùå Debate {debate_id[:8]} failed: {e}")
            
            return DebateResult(
                debate_id=debate_id,
                topic=topic,
                success=False,
                total_duration=total_duration,
                participating_agents=[agent.name for agent in agents],
                error_message=str(e),
                metadata={"error_type": type(e).__name__}
            )
        
        finally:
            # Cleanup
            if debate_id in self._active_debates:
                del self._active_debates[debate_id]
    
    async def _phase_initial_proposals(
        self,
        topic: str,
        agents: List[BaseAgent],
        context: TaskContext,
        debate_id: str
    ) -> List[tuple]:
        """Phase 1: Each agent proposes their solution."""
        
        proposals = []
        
        # Get proposals from all agents in parallel
        async def get_agent_proposal(agent: BaseAgent) -> Optional[tuple]:
            try:
                self.logger.info(f"[{debate_id[:8]}] ü§ñ Getting proposal from {agent.name}...")
                
                async with AsyncPerformanceLogger(
                    self.logger, f"proposal_{agent.name}", task_id=debate_id
                ):
                    response = await asyncio.wait_for(
                        agent.propose_solution(topic, [], context),
                        timeout=self.timeout_per_phase
                    )
                    
                    if response.success:
                        self.logger.info(f"[{debate_id[:8]}] ‚úÖ {agent.name} proposal received ({len(response.content)} chars)")
                        return (agent, response, response.content)
                    else:
                        self.logger.warning(f"[{debate_id[:8]}] ‚ùå {agent.name} proposal failed: {response.error_message}")
                        return None
                        
            except asyncio.TimeoutError:
                self.logger.warning(f"[{debate_id[:8]}] ‚è±Ô∏è {agent.name} proposal timed out after {self.timeout_per_phase}s")
                return None
            except Exception as e:
                self.logger.error(f"[{debate_id[:8]}] üí• {agent.name} proposal error: {e}")
                return None
        
        # Execute all proposals concurrently
        proposal_tasks = [get_agent_proposal(agent) for agent in agents]
        proposal_results = await asyncio.gather(*proposal_tasks, return_exceptions=True)
        
        # Filter out failed proposals
        for result in proposal_results:
            if result and not isinstance(result, Exception):
                proposals.append(result)
        
        self.logger.info(f"[{debate_id[:8]}] üìä Collected {len(proposals)}/{len(agents)} proposals")
        return proposals
    
    async def _phase_critique_round(
        self,
        proposals: List[tuple],
        agents: List[BaseAgent],
        context: TaskContext,
        debate_id: str
    ) -> List[AgentCritique]:
        """Phase 2: Each agent critiques other agents' proposals."""
        
        critiques = []
        
        # Create critique tasks for each agent to review each other agent's proposals
        async def generate_critique(critic_agent: BaseAgent, target_proposal: tuple) -> Optional[AgentCritique]:
            target_agent, target_response, target_content = target_proposal
            
            # Don't critique your own proposal
            if critic_agent.name == target_agent.name:
                return None
            
            try:
                self.logger.info(f"[{debate_id[:8]}] üîç {critic_agent.name} reviewing {target_agent.name}'s work...")
                
                async with AsyncPerformanceLogger(
                    self.logger, f"critique_{critic_agent.name}_vs_{target_agent.name}", task_id=debate_id
                ):
                    critique_response = await asyncio.wait_for(
                        critic_agent.critique(target_content, f"Solution from {target_agent.name}", context),
                        timeout=self.timeout_per_phase
                    )
                    
                    if critique_response.success:
                        self.logger.info(f"[{debate_id[:8]}] ‚úÖ {critic_agent.name} critique of {target_agent.name} complete")
                        return AgentCritique(
                            critic_agent=critic_agent.name,
                            target_proposal=target_agent.name,
                            content=critique_response.content,
                            metadata=critique_response.metadata if hasattr(critique_response, 'metadata') else {}
                        )
                    else:
                        self.logger.warning(
                            f"[{debate_id[:8]}] ‚ùå {critic_agent.name} critique of {target_agent.name} failed"
                        )
                        return None
                        
            except asyncio.TimeoutError:
                self.logger.warning(f"[{debate_id[:8]}] ‚è±Ô∏è {critic_agent.name} critique timed out")
                return None
            except Exception as e:
                self.logger.error(f"[{debate_id[:8]}] üí• Critique error: {e}")
                return None
        
        # Generate all critique tasks
        critique_tasks = []
        for critic_agent in agents:
            for target_proposal in proposals:
                critique_tasks.append(generate_critique(critic_agent, target_proposal))
        
        # Execute critiques concurrently
        critique_results = await asyncio.gather(*critique_tasks, return_exceptions=True)
        
        # Filter successful critiques
        for result in critique_results:
            if result and not isinstance(result, Exception):
                critiques.append(result)
        
        self.logger.info(f"[{debate_id[:8]}] üìù Generated {len(critiques)} critiques")
        return critiques
    
    async def _phase_final_proposals(
        self,
        topic: str,
        initial_proposals: List[tuple],
        critiques: List[AgentCritique],
        agents: List[BaseAgent],
        context: TaskContext,
        debate_id: str
    ) -> List[tuple]:
        """Phase 3: Agents refine their proposals based on critiques."""
        
        final_proposals = []
        
        async def generate_final_proposal(agent: BaseAgent) -> Optional[tuple]:
            try:
                # Find critiques of this agent's proposal
                agent_critiques = [
                    c for c in critiques 
                    if c.target_proposal == agent.name
                ]
                
                if agent_critiques:
                    self.logger.info(f"[{debate_id[:8]}] üîÑ {agent.name} refining based on {len(agent_critiques)} critiques...")
                    
                    # Create prompt incorporating feedback
                    critique_text = "\n\n".join([
                        f"**Feedback from {c.critic_agent}:**\n{c.content}"
                        for c in agent_critiques
                    ])
                    
                    refinement_prompt = f"""Based on the feedback from other AI agents, please refine your solution:

**Original Task:** {topic}

**Feedback Received:**
{critique_text}

Please provide an improved solution that:
- Addresses the valid concerns raised
- Improves any weak areas identified  
- Maintains your solution's core strengths
- Creates a more robust overall approach

Focus on creating the best possible solution incorporating this collaborative feedback."""
                    
                    # Get refined proposal
                    async with AsyncPerformanceLogger(
                        self.logger, f"final_proposal_{agent.name}", task_id=debate_id
                    ):
                        response = await asyncio.wait_for(
                            agent.generate(refinement_prompt, context),
                            timeout=self.timeout_per_phase
                        )
                else:
                    self.logger.info(f"[{debate_id[:8]}] üìã {agent.name} keeping original proposal (no critiques)")
                    
                    # No critiques received, return original proposal
                    original_proposal = next(
                        (prop for a, resp, prop in initial_proposals if a.name == agent.name),
                        None
                    )
                    
                    if original_proposal:
                        response = next(resp for a, resp, prop in initial_proposals if a.name == agent.name)
                    else:
                        # Fallback - generate new proposal
                        response = await agent.propose_solution(topic, [], context)
                
                if response.success:
                    self.logger.info(f"[{debate_id[:8]}] ‚úÖ {agent.name} final proposal ready")
                    return (agent, response, response.content)
                else:
                    self.logger.warning(f"[{debate_id[:8]}] ‚ùå {agent.name} final proposal failed")
                    return None
                    
            except asyncio.TimeoutError:
                self.logger.warning(f"[{debate_id[:8]}] ‚è±Ô∏è {agent.name} final proposal timed out")
                return None
            except Exception as e:
                self.logger.error(f"[{debate_id[:8]}] üí• Final proposal error for {agent.name}: {e}")
                return None
        
        # Generate final proposals concurrently
        final_tasks = [generate_final_proposal(agent) for agent in agents]
        final_results = await asyncio.gather(*final_tasks, return_exceptions=True)
        
        # Filter successful proposals
        for result in final_results:
            if result and not isinstance(result, Exception):
                final_proposals.append(result)
        
        self.logger.info(f"[{debate_id[:8]}] üèÅ Final phase complete: {len(final_proposals)} refined proposals")
        return final_proposals


class DebateError(Exception):
    """Exception raised during debate operations."""
    pass
</file>

<file path="src/orchestrator/task_analyzer.py">
"""
Task Analyzer module for AngelaMCP.
Analyzes incoming tasks and determines the best execution strategy.
"""

import logging
import re
from dataclasses import dataclass
from enum import Enum
from typing import Dict, Any, List, Optional, Set
import ast
import keyword

from src.models.database import AgentType


class TaskComplexity(Enum):
    """Task complexity levels."""
    TRIVIAL = "trivial"
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    CRITICAL = "critical"


class TaskCategory(Enum):
    """Task categories."""
    CODE_GENERATION = "code_generation"
    CODE_REVIEW = "code_review"
    CODE_DEBUG = "code_debug"
    DOCUMENTATION = "documentation"
    RESEARCH = "research"
    ANALYSIS = "analysis"
    PLANNING = "planning"
    TESTING = "testing"
    DEPLOYMENT = "deployment"
    GENERAL = "general"


@dataclass
class TaskAnalysis:
    """Result of task analysis."""
    category: TaskCategory
    complexity: TaskComplexity
    estimated_duration: int  # seconds
    requires_collaboration: bool
    primary_agent: AgentType
    supporting_agents: List[AgentType]
    priority: int  # 1-10, higher is more urgent
    metadata: Dict[str, Any]
    
    @property
    def all_agents(self) -> List[AgentType]:
        """Get all agents involved in the task."""
        agents = [self.primary_agent]
        agents.extend(self.supporting_agents)
        return list(set(agents))  # Remove duplicates


class TaskAnalyzer:
    """Analyzes tasks to determine optimal execution strategy."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Keywords for different task categories
        self.category_keywords = {
            TaskCategory.CODE_GENERATION: [
                "create", "write", "generate", "implement", "build", "develop",
                "code", "function", "class", "script", "program", "app", "api"
            ],
            TaskCategory.CODE_REVIEW: [
                "review", "check", "audit", "inspect", "analyze code", "examine",
                "validate", "verify code", "assess", "evaluate code"
            ],
            TaskCategory.CODE_DEBUG: [
                "debug", "fix", "error", "bug", "issue", "problem", "troubleshoot",
                "resolve", "repair", "correct", "exception", "crash"
            ],
            TaskCategory.DOCUMENTATION: [
                "document", "docs", "readme", "explain", "describe", "manual",
                "guide", "tutorial", "help", "instructions", "comments"
            ],
            TaskCategory.RESEARCH: [
                "research", "investigate", "study", "explore", "find", "search",
                "learn", "discover", "analyze", "compare", "evaluate"
            ],
            TaskCategory.ANALYSIS: [
                "analyze", "assess", "evaluate", "examine", "study", "review",
                "investigate", "measure", "benchmark", "profile"
            ],
            TaskCategory.PLANNING: [
                "plan", "design", "architecture", "strategy", "approach",
                "roadmap", "outline", "structure", "organize"
            ],
            TaskCategory.TESTING: [
                "test", "testing", "unittest", "integration test", "verify",
                "validate", "check", "qa", "quality assurance"
            ],
            TaskCategory.DEPLOYMENT: [
                "deploy", "deployment", "release", "publish", "ship",
                "production", "staging", "ci/cd", "pipeline"
            ]
        }
        
        # Complexity indicators
        self.complexity_indicators = {
            TaskComplexity.TRIVIAL: [
                "hello world", "print", "simple calculation", "basic",
                "trivial", "quick", "one line"
            ],
            TaskComplexity.SIMPLE: [
                "single function", "small script", "basic", "simple",
                "straightforward", "easy"
            ],
            TaskComplexity.MODERATE: [
                "multiple functions", "class", "module", "moderate",
                "several", "medium", "standard"
            ],
            TaskComplexity.COMPLEX: [
                "application", "system", "framework", "complex",
                "advanced", "sophisticated", "multiple files",
                "integration", "database"
            ],
            TaskComplexity.CRITICAL: [
                "critical", "urgent", "production", "security",
                "performance", "scalability", "enterprise",
                "large scale", "mission critical"
            ]
        }
        
        # Programming language detection
        self.programming_languages = {
            "python", "javascript", "typescript", "java", "c++", "c#",
            "go", "rust", "ruby", "php", "kotlin", "swift", "scala",
            "r", "matlab", "sql", "html", "css", "bash", "shell"
        }
        
        # Agent specializations
        self.agent_specializations = {
            AgentType.CLAUDE_CODE: [
                "code generation", "file operations", "debugging",
                "refactoring", "testing", "deployment"
            ],
            AgentType.OPENAI: [
                "code review", "analysis", "optimization", "security",
                "best practices", "documentation"
            ],
            AgentType.GEMINI: [
                "research", "documentation", "planning", "explanation",
                "learning", "comparison", "tutorial"
            ]
        }
    
    def analyze_task(self, task_description: str, context: Dict[str, Any] = None) -> TaskAnalysis:
        """Analyze a task and return execution strategy."""
        context = context or {}
        
        # Normalize the task description
        normalized_task = task_description.lower().strip()
        
        # Analyze different aspects
        category = self._analyze_category(normalized_task)
        complexity = self._analyze_complexity(normalized_task, context)
        duration = self._estimate_duration(category, complexity)
        priority = self._analyze_priority(normalized_task, context)
        
        # Determine agent assignment
        primary_agent, supporting_agents = self._determine_agents(
            category, complexity, normalized_task
        )
        
        # Check if collaboration is needed
        requires_collaboration = self._requires_collaboration(
            category, complexity, len(supporting_agents)
        )
        
        # Extract metadata
        metadata = self._extract_metadata(normalized_task, context)
        
        analysis = TaskAnalysis(
            category=category,
            complexity=complexity,
            estimated_duration=duration,
            requires_collaboration=requires_collaboration,
            primary_agent=primary_agent,
            supporting_agents=supporting_agents,
            priority=priority,
            metadata=metadata
        )
        
        self.logger.debug(f"Task analysis complete: {analysis}")
        return analysis
    
    def _analyze_category(self, task: str) -> TaskCategory:
        """Determine the task category."""
        scores = {}
        
        for category, keywords in self.category_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in task:
                    score += 1
                    
            if score > 0:
                scores[category] = score
        
        if not scores:
            return TaskCategory.GENERAL
            
        # Return category with highest score
        return max(scores, key=scores.get)
    
    def _analyze_complexity(self, task: str, context: Dict[str, Any]) -> TaskComplexity:
        """Determine task complexity."""
        # Check for explicit complexity indicators
        for complexity, indicators in self.complexity_indicators.items():
            for indicator in indicators:
                if indicator in task:
                    return complexity
        
        # Analyze task characteristics
        complexity_score = 0
        
        # Check length and detail
        if len(task) > 500:
            complexity_score += 2
        elif len(task) > 200:
            complexity_score += 1
            
        # Check for multiple requirements
        if any(word in task for word in ["and", "also", "additionally", "furthermore"]):
            complexity_score += 1
            
        # Check for technical terms
        technical_terms = [
            "database", "api", "authentication", "security", "performance",
            "scalability", "integration", "architecture", "framework"
        ]
        complexity_score += sum(1 for term in technical_terms if term in task)
        
        # Check for code complexity indicators
        if self._contains_code(task):
            complexity_score += 1
            
        # Convert score to complexity level
        if complexity_score >= 5:
            return TaskComplexity.CRITICAL
        elif complexity_score >= 3:
            return TaskComplexity.COMPLEX
        elif complexity_score >= 2:
            return TaskComplexity.MODERATE
        elif complexity_score >= 1:
            return TaskComplexity.SIMPLE
        else:
            return TaskComplexity.TRIVIAL
    
    def _estimate_duration(self, category: TaskCategory, complexity: TaskComplexity) -> int:
        """Estimate task duration in seconds."""
        base_durations = {
            TaskComplexity.TRIVIAL: 30,
            TaskComplexity.SIMPLE: 120,
            TaskComplexity.MODERATE: 300,
            TaskComplexity.COMPLEX: 900,
            TaskComplexity.CRITICAL: 1800
        }
        
        category_multipliers = {
            TaskCategory.CODE_GENERATION: 1.5,
            TaskCategory.CODE_REVIEW: 1.2,
            TaskCategory.CODE_DEBUG: 2.0,
            TaskCategory.DOCUMENTATION: 1.0,
            TaskCategory.RESEARCH: 1.3,
            TaskCategory.ANALYSIS: 1.4,
            TaskCategory.PLANNING: 1.1,
            TaskCategory.TESTING: 1.6,
            TaskCategory.DEPLOYMENT: 1.8,
            TaskCategory.GENERAL: 1.0
        }
        
        base_duration = base_durations[complexity]
        multiplier = category_multipliers.get(category, 1.0)
        
        return int(base_duration * multiplier)
    
    def _analyze_priority(self, task: str, context: Dict[str, Any]) -> int:
        """Analyze task priority (1-10)."""
        priority = 5  # Default medium priority
        
        # High priority indicators
        high_priority_words = [
            "urgent", "critical", "emergency", "asap", "immediately",
            "production", "bug", "error", "broken", "down"
        ]
        
        # Low priority indicators  
        low_priority_words = [
            "eventually", "sometime", "nice to have", "optional",
            "future", "later", "enhancement", "improvement"
        ]
        
        for word in high_priority_words:
            if word in task:
                priority += 2
                
        for word in low_priority_words:
            if word in task:
                priority -= 2
                
        # Check context for priority hints
        if context.get("deadline"):
            priority += 1
        if context.get("user_priority"):
            priority = max(priority, context["user_priority"])
            
        return max(1, min(10, priority))
    
    def _determine_agents(self, category: TaskCategory, complexity: TaskComplexity, task: str) -> tuple[AgentType, List[AgentType]]:
        """Determine primary and supporting agents."""
        # Default assignment based on category
        agent_preferences = {
            TaskCategory.CODE_GENERATION: AgentType.CLAUDE_CODE,
            TaskCategory.CODE_DEBUG: AgentType.CLAUDE_CODE,
            TaskCategory.CODE_REVIEW: AgentType.OPENAI,
            TaskCategory.DOCUMENTATION: AgentType.GEMINI,
            TaskCategory.RESEARCH: AgentType.GEMINI,
            TaskCategory.ANALYSIS: AgentType.OPENAI,
            TaskCategory.PLANNING: AgentType.GEMINI,
            TaskCategory.TESTING: AgentType.CLAUDE_CODE,
            TaskCategory.DEPLOYMENT: AgentType.CLAUDE_CODE,
            TaskCategory.GENERAL: AgentType.CLAUDE_CODE
        }
        
        primary_agent = agent_preferences.get(category, AgentType.CLAUDE_CODE)
        supporting_agents = []
        
        # Add supporting agents based on complexity and category
        if complexity in [TaskComplexity.COMPLEX, TaskComplexity.CRITICAL]:
            all_agents = list(AgentType)
            supporting_agents = [agent for agent in all_agents if agent != primary_agent]
        elif complexity == TaskComplexity.MODERATE:
            if category in [TaskCategory.CODE_GENERATION, TaskCategory.CODE_DEBUG]:
                supporting_agents = [AgentType.OPENAI]  # For code review
            elif category == TaskCategory.RESEARCH:
                supporting_agents = [AgentType.OPENAI]  # For analysis
        
        return primary_agent, supporting_agents
    
    def _requires_collaboration(self, category: TaskCategory, complexity: TaskComplexity, num_supporting: int) -> bool:
        """Determine if task requires collaboration."""
        # Always collaborate for critical tasks
        if complexity == TaskComplexity.CRITICAL:
            return True
            
        # Collaborate for complex tasks with multiple aspects
        if complexity == TaskComplexity.COMPLEX and num_supporting > 0:
            return True
            
        # Specific categories that benefit from collaboration
        collaborative_categories = [
            TaskCategory.CODE_REVIEW,
            TaskCategory.ANALYSIS,
            TaskCategory.PLANNING
        ]
        
        if category in collaborative_categories and complexity != TaskComplexity.TRIVIAL:
            return True
            
        return False
    
    def _extract_metadata(self, task: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Extract metadata from task description."""
        metadata = {}
        
        # Detect programming languages
        detected_languages = [lang for lang in self.programming_languages if lang in task]
        if detected_languages:
            metadata["languages"] = detected_languages
            
        # Detect file operations
        file_operations = ["create file", "edit file", "delete file", "read file"]
        detected_operations = [op for op in file_operations if op in task]
        if detected_operations:
            metadata["file_operations"] = detected_operations
            
        # Extract code snippets
        if self._contains_code(task):
            metadata["contains_code"] = True
            
        # Add context metadata
        metadata.update(context)
        
        return metadata
    
    def _contains_code(self, text: str) -> bool:
        """Check if text contains code snippets."""
        code_indicators = [
            "```", "def ", "class ", "function", "import ", "from ",
            "if __name__", "return ", "print(", "console.log"
        ]
        
        return any(indicator in text for indicator in code_indicators)
    
    def get_task_insights(self, task_description: str) -> Dict[str, Any]:
        """Get detailed insights about a task."""
        analysis = self.analyze_task(task_description)
        
        return {
            "analysis": analysis,
            "recommendations": self._get_recommendations(analysis),
            "risks": self._identify_risks(analysis),
            "success_factors": self._identify_success_factors(analysis)
        }
    
    def _get_recommendations(self, analysis: TaskAnalysis) -> List[str]:
        """Get recommendations for task execution."""
        recommendations = []
        
        if analysis.complexity == TaskComplexity.CRITICAL:
            recommendations.append("Consider breaking down into smaller subtasks")
            recommendations.append("Implement thorough testing and validation")
            
        if analysis.requires_collaboration:
            recommendations.append("Schedule collaboration session between agents")
            recommendations.append("Define clear roles and responsibilities")
            
        if analysis.category == TaskCategory.CODE_GENERATION:
            recommendations.append("Review code quality and security")
            recommendations.append("Add comprehensive documentation")
            
        return recommendations
    
    def _identify_risks(self, analysis: TaskAnalysis) -> List[str]:
        """Identify potential risks in task execution."""
        risks = []
        
        if analysis.complexity == TaskComplexity.CRITICAL:
            risks.append("High complexity may lead to longer execution times")
            risks.append("Increased chance of errors or incomplete implementation")
            
        if not analysis.requires_collaboration and analysis.complexity == TaskComplexity.COMPLEX:
            risks.append("Single agent may miss important considerations")
            
        if analysis.estimated_duration > 1200:  # 20 minutes
            risks.append("Long execution time may impact user experience")
            
        return risks
    
    def _identify_success_factors(self, analysis: TaskAnalysis) -> List[str]:
        """Identify factors that contribute to task success."""
        factors = []
        
        if analysis.requires_collaboration:
            factors.append("Multi-agent collaboration provides diverse perspectives")
            
        if analysis.primary_agent == AgentType.CLAUDE_CODE:
            factors.append("Claude Code provides excellent implementation capabilities")
            
        if analysis.category in [TaskCategory.DOCUMENTATION, TaskCategory.RESEARCH]:
            factors.append("Task aligns well with AI agent strengths")
            
        return factors
</file>

<file path="src/orchestrator/task_queue.py">
"""
Task Queue module for AngelaMCP.
Advanced task queuing system with priority, scheduling, and retry capabilities.
"""

import asyncio
import heapq
import logging
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Set
from uuid import uuid4

from src.models.database import TaskStatus
from src.orchestrator.task_analyzer import TaskAnalysis, TaskComplexity


class TaskPriority(Enum):
    """Task priority levels."""
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4
    BACKGROUND = 5


class QueueType(Enum):
    """Queue types for different task categories."""
    IMMEDIATE = "immediate"
    SCHEDULED = "scheduled"
    RETRY = "retry"
    BATCH = "batch"


@dataclass
class QueuedTask:
    """Represents a task in the queue."""
    task_id: str
    priority: TaskPriority
    queue_type: QueueType
    scheduled_time: datetime
    task_data: Dict[str, Any]
    analysis: Optional[TaskAnalysis] = None
    attempts: int = 0
    max_attempts: int = 3
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_attempt: Optional[datetime] = None
    dependencies: Set[str] = field(default_factory=set)
    callback: Optional[Callable] = None
    timeout: Optional[int] = None
    
    def __lt__(self, other):
        """Comparison for priority queue (lower priority value = higher priority)."""
        if self.priority.value != other.priority.value:
            return self.priority.value < other.priority.value
        return self.scheduled_time < other.scheduled_time
    
    @property
    def is_ready(self) -> bool:
        """Check if task is ready for execution."""
        return (
            datetime.utcnow() >= self.scheduled_time and
            not self.dependencies and
            self.attempts < self.max_attempts
        )
    
    @property
    def is_expired(self) -> bool:
        """Check if task has expired."""
        if not self.timeout:
            return False
        return datetime.utcnow() > self.created_at + timedelta(seconds=self.timeout)


class TaskQueue:
    """Advanced task queue with multiple queue types and priority handling."""
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.logger = logging.getLogger(__name__)
        
        # Different queue types
        self.immediate_queue: List[QueuedTask] = []
        self.scheduled_queue: List[QueuedTask] = []
        self.retry_queue: List[QueuedTask] = []
        self.batch_queue: deque = deque()
        
        # Task tracking
        self.active_tasks: Dict[str, QueuedTask] = {}
        self.completed_tasks: deque = deque(maxlen=100)  # Keep last 100
        self.failed_tasks: deque = deque(maxlen=50)      # Keep last 50
        
        # Dependencies tracking
        self.dependency_graph: Dict[str, Set[str]] = defaultdict(set)
        self.dependents: Dict[str, Set[str]] = defaultdict(set)
        
        # Queue statistics
        self.stats = {
            "enqueued": 0,
            "dequeued": 0,
            "completed": 0,
            "failed": 0,
            "retried": 0
        }
        
        # Lock for thread safety
        self.lock = asyncio.Lock()
        
        # Background tasks
        self._cleanup_task: Optional[asyncio.Task] = None
        self._running = False
    
    async def start(self):
        """Start background queue management tasks."""
        if self._running:
            return
            
        self._running = True
        self._cleanup_task = asyncio.create_task(self._cleanup_expired_tasks())
        self.logger.info("Task queue started")
    
    async def stop(self):
        """Stop background tasks."""
        self._running = False
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        self.logger.info("Task queue stopped")
    
    async def enqueue(
        self,
        task_id: str,
        task_data: Dict[str, Any],
        priority: TaskPriority = TaskPriority.NORMAL,
        queue_type: QueueType = QueueType.IMMEDIATE,
        scheduled_time: Optional[datetime] = None,
        dependencies: Optional[Set[str]] = None,
        analysis: Optional[TaskAnalysis] = None,
        callback: Optional[Callable] = None,
        timeout: Optional[int] = None,
        max_attempts: int = 3
    ) -> bool:
        """Enqueue a task for execution."""
        async with self.lock:
            # Check queue capacity
            if self._get_total_queue_size() >= self.max_size:
                self.logger.warning(f"Queue at capacity ({self.max_size}), rejecting task {task_id}")
                return False
            
            # Create queued task
            queued_task = QueuedTask(
                task_id=task_id,
                priority=priority,
                queue_type=queue_type,
                scheduled_time=scheduled_time or datetime.utcnow(),
                task_data=task_data,
                analysis=analysis,
                dependencies=dependencies or set(),
                callback=callback,
                timeout=timeout,
                max_attempts=max_attempts
            )
            
            # Add to appropriate queue
            if queue_type == QueueType.IMMEDIATE:
                heapq.heappush(self.immediate_queue, queued_task)
            elif queue_type == QueueType.SCHEDULED:
                heapq.heappush(self.scheduled_queue, queued_task)
            elif queue_type == QueueType.RETRY:
                heapq.heappush(self.retry_queue, queued_task)
            elif queue_type == QueueType.BATCH:
                self.batch_queue.append(queued_task)
            
            # Update dependency tracking
            if dependencies:
                self.dependency_graph[task_id] = dependencies.copy()
                for dep in dependencies:
                    self.dependents[dep].add(task_id)
            
            self.stats["enqueued"] += 1
            self.logger.debug(f"Enqueued task {task_id} with priority {priority.name}")
            
            return True
    
    async def dequeue(self, agent_type: Optional[str] = None) -> Optional[QueuedTask]:
        """Dequeue the next ready task."""
        async with self.lock:
            # Check queues in priority order
            for queue_list in [self.immediate_queue, self.retry_queue, self.scheduled_queue]:
                if queue_list:
                    # Find the first ready task
                    ready_tasks = []
                    non_ready_tasks = []
                    
                    while queue_list:
                        task = heapq.heappop(queue_list)
                        if task.is_ready and not task.is_expired:
                            ready_tasks.append(task)
                        elif not task.is_expired:
                            non_ready_tasks.append(task)
                        else:
                            # Task expired, move to failed
                            self.failed_tasks.append(task)
                            self.stats["failed"] += 1
                    
                    # Put non-ready tasks back
                    for task in non_ready_tasks:
                        heapq.heappush(queue_list, task)
                    
                    # Return highest priority ready task
                    if ready_tasks:
                        task = min(ready_tasks)  # Lowest priority value = highest priority
                        # Put other ready tasks back
                        for other_task in ready_tasks:
                            if other_task != task:
                                heapq.heappush(queue_list, other_task)
                        
                        # Mark as active
                        self.active_tasks[task.task_id] = task
                        self.stats["dequeued"] += 1
                        
                        self.logger.debug(f"Dequeued task {task.task_id}")
                        return task
            
            # Check batch queue if no priority tasks
            if self.batch_queue:
                task = self.batch_queue.popleft()
                if task.is_ready and not task.is_expired:
                    self.active_tasks[task.task_id] = task
                    self.stats["dequeued"] += 1
                    return task
                elif not task.is_expired:
                    # Put back if not ready
                    self.batch_queue.appendleft(task)
                else:
                    # Task expired
                    self.failed_tasks.append(task)
                    self.stats["failed"] += 1
            
            return None
    
    async def mark_completed(self, task_id: str, result: Any = None):
        """Mark a task as completed."""
        async with self.lock:
            if task_id in self.active_tasks:
                task = self.active_tasks.pop(task_id)
                task.task_data["result"] = result
                self.completed_tasks.append(task)
                self.stats["completed"] += 1
                
                # Execute callback if provided
                if task.callback:
                    try:
                        await task.callback(task_id, result)
                    except Exception as e:
                        self.logger.error(f"Callback error for task {task_id}: {e}")
                
                # Resolve dependencies
                await self._resolve_dependencies(task_id)
                
                self.logger.debug(f"Task {task_id} completed")
    
    async def mark_failed(self, task_id: str, error: Exception, retry: bool = True):
        """Mark a task as failed and optionally retry."""
        async with self.lock:
            if task_id in self.active_tasks:
                task = self.active_tasks.pop(task_id)
                task.attempts += 1
                task.last_attempt = datetime.utcnow()
                task.task_data["last_error"] = str(error)
                
                if retry and task.attempts < task.max_attempts:
                    # Calculate retry delay with exponential backoff
                    delay = min(60, 2 ** task.attempts)  # Max 60 seconds
                    task.scheduled_time = datetime.utcnow() + timedelta(seconds=delay)
                    task.queue_type = QueueType.RETRY
                    
                    # Re-enqueue for retry
                    heapq.heappush(self.retry_queue, task)
                    self.stats["retried"] += 1
                    
                    self.logger.info(f"Task {task_id} failed, retrying in {delay}s (attempt {task.attempts}/{task.max_attempts})")
                else:
                    # Max attempts reached or no retry
                    self.failed_tasks.append(task)
                    self.stats["failed"] += 1
                    
                    # Fail dependent tasks
                    await self._fail_dependents(task_id)
                    
                    self.logger.error(f"Task {task_id} failed permanently after {task.attempts} attempts")
    
    async def cancel_task(self, task_id: str) -> bool:
        """Cancel a task."""
        async with self.lock:
            # Remove from active tasks
            if task_id in self.active_tasks:
                task = self.active_tasks.pop(task_id)
                self.failed_tasks.append(task)
                await self._fail_dependents(task_id)
                self.logger.info(f"Cancelled active task {task_id}")
                return True
            
            # Remove from queues
            for queue_list in [self.immediate_queue, self.scheduled_queue, self.retry_queue]:
                updated_queue = [t for t in queue_list if t.task_id != task_id]
                if len(updated_queue) != len(queue_list):
                    queue_list.clear()
                    for task in updated_queue:
                        heapq.heappush(queue_list, task)
                    self.logger.info(f"Cancelled queued task {task_id}")
                    return True
            
            # Check batch queue
            original_len = len(self.batch_queue)
            self.batch_queue = deque(t for t in self.batch_queue if t.task_id != task_id)
            if len(self.batch_queue) != original_len:
                self.logger.info(f"Cancelled batch task {task_id}")
                return True
            
            return False
    
    async def add_dependency(self, task_id: str, dependency_id: str):
        """Add a dependency for a task."""
        async with self.lock:
            self.dependency_graph[task_id].add(dependency_id)
            self.dependents[dependency_id].add(task_id)
            
            # Update task in queues to reflect new dependency
            await self._update_task_dependencies(task_id)
    
    async def remove_dependency(self, task_id: str, dependency_id: str):
        """Remove a dependency for a task."""
        async with self.lock:
            self.dependency_graph[task_id].discard(dependency_id)
            self.dependents[dependency_id].discard(task_id)
            
            # Update task in queues
            await self._update_task_dependencies(task_id)
    
    async def get_queue_stats(self) -> Dict[str, Any]:
        """Get comprehensive queue statistics."""
        async with self.lock:
            return {
                "queue_sizes": {
                    "immediate": len(self.immediate_queue),
                    "scheduled": len(self.scheduled_queue),
                    "retry": len(self.retry_queue),
                    "batch": len(self.batch_queue),
                    "active": len(self.active_tasks)
                },
                "totals": self.stats.copy(),
                "capacity": {
                    "max_size": self.max_size,
                    "current_size": self._get_total_queue_size(),
                    "utilization": self._get_total_queue_size() / self.max_size * 100
                },
                "recent_completed": len(self.completed_tasks),
                "recent_failed": len(self.failed_tasks)
            }
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get status of a specific task."""
        async with self.lock:
            # Check active tasks
            if task_id in self.active_tasks:
                task = self.active_tasks[task_id]
                return {
                    "status": "active",
                    "priority": task.priority.name,
                    "queue_type": task.queue_type.value,
                    "attempts": task.attempts,
                    "created_at": task.created_at.isoformat(),
                    "dependencies": list(task.dependencies)
                }
            
            # Check completed tasks
            for task in self.completed_tasks:
                if task.task_id == task_id:
                    return {
                        "status": "completed",
                        "priority": task.priority.name,
                        "attempts": task.attempts,
                        "created_at": task.created_at.isoformat(),
                        "result": task.task_data.get("result")
                    }
            
            # Check failed tasks
            for task in self.failed_tasks:
                if task.task_id == task_id:
                    return {
                        "status": "failed",
                        "priority": task.priority.name,
                        "attempts": task.attempts,
                        "created_at": task.created_at.isoformat(),
                        "last_error": task.task_data.get("last_error")
                    }
            
            # Check all queues
            for queue_name, queue_list in [
                ("immediate", self.immediate_queue),
                ("scheduled", self.scheduled_queue),
                ("retry", self.retry_queue)
            ]:
                for task in queue_list:
                    if task.task_id == task_id:
                        return {
                            "status": "queued",
                            "queue": queue_name,
                            "priority": task.priority.name,
                            "scheduled_time": task.scheduled_time.isoformat(),
                            "is_ready": task.is_ready,
                            "dependencies": list(task.dependencies)
                        }
            
            # Check batch queue
            for task in self.batch_queue:
                if task.task_id == task_id:
                    return {
                        "status": "queued",
                        "queue": "batch",
                        "priority": task.priority.name,
                        "is_ready": task.is_ready,
                        "dependencies": list(task.dependencies)
                    }
            
            return None
    
    async def _resolve_dependencies(self, completed_task_id: str):
        """Resolve dependencies when a task completes."""
        for dependent_id in self.dependents[completed_task_id]:
            self.dependency_graph[dependent_id].discard(completed_task_id)
            await self._update_task_dependencies(dependent_id)
        
        # Clean up dependency tracking
        del self.dependents[completed_task_id]
        if completed_task_id in self.dependency_graph:
            del self.dependency_graph[completed_task_id]
    
    async def _fail_dependents(self, failed_task_id: str):
        """Fail all tasks that depend on a failed task."""
        dependents_to_fail = list(self.dependents[failed_task_id])
        
        for dependent_id in dependents_to_fail:
            await self.cancel_task(dependent_id)
            self.logger.warning(f"Failed dependent task {dependent_id} due to failure of {failed_task_id}")
    
    async def _update_task_dependencies(self, task_id: str):
        """Update a task's dependencies in the queues."""
        # This is a simplified version - in a real implementation,
        # you'd need to find and update the task in the appropriate queue
        pass
    
    async def _cleanup_expired_tasks(self):
        """Background task to clean up expired tasks."""
        while self._running:
            try:
                async with self.lock:
                    current_time = datetime.utcnow()
                    
                    # Clean up all queues
                    for queue_list in [self.immediate_queue, self.scheduled_queue, self.retry_queue]:
                        valid_tasks = []
                        while queue_list:
                            task = heapq.heappop(queue_list)
                            if not task.is_expired:
                                valid_tasks.append(task)
                            else:
                                self.failed_tasks.append(task)
                                self.stats["failed"] += 1
                                self.logger.debug(f"Expired task {task.task_id}")
                        
                        # Rebuild heap
                        for task in valid_tasks:
                            heapq.heappush(queue_list, task)
                    
                    # Clean batch queue
                    valid_batch_tasks = []
                    while self.batch_queue:
                        task = self.batch_queue.popleft()
                        if not task.is_expired:
                            valid_batch_tasks.append(task)
                        else:
                            self.failed_tasks.append(task)
                            self.stats["failed"] += 1
                    
                    self.batch_queue.extend(valid_batch_tasks)
                
                await asyncio.sleep(60)  # Check every minute
                
            except Exception as e:
                self.logger.error(f"Error in cleanup task: {e}")
                await asyncio.sleep(60)
    
    def _get_total_queue_size(self) -> int:
        """Get total size across all queues."""
        return (
            len(self.immediate_queue) +
            len(self.scheduled_queue) +
            len(self.retry_queue) +
            len(self.batch_queue) +
            len(self.active_tasks)
        )


class PriorityTaskScheduler:
    """High-level scheduler that uses TaskQueue with intelligent priority assignment."""
    
    def __init__(self, task_queue: TaskQueue):
        self.task_queue = task_queue
        self.logger = logging.getLogger(__name__)
    
    async def schedule_task(
        self,
        task_id: str,
        task_data: Dict[str, Any],
        analysis: Optional[TaskAnalysis] = None,
        scheduled_time: Optional[datetime] = None,
        dependencies: Optional[Set[str]] = None
    ) -> bool:
        """Schedule a task with intelligent priority assignment."""
        
        # Determine priority based on analysis
        priority = self._calculate_priority(analysis, task_data)
        
        # Determine queue type
        queue_type = self._determine_queue_type(analysis, scheduled_time)
        
        # Calculate timeout
        timeout = self._calculate_timeout(analysis)
        
        # Calculate max attempts
        max_attempts = self._calculate_max_attempts(analysis)
        
        return await self.task_queue.enqueue(
            task_id=task_id,
            task_data=task_data,
            priority=priority,
            queue_type=queue_type,
            scheduled_time=scheduled_time,
            dependencies=dependencies,
            analysis=analysis,
            timeout=timeout,
            max_attempts=max_attempts
        )
    
    def _calculate_priority(self, analysis: Optional[TaskAnalysis], task_data: Dict[str, Any]) -> TaskPriority:
        """Calculate task priority based on analysis."""
        if not analysis:
            return TaskPriority.NORMAL
        
        # Critical complexity always gets high priority
        if analysis.complexity == TaskComplexity.CRITICAL:
            return TaskPriority.CRITICAL
        
        # Check for urgency indicators
        if analysis.priority >= 8:
            return TaskPriority.HIGH
        elif analysis.priority >= 6:
            return TaskPriority.NORMAL
        elif analysis.priority >= 3:
            return TaskPriority.LOW
        else:
            return TaskPriority.BACKGROUND
    
    def _determine_queue_type(self, analysis: Optional[TaskAnalysis], scheduled_time: Optional[datetime]) -> QueueType:
        """Determine appropriate queue type."""
        if scheduled_time and scheduled_time > datetime.utcnow():
            return QueueType.SCHEDULED
        
        if analysis and analysis.complexity in [TaskComplexity.TRIVIAL, TaskComplexity.SIMPLE]:
            return QueueType.BATCH
        
        return QueueType.IMMEDIATE
    
    def _calculate_timeout(self, analysis: Optional[TaskAnalysis]) -> Optional[int]:
        """Calculate task timeout based on analysis."""
        if not analysis:
            return 3600  # 1 hour default
        
        # Base timeout on estimated duration with buffer
        base_timeout = analysis.estimated_duration * 3  # 3x buffer
        
        # Minimum and maximum timeouts
        return max(300, min(7200, base_timeout))  # 5 min to 2 hours
    
    def _calculate_max_attempts(self, analysis: Optional[TaskAnalysis]) -> int:
        """Calculate maximum retry attempts."""
        if not analysis:
            return 3
        
        if analysis.complexity == TaskComplexity.CRITICAL:
            return 5  # More retries for critical tasks
        elif analysis.complexity == TaskComplexity.TRIVIAL:
            return 1  # Fewer retries for simple tasks
        else:
            return 3  # Standard retries
</file>

<file path="src/orchestrator/voting.py">
"""
Voting System for AngelaMCP.

This module implements a weighted voting system where AI agents vote on proposals
from debates. Claude Code has senior developer voting weight and veto power since
it's the agent with actual file system access and execution capabilities.
"""

import asyncio
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime

from src.agents.base import BaseAgent, AgentResponse, TaskContext, AgentType
from src.orchestrator.debate import DebateResult, AgentProposal
from src.utils.logger import get_logger, AsyncPerformanceLogger

logger = get_logger("orchestrator.voting")


class VoteChoice(str, Enum):
    """Possible vote choices."""
    APPROVE = "approve"
    REJECT = "reject"
    ABSTAIN = "abstain"


@dataclass
class AgentVote:
    """A vote from an agent."""
    agent_name: str
    agent_type: str
    proposal_target: str  # The agent whose proposal is being voted on
    choice: VoteChoice
    confidence: float = 0.5  # 0.0 to 1.0
    reasoning: str = ""
    timestamp: datetime = field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ProposalScore:
    """Scoring for a single proposal."""
    proposal: AgentProposal
    total_score: float = 0.0
    weighted_score: float = 0.0
    votes: List[AgentVote] = field(default_factory=list)
    approval_count: int = 0
    rejection_count: int = 0
    abstain_count: int = 0
    claude_approved: bool = False
    claude_vetoed: bool = False


@dataclass
class VotingResult:
    """Result of the voting process."""
    voting_id: str
    success: bool
    winner: Optional[str] = None
    winning_proposal: Optional[AgentProposal] = None
    proposal_scores: List[ProposalScore] = field(default_factory=list)
    total_duration: float = 0.0
    consensus_reached: bool = False
    claude_used_veto: bool = False
    voting_summary: str = ""
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class VotingSystem:
    """
    Manages weighted voting between AI agents with Claude Code as senior developer.
    
    Voting weights:
    - Claude Code: 2.0 (senior developer with file system access)
    - OpenAI: 1.0 (reviewer)
    - Gemini: 1.0 (researcher)
    
    Special rules:
    - Claude Code can veto any proposal (overrides all other votes)
    - Requires majority approval to win
    - In case of tie, Claude Code's preference wins
    """
    
    def __init__(self, claude_vote_weight: float = 2.0, enable_claude_veto: bool = True, voting_timeout: int = 120):
        """
        Initialize the voting system.
        
        Args:
            claude_vote_weight: Weight multiplier for Claude Code votes
            enable_claude_veto: Whether Claude Code can veto proposals
            voting_timeout: Timeout for voting phase in seconds
        """
        self.claude_vote_weight = claude_vote_weight
        self.enable_claude_veto = enable_claude_veto
        self.voting_timeout = voting_timeout
        self.logger = get_logger("voting")
        
        # Agent vote weights
        self.vote_weights = {
            AgentType.CLAUDE_CODE.value: claude_vote_weight,
            AgentType.OPENAI.value: 1.0,
            AgentType.GEMINI.value: 1.0
        }
    
    async def conduct_voting(
        self,
        debate_result: DebateResult,
        agents: List[BaseAgent],
        context: TaskContext
    ) -> VotingResult:
        """
        Conduct voting on proposals from a debate.
        
        Args:
            debate_result: Result from the debate phase
            agents: List of participating agents
            context: Task context for voting
            
        Returns:
            VotingResult with winner and vote breakdown
        """
        voting_id = str(uuid.uuid4())
        start_time = time.time()
        
        self.logger.info(f"üó≥Ô∏è Starting voting {voting_id[:8]} on {len(debate_result.rounds[0].proposals)} proposals")
        
        try:
            async with AsyncPerformanceLogger(self.logger, "voting_full", task_id=voting_id):
                # Get proposals from debate result
                if not debate_result.rounds or not debate_result.rounds[0].proposals:
                    return VotingResult(
                        voting_id=voting_id,
                        success=False,
                        total_duration=time.time() - start_time,
                        error_message="No proposals to vote on"
                    )
                
                proposals = debate_result.rounds[0].proposals
                
                # Collect votes from all agents for all proposals
                proposal_scores = await self._collect_votes(proposals, agents, context, voting_id)
                
                # Determine winner
                winner_result = self._determine_winner(proposal_scores, voting_id)
                
                total_duration = time.time() - start_time
                
                # Create voting summary
                summary = self._create_voting_summary(proposal_scores, winner_result)
                
                result = VotingResult(
                    voting_id=voting_id,
                    success=winner_result is not None,
                    winner=winner_result.proposal.agent_name if winner_result else None,
                    winning_proposal=winner_result.proposal if winner_result else None,
                    proposal_scores=proposal_scores,
                    total_duration=total_duration,
                    consensus_reached=self._check_consensus(proposal_scores),
                    claude_used_veto=any(score.claude_vetoed for score in proposal_scores),
                    voting_summary=summary,
                    metadata={
                        "total_proposals": len(proposals),
                        "claude_vote_weight": self.claude_vote_weight,
                        "veto_enabled": self.enable_claude_veto
                    }
                )
                
                self.logger.info(f"üèÜ Voting {voting_id[:8]} completed: Winner is {result.winner or 'None'}")
                return result
                
        except Exception as e:
            self.logger.error(f"‚ùå Voting {voting_id[:8]} failed: {e}")
            return VotingResult(
                voting_id=voting_id,
                success=False,
                total_duration=time.time() - start_time,
                error_message=str(e),
                metadata={"error_type": type(e).__name__}
            )
    
    async def _collect_votes(
        self,
        proposals: List[AgentProposal],
        agents: List[BaseAgent],
        context: TaskContext,
        voting_id: str
    ) -> List[ProposalScore]:
        """Collect votes from all agents for all proposals."""
        
        # Initialize proposal scores
        proposal_scores = [
            ProposalScore(proposal=proposal)
            for proposal in proposals
        ]
        
        # Collect votes for each proposal from each agent
        async def vote_on_proposal(voter_agent: BaseAgent, proposal: AgentProposal) -> List[AgentVote]:
            """Get a single agent's vote on a single proposal."""
            
            # Don't vote on your own proposal
            if voter_agent.name == proposal.agent_name:
                return []
            
            try:
                self.logger.info(f"[{voting_id[:8]}] üó≥Ô∏è {voter_agent.name} voting on {proposal.agent_name}'s proposal...")
                
                # Create voting prompt
                voting_prompt = f"""You are participating in a collaborative AI voting process. Please evaluate the following proposal and vote on it.

**Task Context:** {context.task_type.value if hasattr(context, 'task_type') else 'General Task'}

**Proposal from {proposal.agent_name}:**
{proposal.content}

Please provide your vote and reasoning in this exact format:

**VOTE:** [APPROVE/REJECT/ABSTAIN]
**CONFIDENCE:** [0.0-1.0]
**REASONING:** [Your detailed reasoning for this vote]

Voting guidelines:
- APPROVE: This proposal is technically sound, practical, and well-implemented
- REJECT: This proposal has significant issues that make it unsuitable
- ABSTAIN: You cannot adequately evaluate this proposal or it's outside your expertise

Consider:
- Technical correctness and feasibility
- Code quality and best practices (if applicable)
- Completeness and thoroughness
- Potential risks or issues
- Overall value and effectiveness

Be objective and constructive in your evaluation."""

                # Get vote response
                async with AsyncPerformanceLogger(
                    self.logger, f"vote_{voter_agent.name}_on_{proposal.agent_name}", task_id=voting_id
                ):
                    response = await asyncio.wait_for(
                        voter_agent.generate(voting_prompt, context),
                        timeout=self.voting_timeout
                    )
                
                if response.success:
                    # Parse vote from response
                    vote = self._parse_vote_response(
                        voter_agent, proposal.agent_name, response.content
                    )
                    if vote:
                        self.logger.info(f"[{voting_id[:8]}] ‚úÖ {voter_agent.name} voted {vote.choice.value} on {proposal.agent_name}")
                        return [vote]
                    else:
                        self.logger.warning(f"[{voting_id[:8]}] ‚ùì Could not parse vote from {voter_agent.name}")
                else:
                    self.logger.warning(f"[{voting_id[:8]}] ‚ùå {voter_agent.name} voting failed: {response.error_message}")
                
                return []
                
            except asyncio.TimeoutError:
                self.logger.warning(f"[{voting_id[:8]}] ‚è±Ô∏è {voter_agent.name} vote timed out")
                return []
            except Exception as e:
                self.logger.error(f"[{voting_id[:8]}] üí• Voting error for {voter_agent.name}: {e}")
                return []
        
        # Collect all votes concurrently
        voting_tasks = []
        for voter_agent in agents:
            for proposal in proposals:
                voting_tasks.append(vote_on_proposal(voter_agent, proposal))
        
        # Execute all voting tasks
        vote_results = await asyncio.gather(*voting_tasks, return_exceptions=True)
        
        # Organize votes by proposal
        for i, vote_list in enumerate(vote_results):
            if isinstance(vote_list, list) and vote_list:
                vote = vote_list[0]
                
                # Find the corresponding proposal score
                for score in proposal_scores:
                    if score.proposal.agent_name == vote.proposal_target:
                        score.votes.append(vote)
                        
                        # Update vote counts
                        if vote.choice == VoteChoice.APPROVE:
                            score.approval_count += 1
                        elif vote.choice == VoteChoice.REJECT:
                            score.rejection_count += 1
                        else:
                            score.abstain_count += 1
                        
                        # Check for Claude Code special handling
                        if vote.agent_type == AgentType.CLAUDE_CODE.value:
                            if vote.choice == VoteChoice.APPROVE:
                                score.claude_approved = True
                            elif vote.choice == VoteChoice.REJECT and self.enable_claude_veto:
                                score.claude_vetoed = True
                                self.logger.info(f"[{voting_id[:8]}] üö´ Claude Code VETOED {score.proposal.agent_name}'s proposal")
                        
                        break
        
        # Calculate weighted scores
        for score in proposal_scores:
            total_weighted_score = 0.0
            total_raw_score = 0.0
            
            for vote in score.votes:
                vote_value = 0.0
                if vote.choice == VoteChoice.APPROVE:
                    vote_value = 1.0 * vote.confidence
                elif vote.choice == VoteChoice.REJECT:
                    vote_value = -1.0 * vote.confidence
                # ABSTAIN = 0.0
                
                weight = self.vote_weights.get(vote.agent_type, 1.0)
                total_weighted_score += vote_value * weight
                total_raw_score += vote_value
            
            score.total_score = total_raw_score
            score.weighted_score = total_weighted_score
        
        self.logger.info(f"[{voting_id[:8]}] üìä Collected votes: {sum(len(s.votes) for s in proposal_scores)} total votes")
        return proposal_scores
    
    def _parse_vote_response(self, voter_agent: BaseAgent, proposal_target: str, response_content: str) -> Optional[AgentVote]:
        """Parse vote information from agent response."""
        try:
            lines = response_content.strip().split('\n')
            
            vote_choice = None
            confidence = 0.5
            reasoning = ""
            
            for line in lines:
                line = line.strip()
                if line.startswith("**VOTE:**"):
                    vote_text = line.replace("**VOTE:**", "").strip().upper()
                    if "APPROVE" in vote_text:
                        vote_choice = VoteChoice.APPROVE
                    elif "REJECT" in vote_text:
                        vote_choice = VoteChoice.REJECT
                    elif "ABSTAIN" in vote_text:
                        vote_choice = VoteChoice.ABSTAIN
                
                elif line.startswith("**CONFIDENCE:**"):
                    conf_text = line.replace("**CONFIDENCE:**", "").strip()
                    try:
                        confidence = float(conf_text)
                        confidence = max(0.0, min(1.0, confidence))  # Clamp to [0,1]
                    except ValueError:
                        confidence = 0.5
                
                elif line.startswith("**REASONING:**"):
                    reasoning = line.replace("**REASONING:**", "").strip()
            
            # Also try to extract reasoning from the rest of the response
            if not reasoning:
                reasoning_start = response_content.find("**REASONING:**")
                if reasoning_start != -1:
                    reasoning = response_content[reasoning_start + len("**REASONING:")]:].strip()
            
            if vote_choice:
                return AgentVote(
                    agent_name=voter_agent.name,
                    agent_type=voter_agent.agent_type.value,
                    proposal_target=proposal_target,
                    choice=vote_choice,
                    confidence=confidence,
                    reasoning=reasoning or "No reasoning provided"
                )
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error parsing vote response: {e}")
            return None
    
    def _determine_winner(self, proposal_scores: List[ProposalScore], voting_id: str) -> Optional[ProposalScore]:
        """Determine the winning proposal based on votes and weights."""
        
        if not proposal_scores:
            return None
        
        # First, eliminate any Claude-vetoed proposals
        valid_proposals = [
            score for score in proposal_scores
            if not score.claude_vetoed
        ]
        
        if not valid_proposals:
            self.logger.info(f"[{voting_id[:8]}] ‚ùå All proposals were vetoed by Claude Code")
            return None
        
        # Sort by weighted score (highest first)
        valid_proposals.sort(key=lambda x: x.weighted_score, reverse=True)
        
        # Check if we have a clear winner
        winner = valid_proposals[0]
        
        # Must have positive weighted score to win
        if winner.weighted_score <= 0:
            self.logger.info(f"[{voting_id[:8]}] ‚ùå No proposal received positive approval")
            return None
        
        self.logger.info(f"[{voting_id[:8]}] üèÜ Winner: {winner.proposal.agent_name} (score: {winner.weighted_score:.2f})")
        return winner
    
    def _check_consensus(self, proposal_scores: List[ProposalScore]) -> bool:
        """Check if there's strong consensus in the voting."""
        if not proposal_scores:
            return False
        
        # Sort by weighted score
        sorted_scores = sorted(proposal_scores, key=lambda x: x.weighted_score, reverse=True)
        
        if len(sorted_scores) < 2:
            return True
        
        # Check if winner has significantly higher score than second place
        winner_score = sorted_scores[0].weighted_score
        second_score = sorted_scores[1].weighted_score
        
        # Consider consensus if winner has at least 2x the score of second place
        return winner_score > 0 and (second_score <= 0 or winner_score >= 2 * second_score)
    
    def _create_voting_summary(self, proposal_scores: List[ProposalScore], winner: Optional[ProposalScore]) -> str:
        """Create a human-readable voting summary."""
        if not proposal_scores:
            return "No proposals to vote on."
        
        summary_lines = ["**üó≥Ô∏è Voting Results:**"]
        
        # Sort proposals by score for display
        sorted_scores = sorted(proposal_scores, key=lambda x: x.weighted_score, reverse=True)
        
        for i, score in enumerate(sorted_scores):
            status = "üèÜ WINNER" if winner and score.proposal.agent_name == winner.proposal.agent_name else ""
            if score.claude_vetoed:
                status = "üö´ VETOED"
            
            summary_lines.append(
                f"{i+1}. **{score.proposal.agent_name}** "
                f"(Score: {score.weighted_score:.2f}, "
                f"Votes: ‚úÖ{score.approval_count} ‚ùå{score.rejection_count} ‚ûñ{score.abstain_count}) "
                f"{status}"
            )
        
        if winner:
            summary_lines.append(f"\n**Final Decision:** {winner.proposal.agent_name}'s solution will be implemented")
        else:
            summary_lines.append(f"\n**Final Decision:** No solution reached consensus")
        
        return "\n".join(summary_lines)


class VotingError(Exception):
    """Exception raised during voting operations."""
    pass
</file>

<file path="README.md">
# AngelaMCP
# Multi-AI Agent Collaboration Platform

A production-grade terminal-based platform that orchestrates collaboration between multiple AI agents, featuring Claude Code as the senior developer with OpenAI and Gemini as supporting agents.

## üéØ Overview

AngelaMCP enables sophisticated AI collaboration through:
- **Claude Code** as the primary agent with file system access and code execution
- **OpenAI o3-mini** for code review and alternative perspectives
- **Gemini 2.5-pro** for research, documentation, and parallel analysis
- **Debate & Voting System** for collaborative decision-making
- **Async Task Management** for efficient parallel execution
- **Rich Terminal UI** with real-time streaming

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   AngelaMCP Orchestrator                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Claude Code   ‚îÇ  OpenAI API   ‚îÇ    Gemini API        ‚îÇ
‚îÇ   (via CLI)     ‚îÇ   (o3-mini)   ‚îÇ (2.5-pro-preview)    ‚îÇ
‚îÇ                 ‚îÇ               ‚îÇ                       ‚îÇ
‚îÇ ‚Ä¢ File System   ‚îÇ ‚Ä¢ Code Review ‚îÇ ‚Ä¢ Research           ‚îÇ
‚îÇ ‚Ä¢ Code Exec     ‚îÇ ‚Ä¢ Critique    ‚îÇ ‚Ä¢ Documentation      ‚îÇ
‚îÇ ‚Ä¢ Main Tasks    ‚îÇ ‚Ä¢ Validation  ‚îÇ ‚Ä¢ Parallel Analysis  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    PostgreSQL + Redis
                    (Persistence Layer)
```

## ‚ú® Key Features

### 1. **Intelligent Task Distribution**
- Claude Code handles primary development tasks
- Parallel task execution for supporting agents
- Smart routing based on task type

### 2. **Collaborative Decision Making**
- Agents propose solutions independently
- Structured debate protocol for critiques
- Weighted voting system (Claude Code has senior vote)
- Consensus building with override capabilities

### 3. **Production-Ready Infrastructure**
- PostgreSQL for conversation persistence
- Redis for session caching
- Comprehensive error handling
- Rate limiting and cost tracking
- Async/await for optimal performance

### 4. **Rich Terminal Experience**
- Real-time streaming of agent thoughts
- Interactive command interface
- Progress indicators and status updates
- Color-coded agent outputs

## üöÄ Quick Start

```bash
# Clone the repository
git clone https://github.com/CarterPerez-dev/AngelaMCP.git
cd AngelaMCP

# Set up the environment
make setup

# Configure your API keys
cp .env.example .env
# Edit .env with your API keys

# Initialize the database
make db-init

# Run the platform
make run
```

## üìã Prerequisites

- **Debian/Ubuntu Linux** (tested on Debian 11+)
- **Python 3.10+**
- **PostgreSQL 14+**
- **Redis 6+**
- **Claude Code** installed and configured
- API keys for OpenAI and Google Gemini

## üîß Configuration

The platform uses environment variables for configuration:

```bash
# Claude Code (uses existing Claude account)
CLAUDE_CODE_PATH=/usr/local/bin/claude

# OpenAI
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=o3-mini # Or change it (Use smartest model is my recommendation)

# Google Gemini
GOOGLE_API_KEY=your-api-key-here
GEMINI_MODEL=gemini-2.5-pro-preview-06-05

# Database
DATABASE_URL=postgresql://user:pass@localhost/angelamcp
REDIS_URL=redis://localhost:6379
```

## üìñ Usage Examples

### Basic Task Execution
```bash
# Start MACP
$ macp

MACP> Create a REST API for a todo application with authentication

[Claude Code]: Analyzing requirements...
[Gemini]: Researching best practices for REST API design...
[OpenAI]: Preparing to review implementation...
```

### Guided Collaboration
```bash
MACP> /debate Should we use JWT or session-based auth?

[Debate Initiated]
[Claude Code]: I recommend JWT for stateless authentication...
[OpenAI]: Consider session-based for better security...
[Gemini]: Here's a comparison of both approaches...
[Voting]: JWT (2 votes) vs Sessions (1 vote)
```

## üß™ Testing

```bash
# Run all tests
make test

# Run specific test suites
make test-unit
make test-integration

# Test agent connectivity
python scripts/test_agents.py
```

## üìö Documentation

- [SETUP.md](./SETUP.md) - Detailed installation instructions
- [ARCHITECTURE.md](./ARCHITECTURE.md) - System design and flow
- [API.md](./docs/API.md) - API reference
- [Examples](./docs/examples/) - Usage examples

## ü§ù Contributing

Please see [CONTRIBUTING.md](./docs/CONTRIBUTING.md) for guidelines.

## üìÑ License

MIT License - see [LICENSE](./LICENSE) for details.

## üîÆ Roadmap

- [ ] Web UI interface
- [ ] Plugin system for custom agents
- [ ] Advanced memory and learning
- [ ] Multi-project workspace support
- [ ] Team collaboration features

---

Built with ‚ù§Ô∏è using Claude Code, OpenAI, and Gemini
</file>

</files>
